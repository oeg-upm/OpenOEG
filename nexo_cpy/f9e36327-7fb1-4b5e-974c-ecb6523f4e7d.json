{
  "message": "SYSTEM: Wiki: https://delicias.dia.fi.upm.es/wiki/index.php/DrInventor \n\nPromoting Scientific Creativity by Utilising Web-based Research Objects\nScientific creativity and innovation represent the beating heart of European growth at a time of rapid technological change. Dr Inventor is built on the vision that technologies have great potential to supplement human ingenuity in science by overcoming the limitations that people suffer in pursuing scientific discovery. It presents an original system that will provide inspiration for scientific creativity by utilising the rich presence of web-based research resources. Dr Inventor will act as a personal research assistant, utilising machine-empowered search and computation to bring researchers extended perspectives for scientific innovation by informing them of a broad spectrum of relevant research concepts and approaches, by assessing the novelty of research ideas, and by offering suggestions of new concepts and workflows with unexpected features for new scientific discovery.\nDr Inventor is an attempt to understand the potential of technology in the scientific creative process within current technology limitations. It represents a sound balance between scientific insight into individual scientific creative processes and technical implementation using innovative technologies in information extraction, document summarization, semantics and visual analytics. The outcomes will be integrated into a web-based system that will allow evaluation in a selected research area under real-world settings with carefully designed metrics, benchmarks and baseline for creative performance, leading to tangible measurements on the performance of the technologies in enhancing human creativity and a blueprint for future technologies in computational creativity.\nDr Inventor has huge implications for scientific innovation in Europe, as it has the potential to change the way in which scientific research is undertaken. The acceptance of the system by general research communities will open opportunities for many industrial sectors, leading to reinforced leadership of European industry.\nMeetings\nContents\n[\nhide\n]\n1\nOntology\n2\nRecommendation\n2.1\nEvaluación\n3\nRepository\n3.1\nInstallation\n3.2\nFunctionalities\n3.2.1\nAPI REST\n3.3\nSystem architecture\n3.3.1\nPom.xml structure\nOntology\n\nRecommendation\n\nIdeas iniciales del recomendador para su discusión\nInformación disponible para hacer las recomendaciones:\nArtículos sobre computer graphics (CG) en txt\nTítulo de cada artículo\nKeywords establecidas por los autores\nLista de referncias de cada artículo\nPosible información semántica de cada artículo\nElementos retóricos (frase+clase)\nSignificado de la presencia de cada cita (\ntrabajo futuro\n)\nPrimer recomendador: Recomendador basado en contenido\nItems: artículos a recomendar representados por: a) vector de las 100 palabras más importantes (=frecuentes); b) keywords\nUsuario: perfil de usuario representado por las keywords más frecuentes encontradas en los artículos visitados\nRecomendación: items que encajan con el perfil de usuario\nMirar cómo hacían la representación de los documentos en (son de los primeros recomendadores pero efectivos):\nK. D. Bollacker, S. Lawrence, y C. L. Giles, «CiteSeer: an autonomous Web agent for automatic retrieval and identification of interesting publications», New York, NY, USA, 1998, pp. 116–123\n.\nJ. Ahn, P. Brusilovsky, J. Grady, D. He, y S. Y. Syn, «Open user profiles for adaptive news systems: help or harm?», New York, NY, USA, 2007, pp. 11–20\n.\nMirar cómo hacían el indexado y recomendación en:\nJ. Hannon, M. Bennett, y B. Smyth, «Recommending twitter users to follow using content and collaborative filtering approaches», New York, NY, USA, 2010, pp. 199–206\n.\nEsta primera aproximación nos dará un recomendador (básico) del que partir.\nSegundo recomendador: Recomendador basado en contenido utilizando información semántica\nItems: artículos a recomendar representados por: a) vectores de los elementos semánticos del discurso que contienen las 100 palabras más importante de cada elemento (i.e: [hypothesis, [words-related-to-hypothesis...]]): b) keywords\nEn este caso para la recomendación se utilizará la información semántica, haciendo comparaciones entre vectores del mismo elemento semántico, i.e. similitud entre vectores de hipótesis, entre vectores de objetivos, etc...\nA partir de los resultados obtenidos podremos observar cómo varían las recomendaciones con respecto al caso mejor, ¿mejoramos precisión? ¿mejoramos CTR? (\nsería ideal poder hacer experimentos con usuarios para medir CTR\n)\nVersiones mejores\nCuando esté disponible todo el proceso de ontology learning y ontology matching incorporar dicha información al recomendador y comparar las recomendaciones con el algoritmo de FolkRank al estilo de cómo hicieron\nS. Doerfel, R. Jäschke, A. Hotho, y G. Stumme, «Leveraging Publication Metadata and Social Data into FolkRank for Scientific Publication Recommendation», en Proceedings of the 4th ACM RecSys Workshop on Recommender Systems and the Social Web, New York, NY, USA, 2012, pp. 9–16\n.\nEvaluación\n\nDataset\nPosibles Dataset para la evaluación\nPapers de computer graphics, los ficheros que tenemos del proyecto. Es necesario dividir en dos el datasest para tener un conjunto de entrentamiento y uno de prueba. También sería necesario establecer relevancias o bien decidir cómo se considera que un paper recomendado es relevante.\nOAI-PMH, ¿de qué fuentes tenemos datos? (arxiv? citeseer? pubmed?) ¿qué datos estamos recopilando guardando?\narxiv, supongo que estaremos guardando todos los metadatos de su oai-pmh. Igualmente habría que formar el dataset y decidir cuándo un artículo es relevante y cuándo no.\nOpciones para construir el dataset de prueba\nUsuario/s que evalúen los papers\nRegla que indique cuándo un paper es relevante (i.e. for each topic all the relevant documents in the collection are selected and classified as relevant if it contains information that is associated with the topic)\nMétricas de evaluación\nPerformance / Relevance evaluation\nNDCG\nDCG measures the usefulness, or gain, of a document based on its position in the result list. The gain is accumulated from the top of the result list to the bottom with the gain of each result discounted at lower ranks.\nPrecision at the top N ranked results (P@N). Precision is defined as the number of retrieved relevant documents divided by the total number of retrieved documents\nMean Average Precision (MAP).  MAP is a precision metric that emphasises ranking relevant documents higher.\nUsage / Users evaluation\nClick-through rate (CTR) is a commonly used metric for assessing the impact of a recommendation system on final users.\nClick-through rate = Click-throughs (#) / Impressions (#)\nFramework\nhttp://rival.recommenders.net/\nRepository\n\nInstallation",
  "speaker": "SYSTEM",
  "uuid": "f9e36327-7fb1-4b5e-974c-ecb6523f4e7d"
}