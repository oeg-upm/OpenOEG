{
  "message": "SYSTEM: Latent_Topics_in_Microposts.pdf: Página 3\nLDA – Latent Dirichlet Allocation \n• M: Documents \n• N: Words \n• K: Topics \n• θi: topic distribution for document i \n• Φk : is the word distribution for topic k \n• zi,j : is the topics for the j-th word in the doc i \n• wi,j : is the j-tj word in the doc I \n• α: Dirichlet prior on the per-document topic \ndistributions. \n• ϐ: Dirichlet prior on the per-topic word \ndistribution \n \n Blei, David M.; Ng, Andrew Y.; Jordan, Michael I (January 2003). Lafferty, John. ed. \"Latent \nDirichlet allocation\". Journal of Machine Learning Research 3 (4–5): pp. 993–1022. \nAllows sets of observations (words) to be explained by unobserved groups \n(Topics) that explain why some parts of the data are similar. \nBayesian Inference problem \n",
  "speaker": "SYSTEM",
  "uuid": "ee02b91e-c7d0-410c-8a15-b525966926ed"
}