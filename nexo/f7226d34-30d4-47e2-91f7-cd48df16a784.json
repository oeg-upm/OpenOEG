{
  "message": "SYSTEM: Wiki: https://delicias.dia.fi.upm.es/wiki/index.php/Preguntas_de_investigaci%C3%B3n \n\nResearch hypotheses\n\nH1 - Given a set of RDF data streams, it is possible to define an RDF interchange format that optimizes the space and time for the data exchange and parsing.\nH2 - Given an RDF streaming engine, and a set of SPARQL queries, a RDF interchange format can be tuned to offer better performance in data exchange among processing nodes and query resolution.\nResearch questions\n\nIs the compressed format RDF/HDT a good solution for these dynamic data?\nWhich are the particularities/regularities of RDF data streams?\nCan an RDF interchange format be parallelizable for compression and decompression (parsing)?\nRelevance\n\nThere is actually a vast diversity of devices and networks managing data streams; the tremendous success of mobile devices and the emergent Internet of Things are just a small seed of this. RDF streaming is starting to follow the same pattern of deployment. We focus on the efficient transmission of RDF streams, a necessary step to ensure higher throughput for RDF Stream processors. To do so, different representations and compression techniques are required. Previous works on RDF compression show important size reductions of large RDF datasets, hence enabling an efficient RDF exchange. However, these solutions consider static RDF datasets, and need to read the whole dataset to take advantage of data regularities. We put the focus on specific RDF compression for RDF data streams.\nJorge Gracia\n\nResearch questions\n\nHow to reconcile information expressed in different natural languages in the Web of Data?\nHow to create an ecosystem of Linked Data-based Language Resources to support a new generation of LD-aware NLP services?\nRelevance\n\nThe Web of Data is increasingly multilingual. Ideally, users should be able to access information in their own language on the Web of Data, no matter the language in which the information is actually described. To achieve this objective, new methods, techniques and tools are needed to overcome the language barriers among datasets and ontologies on the Web. \nFurther, an emerging ecosystem of Language Resources available as linked data on the Web will largely benefit such objective, allowing also the emergence of a new generation of linked data-aware NLP applications.\nResearch hypotheses\n\nA critical mass of multilingual data and language resources has to be available on the Web (as linked data) to support a new family of multilingual and cross-lingual services (for querying, relation discovery, matching, etc.)\nAlmudena Ruiz-Iniesta\n\nResearch questions\n\nIs it possible to suggest new research ideas to the users in an autonomous manner?\nIs it possible to replicate human creativity in a certain degree by taking advantage of computing power and the Internet to combine the widely accessible resources to generate new concepts with unexpected features?\nRelevance\n\nCreativity is a way in which we examine a problem with an open mind and fresh eyes to explore new possibilities outside the established approaches through the use of our imagination based on knowledge. New technologies, in particular artificial intelligence, are drastically changing the nature of creative processes. Recent studies show significant contributions by the Internet to scientific research. In fact, the heavy presence of all types of research objects on the Web, which are understood as aggregations of scientific items that are important in the context of scientific investigation, such as publications, source code and data in research website, wiki, blog, is now becoming incredibly useful in research and learning. Although the Web has become an extremely rich resource for a huge number of research objects, currently these objects can only be explored manually in research activities due to the lack of effective tools.\nResearch hypotheses\n\nAntonio Sánchez-Padial\n\nResearch questions\n\nHow can the machine-actionable elements that appear in data management plans for agriculture, food and forestry research be used to access and check accomplishment within automatically negotiated services?\nRelevance\n\nThe importance of Data Management Plans is increasing, as awareness of the importance of preservation of research data rises up. However, because of their very nature DMPs can become a burocratic charge for researchers, addind few value to their processes, and be extremely difficult to check by funders, especially for stages in the data lifecycle other than preservation or publishing. Active (or actionable) Data Management Plans are a proposal to be sure that DMP are machine-readable and machine-actionable so that they it's possible to build method to automatically measure their level of accomplishment.\nThis work will propose a way to leverage Active Data Management Plans to provide data managers in research projects with tools able to communicate automatically with data management services.\nResearch hypotheses\n\n1- Given a a model of actions, events and roles in data management plans, and a model to describe the data services provided by differente platforms; it will be possible to develop a map between both model matching requirements in the Data Management Model with data services.\nResearch hypotheses\n\nMaría Navas-Loro\n\nResearch questions\n\nWhat are the particularities of temporal processing in the legal domain?\nAre state-of-the-art generic temporal taggers enough or they need to be adapted to the legal domain?\nShould a brand new approach be considered?\nRelevance\n\nTemporal annotation in the legal domain has not recieved a lot of atention in literature, in contrast to annotation in other kind of documents such as news or historical texts. Nevertheless, being able to process the temporal dimension of legal texts (texts containing legal nuances, such as open data datasets or news) would help in several tasks, such as summarization, timeline generation or time-contraint related norms extraction, and will also be applicable to further applications (such as temporal reasoning on legal texts).\nMy PhD thesis focuses on dealing specifically with judgments and decision courts, both in English and Spanish, annotating and representing it in the form of a timeline. Besides the processing of temporal information, other information surronding temporal expressions or related to their extraction (such as misleading legal references, events or NER) is also in the scope of this thesis, as far as the temporal annotation is concerned.\nResearch hypotheses\n\nA dedicated approach will improve current generic temporal taggers (precision, recall, f-measure).",
  "speaker": "SYSTEM",
  "uuid": "f7226d34-30d4-47e2-91f7-cd48df16a784"
}