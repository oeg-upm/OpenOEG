{
  "message": "SYSTEM: Science-2015-HirschbergManning-NLP.pdf: Página 1\nREVIEW\nAdvances in natural\nlanguage processing\nJulia Hirschberg1* and Christopher D. Manning2,3\nNatural language processing employs computational techniques for the purpose of learning,\nunderstanding, and producing human language content. Early computational approaches to\nlanguage research focused on automating the analysis of the linguistic structure of language\nand developing basic technologies such as machine translation, speech recognition, and speech\nsynthesis.Today’s researchers refine and make use of such tools in real-world applications,\ncreating spoken dialogue systems and speech-to-speech translation engines, mining social\nmedia for information about health or finance, and identifying sentiment and emotion toward\nproducts and services. We describe successes and challenges in this rapidly advancing area.\nO\nver the past 20 years, computational lin-\nguistics has grown into both an exciting\narea of scientific research and a practical\ntechnology that is increasingly being in-\ncorporated into consumer products (for\nexample, in applications such as Apple’s Siri and\nSkype Translator). Four key factors enabled these\ndevelopments: (i) a vast increase in computing\npower, (ii) the availability of very large amounts\nof linguistic data, (iii) the development of highly\nsuccessful machine learning (ML) methods, and\n(iv) a much richer understanding of the structure\nof human language and its deployment in social\ncontexts. In this Review, we describe some cur-\nrent application areas of interest in language\nresearch. These efforts illustrate computational\napproaches to big data, based on current cutting-\nedge methodologies that combine statistical anal-\nysis and ML with knowledge of language.\nComputational linguistics, also known as nat-\nural language processing (NLP), is the subfield\nof computer science concerned with using com-\nputational techniques to learn, understand, and\nproduce human language content. Computation-\nal linguistic systems can have multiple purposes:\nThe goal can be aiding human-human commu-\nnication, such as in machine translation (MT);\naiding human-machine communication, such as\nwith conversational agents; or benefiting both\nhumans and machines by analyzing and learn-\ning from the enormous quantity of human lan-\nguage content that is now available online.\nDuring the first several decades of work in\ncomputational linguistics, scientists attempted\nto write down for computers the vocabularies\nand rules of human languages. This proved a\ndifficult task, owing to the variability, ambiguity,\nand context-dependent interpretation of human\nlanguages. For instance, a star can be either an\nastronomical object or a person, and “star” can\nbe a noun or a verb. In another example, two in-\nterpretations are possible for the headline “Teacher\nstrikes idle kids,” depending on the noun, verb, and\nadjective assignments of the words in the sentence,\nas well as grammatical structure. Beginning in the\n1980s, but more widely in the 1990s, NLP was\ntransformed by researchers starting to build mod-\nels over large quantities of empirical language\ndata. Statistical or corpus (“body of words”)–based\nNLP was one of the first notable successes of\nthe use of big data, long before the power of\nML was more generally recognized or the term\n“big data” even introduced.\nA central finding of this statistical approach to\nNLP has been that simple methods using words,\npart-of-speech (POS) sequences (such as whether\na word is a noun, verb, or preposition), or simple\ntemplates can often achieve notable results when\ntrained on large quantities of data. Many text\nand sentiment classifiers are still based solely on\nthe different sets of words (“bag of words”) that\ndocuments contain, without regard to sentence\nand discourse structure or meaning. Achieving\nimprovements over these simple baselines can be\nquite difficult. Nevertheless, the best-performing\nsystems now use sophisticated ML approaches\nand a rich understanding of linguistic structure.\nHigh-performance tools that identify syntactic\nand semantic information as well as information\nabout discourse context are now available. One\nexample is Stanford CoreNLP (1), which provides\na standard NLP preprocessing pipeline that in-\ncludes POS tagging (with tags such as noun, verb,\nand preposition); identification of named entities,\nsuch as people, places, and organizations; parsing\nof sentences into their grammatical structures;\nand identifying co-references between noun\nphrase mentions (Fig. 1).\nHistorically, two developments enabled the\ninitial transformation of NLP into a big data field.\nThe first was the early availability to researchers\nof linguistic data in digital form, particularly\nthrough the Linguistic Data Consortium (LDC)\n(2), established in 1992. Today, large amounts\nof digital text can easily be downloaded from\nthe Web. Available as linguistically annotated\ndata are large speech and text corpora anno-\ntated with POS tags, syntactic parses, semantic\nlabels, annotations of named entities (persons,\nplaces, organizations), dialogue acts (statement,\nquestion, request), emotions and positive or neg-\native sentiment, and discourse structure (topic\nor rhetorical structure). Second, performance im-\nprovements in NLP were spurred on by shared\ntask competitions. Originally, these competitions\nwere largely funded and organized by the U.S.\nDepartment of Defense, but they were later or-\nganized by the research community itself, such\nas the CoNLL Shared Tasks (3). These tasks were\na precursor of modern ML predictive modeling\nand analytics competitions, such as on Kaggle (4),\nin which companies and researchers post their\ndataand statisticiansand data miners fromallover\nthe world compete to produce the best models.\nA major limitation of NLP today is the fact that\nmost NLP resources and systems are available\nonly for high-resource languages (HRLs), such as\nEnglish, French, Spanish, German, and Chinese.\nIn contrast, many low-resource languages (LRLs)—\nsuch as Bengali, Indonesian, Punjabi, Cebuano,\nand Swahili—spoken and written by millions of\npeople have no such resources or systems avail-\nable. A future challenge for the language commu-\nnity is how to develop resources and tools for\nhundreds orthousands of languages, not just a few.\nMachine translation\nProficiency in languages was traditionally a hall-\nmark of a learned person. Although the social\nstanding of this human skill has declined in the\nmodern age of science and machines, translation\nbetween human languages remains crucially im-\nportant, and MT is perhaps the most substantial\nway in which computers could aid human-human\ncommunication. Moreover, the ability of com-\nputers to translate between human languages\nremains a consummate test of machine intel-\nligence: Correct translation requires not only\nthe ability to analyze and generate sentences in\nhuman languages but also a humanlike under-\nstanding of world knowledge and context, de-\nspite the ambiguities of languages. For example,\nthe French word“bordel” straightforwardly means\n“brothel”; but if someone says “My room is un\nbordel,” then a translating machine has to know\nenough to suspect that this person is probably not\nrunning a brothel in his or her room but rather is\nsaying “My room is a complete mess.”\nMachine translation was one of the first non-\nnumericapplicationsofcomputersandwasstudied\nintensively starting in the late 1950s. However, the\nhand-built grammar-based systems of early dec-\nades achieved very limited success. The field was\ntransformed in the early 1990s when researchers\nat IBM acquired a large quantity of English and\nFrench sentences that were translations of each\nother (known as parallel text), produced as the\nproceedings of the bilingual Canadian Parliament.\nThese data allowed them to collect statistics of\nword translations and word sequences and to\nbuild a probabilistic model of MT (5).\nFollowing a quiet period in the late 1990s,\nthe new millennium brought the potent combina-\ntion of ample online text, including considerable\nquantities of parallel text, much more abundant\nand inexpensive computing, and a new idea\nfor building statistical phrase-based MT systems\nSCIENCE sciencemag.org\n17 JULY 2015 • VOL 349 ISSUE 6245\n261\n1Department of Computer Science, Columbia University, New York,\nNY 10027, USA. 2Department of Linguistics, Stanford University,\nStanford, CA 94305-2150, USA. 3Department of Computer\nScience, Stanford University, Stanford, CA 94305-9020, USA.\n*Corresponding author. E-mail: julia@cs.columbia.edu\n on July 17, 2015\nwww.sciencemag.org\nDownloaded from \n on July 17, 2015\nwww.sciencemag.org\nDownloaded from \n on July 17, 2015\nwww.sciencemag.org\nDownloaded from \n on July 17, 2015\nwww.sciencemag.org\nDownloaded from \n on July 17, 2015\nwww.sciencemag.org\nDownloaded from \n on July 17, 2015\nwww.sciencemag.org\nDownloaded from \n",
  "speaker": "SYSTEM",
  "uuid": "bbc82430-5028-4fbd-bcf1-e32a7725fd65"
}