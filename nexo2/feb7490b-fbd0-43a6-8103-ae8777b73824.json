{
  "message": "SYSTEM: Sauper2007.pdf: Página 2\nword “pleased” should contribute most strongly to\nthe sentiment of the audio aspect when it is aug-\nmented with a relevant topic indicator.\nThe coupling of the content model and the task-\nspeciﬁc model allows the two components to mutu-\nally inﬂuence each other during learning. The con-\ntent model leverages unannotated data to improve\nthe performance of the task-speciﬁc model, while\nthe task-speciﬁc model provides feedback to im-\nprove the relevance of the content model. The com-\nbined model can be learned effectively using a novel\nEM-based method for joint training.\nWe evaluate our approach on two complementary\ntext analysis tasks. Our ﬁrst task is a multi-aspect\nsentiment analysis task, where a system predicts the\naspect-speciﬁc sentiment ratings (Snyder and Barzi-\nlay, 2007). Second, we consider a multi-aspect ex-\ntractive summarization task in which a system ex-\ntracts key properties for a pre-speciﬁed set of as-\npects. On both tasks, our method for incorporating\ncontent structure consistently outperforms structure-\nagnostic counterparts.\nMoreover, jointly learning\ncontent and task parameters yields additional gains\nover independently learned models.\n2\nRelated Work\nPrior research has demonstrated the usefulness of\ncontent models for discourse-level tasks. Examples\nof such tasks include sentence ordering (Barzilay\nand Lee, 2004; Elsner et al., 2007), extraction-based\nsummarization (Haghighi and Vanderwende, 2009)\nand text segmentation (Chen et al., 2009).\nSince\nthese tasks are inherently tied to document structure,\na content model is essential to performing them suc-\ncessfully. In contrast, the applications considered in\nthis paper are typically developed without any dis-\ncourse information, focusing on capturing sentence-\nlevel relations. Our goal is to augment these models\nwith document-level content information.\nSeveral applications in information extraction\nand sentiment analysis are close in spirit to our\nwork (Pang and Lee, 2004; Patwardhan and Riloff,\n2007; McDonald et al., 2007). These approaches\nconsider global contextual information when de-\ntermining whether a given sentence is relevant to\nthe underlying analysis task. All assume that rele-\nvant sentences have been annotated. For instance,\nPang and Lee (2004) reﬁne the accuracy of sen-\ntiment analysis by considering only the subjective\nsentences of a review as determined by an indepen-\ndent classiﬁer. Patwardhan and Riloff (2007) take\na similar approach in the context of information ex-\ntraction. Rather than applying their extractor to all\nthe sentences in a document, they limit it to event-\nrelevant sentences. Since these sentences are more\nlikely to contain information of interest, the extrac-\ntion performance increases.\nAnother approach, taken by Choi and Cardie\n(2008) and Somasundaran et al. (2009) uses lin-\nguistic resources to create a latent model in a task-\nspeciﬁc fashion to improve performance, rather than\nassuming sentence-level task relevancy. Choi and\nCardie (2008) address a sentiment analysis task by\nusing a heuristic decision process based on word-\nlevel intermediate variables to represent polarity.\nSomasundaran et al. (2009) similarly uses a boot-\nstrapped local polarity classiﬁer to identify sentence\npolarity.\nMcDonald\net\nal.\n(2007)\npropose\na\nmodel\nwhich jointly identiﬁes global polarity as well as\nparagraph- and sentence-level polarity, all of which\nare observed in training data. While our approach\nuses a similar hierarchy, McDonald et al. (2007) is\nconcerned with recovering the labels at all levels,\nwhereas in this work we are interested in using la-\ntent document content structure as a means to beneﬁt\ntask predictions.\nWhile our method also incorporates contextual\ninformation into existing text analysis applications,\nour approach is markedly different from the above\napproaches. First, our representation of context en-\ncodes more than the relevance-based binary distinc-\ntion considered in the past work. Our algorithm ad-\njusts the content model dynamically for a given task\nrather than pre-specifying it. Second, while previ-\nous work is fully supervised, in our case relevance\nannotations are readily available for only a few ap-\nplications and are prohibitively expensive to obtain\nfor many others. To overcome this drawback, our\nmethod induces a content model in an unsupervised\nfashion and connects it via latent variables to the\ntarget model. This design not only eliminates the\nneed for additional annotations, but also allows the\nalgorithm to leverage large quantities of raw data for\ntraining the content model. The tight coupling of rel-\n",
  "speaker": "SYSTEM",
  "uuid": "feb7490b-fbd0-43a6-8103-ae8777b73824"
}