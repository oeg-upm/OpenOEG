{
  "message": "SYSTEM: Wiki: https://delicias.dia.fi.upm.es/wiki/index.php/ElViajeroLinkedData \n\nVisualizador\nQue la fecha se meta como un long y no como un literal (en caso de que se pueda).\n(No se hará debido a los formatos en que están los datos)\nQuitar los espacios de los upos (meterlos en mayúsculas y con urlencode)\nMeter los autores sin acentos ni espacios (hacer urlencode)\nEn general, hacer urlencode de todas las instancias para evitar errores. UTF-8\nActualizar los upos, repitiendo el parser de upos en el nuevo formato (que espero NO VUELVAN A CAMBIAR)\nCorregir los links de las guías que terminan en #despiece, ya que se refieren a zonas de otras guías. Para ello hay que quitar lo que haya después del /Tes, referenciando adecuadamente el rol (que es que utiliza parte de otro fichero).\nEn los comentarios, añadir también al creador en mayúsculas y sin espacios (con urlencode).\nEliminar los espacios, \\n y \\\\n (como en \\\\nMadrid)\nEn el modelo, he puesto que dc:creator es equivalente a mpeg7:creator, cuando es una superprpopiedad\nCorregir los namespaces del modelo, cambiando por los nuevos que tendrá la ontología para ser publicada\nCrear una nueva ontología inportando el core del opmo y expandiendo las clases adecuadas en un profile, en vez de extender la que ya existe (por el problema de los namespaces)\nLos links se debería de poner como xsd:anyuri (y meterlos como dataproperty). Añadir la función.\nSugerencia de Victor: consultar DBpedia o geolinkeddata para extraer información concreta de geolocalización acerca de la localidad extraída de la noticia. Enlazar con una relación owl:sameAs con el recurso apropiadamente.\nEn vez de proceso de creación, meter las guías como proceso de publicación. (O no?)\nEn los enlaces, cambiar los \"/\" por \"_\" para que no haya problemas con las rutas.\nFallo al introducir los roles de uso: algunos de los individuos se llaman igual que la clase\nAgentes en mayúsculas.\nPor petición de Luis, incluir también el body de los posts y noticias. Por si terceros quieren parsearlo en busca de más extracción de información.\nGuardar un log con los ficheros que fallan o los posts que no tienen nicetitle para informar a prisacom.\nValorar si incluir los .PES (pdfs)\n. DESECHADO POR AHORA\nPosible mapping entre sioc:content y opmo:content\nAñadir a la ontología los labels en español con cada una de las clases\nUtilizar SILK para completar los enlaces con la dbpedia. Para ello, añadir etiquetas a cada localización con su significado en español.\nHe descubierto que los creadores de las guías son Agents (opmAgents) cuando deberían ser accounts. Es decir, que la tripla: <\nhttp://webenemasuno.linkeddata.es/resource/elviajero/Guide/20100410ELPVIAVJE_1.TES\n>                 | <\nhttp://rdfs.org/sioc/ns#has_creator\n> <\nhttp://webenemasuno.linkeddata.es/resource/elviajero/ContentProvider/ISIDORO_MERINO\n> estaría mal. Debería ser /Account/ACCISIDORO_MERINO\nFallo al hacer los sameAs (YA LO HE CORREGIDO EN EL PROGRAMA, PERO HAY QUE REPARSEAR. SE METE EN LA SIGUIENTE ITERACIÓN). Ejemplo:\nidNoticia | <\nhttp://www.w3.org/2002/07/owl#sameAs\n>            | <\nhttp://webenemasuno.linkeddata.es/resource/elviajero/geo.linkeddata.es/RESOURCE_MUNICIPIO_SEVILLA\n>, en lugar de ser <\nhttp://geo.linkeddata.es/resource/Municipio/Sevilla\n>\nCreo que he añadido mal los labels en la ontología (las etiquetas de idiomas). Consultar Boris. Efectivamente lo he hecho mal. Reparsear y cargar el nuevo repositorio.\nTodo aquello que pueda llevar label ha de llevarlo (autores, etc).\n(Creo que está todo)\nCorregir lo que ha dicho Alex del Pubby. No utilizar urlEncode, sino uriEncode. Sino Pubby no podrá utilizar las urls bien\nHe descubierto porqué no carga en Virtuoso. Algunas URIs son demasiado largas (4000 caracteres!)\nERROR de algunos videos: Error: Cannot convert node\nhttp://metadata.net/mpeg7/mpeg7.owl#mediaDuration\nto OntProperty. Fácil corregir, pero necesita otro parseo----> Resulta que es MediaDuration\n(bad practise!!!)\nParte de geolinkeddata no se ha cargado debido a fallo en el server. Volver a enriquecer los datos\nSILK no soporta acentos (Ejemplo ComunidadAutónoma = ComunidadAut%C3%B3noma y da un fallo de SPARQL Syntax).\n¿Por qué no está presente el contenido de muchas noticias? Investigar.\nURIs de links y videos no resolubles (tienen \"=\"). Sustituirlos.\nError al ver algunas URIs con el Pubby (no soporta los \"=\", por ejemplo)\nMejorar y automatizar el proceso de: comparación SILK, cargar al repositorio, buscar en DBPedia los que no estén en geolinkeddata y subir otra vez.\nAutomatizado en la medida de lo posible. Hay que ejecutar 4 scripts.\nBastante importante, aunque no se incluye en la versión inicial. HAY CICLOS EN EL GRAFO, cuando en OPM se dice que no debería. Esto hace que el grafo actual sea un grafo de provenance ilegal. Los ciclos que encontramos es porque se han modificado los datos y no tenemos acceso a dichas modificaciones. Por ejemplo: una noticia utiliza un enlace como \"más información\", y la otra noticia hace lo mismo con la primera. Para solucionarlo, habría que: comparar ambas noticias, ver cuál de ellas es anterior y decir que una usa a la otra de acuerdo con ello. La segunda lo que usaría sería una versión de la otra noticia, de manera que se rompería el ciclo y se haría secuencial. Pero el proceso de anotación sería más lento.\nValorar si incluir todas las accounts como pertenecientes a un solo OPM Graph, y decir que ese OPM graph soporta el perfil que hemos definido (así es como pone que ha de hacerse en la especificación).\nHablar con Prisacom sobre los fallos del parser en torno al 8% de los ficheros procesados... se pierde muchísima infomración (Unos 1500 archivos de noticias, fotos y videos)! --> Hemos hablado y \"verán lo que pueden hacer\"\nEn esta primera iteración no se ha mejorado la geolocalización de aristas, solo se ha hecho con las noticias (UPOS). A pesar de que la mayoría han sido publicadas y editadas en Madrid, hay que añadir las coordenadas de geolocalización. Conveniente hacer una query al repositorio y asignársela 1 a 1.\nCuando se parseen los posts, si tienen enlaces a wikipedia, extraer el concepto y enlazar con dbpedia.\nERROR Parser prisa : Problema al leer el fichero Exception: Exception: -error-if-this-is-used- is a required field. Ejemplo: E:\\Trabajo\\WEBN+1\\ARCHIVOS PRISACOM\\DatosXMLprisaCompleto\\200701\\23\\destinos\\20070123elpviadst_1.Tes.\nEjemplo error parsing : Se procesa E:\\Trabajo\\WEBN+1\\ARCHIVOS PRISACOM\\DatosXMLprisaCompleto\\200706\\15\\destinos\\20070615elpviadst_5.Ies\nProblema al leer el fichero Exception: Exception: Parsing Error : The content of element type \"hedline\" is incomplete, it must match \"(hl1,hl2*)\".\nDecir a Prisa que hay algunos errores de codificación en sus comentarios.\nUn post se puede referenciar por comentarios con el #comments detrás del título del post\nDetectar otros tipos de artefactos como presentaciones (ahora metidos como videos)\nRecargar los excels y las carpetas del nuevo mes\nLos de Prisa han vuelto a liarla con las URLS (en concreto todas las de Paco Nadal). Hablar con ellos y con Boris a ver cómo se puede hacer bien. También han cambiado algunos nice-titles.\nDoble label en algunos (muchos) content providers (Ej: Paco Nadal)\nHay que volver a procesar lo de la DBPedia: algunas etiquetas tienen acentos (Ámsterdam, Moscú, Sudán), mientras que las nuestras no. Explorar las etiquetas que tengan inglés.\nMeter la mejora del query framework. Es decir, rehacer las consultas por defecto como se habían hecho para el caso de uso desechado.\nAdecuar el API REST para manejar las queries anteriores.\nResulta que la copia de DBpedia que tenemos no tiene bien algunas coordenadas (por ejemplo, Sydney, República de Irlanda, etc.). Corregir en la medida de lo posible esto. (Cambiar a la copia que tiene Angelito)\nProcesar los nuevos UPOs (se han ampliado) cuando Luis solucione lo de los municipios. Esto conlleva a corregir también lo de las etiquetas de Silk y, ya de paso, las coordenadas (cambiar al endpoint nuevo).",
  "speaker": "SYSTEM",
  "uuid": "60aa3980-9108-4ee2-a451-d205cb4828dc"
}