{
  "message": "SYSTEM: Iswc10_gracia.pdf: Página 12\nthe experiment with DBpedia but searching for alternative suggested forms when the\nterm was not found in its initial form. The coverage of using DBpedia + Yahoo Spelling\nSuggestion service raised to a 78%.\nThough the reached 78% coverage indicates that DBpedia covers well the stud-\nied domain speciﬁc terminologies, other question arises, which is whether these pro-\nposed DBpedia groundings are acceptable according to human opinion or not. In order\nto answer that, we randomly selected 909 terms covered by DBpedia from the same\nglossaries used in the previous experiments. We asked 8 evaluators (experts in differ-\nent ﬁelds of environmental science) to assess the correctness of the possible meanings\ngiven by DBpedia. Each evaluator assessed between 200 and 300 terms. The grounding\nof each term was double-evaluated.\nWe counted as positive groundings those terms for which there were at least one\nsuitable meaning among the list of DBpedia results for such a term. We deﬁne accu-\nracy as the amount of positive groundings divided by the number of evaluated ground-\nings. The obtained average accuracy in the experiment was 83%. The observed inter-\nevaluator agreement was 85%, and Cohen’s kappa of inter-evaluator agreement [10]\nwas 0.47, which can be considered as “moderate” and gives us an idea of the difﬁculty\nof the task (if another expert does not fully agree with me, why a computer should?).\nFinally, although DBpedia exhibits a good coverage, we wonder whether it can be\nfurther improved with the addition of other resources. Table 2 shows the result of com-\nbining DBpedia with the other resources. From the data we conclude that the combined\nuse of OpenCyc and DBpedia increases the coverage signiﬁcantly (while the further\naddition of other sources has only a minor effect).\nKnowledge Source\nCoverage\nDBpedia and OpenCyc\n87%\nDBpedia and Watson\n73%\nDBpedia and WordNet\n72%\nDBpedia, OpenCyc, Watson, and WordNet\n88%\nTable 2. Combined Coverage of Knowledge Sources\nDiscussion The results of the grounding experiment show a high coverage degree of\nDBpedia (78%) when used to ground domain speciﬁc terminologies, as well as a high\naccuracy (85%) of the covered terms according to human opinion. These results sup-\nport the use of DBpedia as preferred source of knowledge for the grounding of the\nvocabulary involved in QR modelling. Notice that although the coverage of OpenCyc\nand DBpedia are comparable (see Table 1), there are other good reasons for choosing\nDBpedia, such as its multilingual capabilities: DBpedia contains data in up to 92 lan-\nguages while OpenCyc is monolingual in English, thus reducing its potential usage in\na multilingual modelling environment. Nevertheless, the combined use of both ontolo-\ngies further improves the coverage of English terms up to a 87%, as we have found out\nempirically.\n",
  "speaker": "SYSTEM",
  "uuid": "728f1e2d-e592-4ba2-8753-8224535af379"
}