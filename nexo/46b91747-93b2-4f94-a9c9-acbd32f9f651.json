{
  "message": "SYSTEM: Neon_2008_d2.2.2.pdf: Página 113\nD2.2.2 Methods and Tools Supporting Re-engineering\nPage 113 of 124\nCreate a new property\nCLASSES/INSTANCES have CLASSES/INSTANCES.\ne.g., Dolphins have acute eyesight and few natural enemies.\nIterate through the cross-product of the chunks in the two chunk-lists. For each pair, if both are classes, create\na property of the form Domain_has_Range. If both are instances, ﬁnd a suitable property and instantiate it\nwith those instances; if there is a class-instance mismatch or a suitable property does not exist, generate an\nerror. (Unlike the others above, this function requires the classes or instances to exist already—it would be\nimpossible to determine what to create otherwise.)\nChanges from CLOnE\nBecause CLOnE was designed to interpret without ambiguity a controlled language that users wrote de-\nliberately for the purposes of creating and editing ontologies, it imposed strict rules on the order of input\nsentences and the creation of resources in the ontology. For example, the function to create a subclass\nrequired the superclass to exist already, and the function to create an instance made the same stipulation\nabout the new instance’s class. NEBOnE, however, processes uncontrolled natural language so we cannot\nimpose such restrictions. The NEBOnE library does, however, reject function calls that would otherwise try\nto create an instance with the same name as an existing class, or the other way round.\n7.4\nEvaluation\nWe evaluated the accuracy of the lexical patterns using a corpus of 25 randomly selected wikipedia articles\nabout animals, such as the entries for rabbit, sheep etc. We ran SPRAT and examined the results in some\ndetail. In total, SPRAT generated 201 classes, 21 instances, and 98 synonyms, and 107 other properties.\nTable 7.1 shows the results for each type. Note that, unlike in traditional named entity recognition evaluation,\nwe use a strict method of scoring where a partially correct response, i.e. one where the span of the extracted\nentity is too short or too long, is considered as incorrect. This is because for ontology population, having an\nincorrect span is generally a more serious error than in named entity recognition.\n7.4.1\nSubclass Relations\nIn total, 163 subclass relations were generated, of which 79 were correct (48%). However, 15 of these were\nnot really useful classiﬁcations: e.g., turtle as subclass of local creature makes sense only in a very speciﬁc\ncontext. Of the subclass relations, 69 were found by the Hearst patterns, of which 58 were correct (84%).\nOf the incorrect relations generated, most contained at least one correct subclass out of a list. For example,\nin the phrase “by disturbing the natural state of pasture, sheep and other livestock. . . ” both natural state of\npasture and sheep were recognised as subclasses of livestock, of which the former is incorrect but the latter\nis correct. Some reﬁnement of the rules (for example, avoiding NPs containing of) could help improve the\nresults. The remaining patterns accounted for 94 of the subclass relations, of which 21 were correct (22%).\nRelation\nTotal Extracted\nCorrect\nPrecision\nSubclass\n163\n79\n48.5%\nInstance\n21\n10\n47.6%\nSynonym\n98\n47\n48.0%\nProperty\n107\n24\n22.4%\nTable 7.1: Results of relation extraction on 25 wikipedia documents\n2006–2008 c⃝Copyright lies with the respective authors and their institutions.\n",
  "speaker": "SYSTEM",
  "uuid": "46b91747-93b2-4f94-a9c9-acbd32f9f651"
}