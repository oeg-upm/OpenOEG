{
  "message": "SYSTEM: Sauper2007.pdf: Página 8\nM = Movie\nV = Video\nA = Audio\nE = Extras\nM This collection certainly offers some nostalgic \nfun, but at the end of the day, the shows themselves, \nfor the most part, just don't hold up. (5)\nV Regardless, this is a fairly solid presentation, but \nit's obvious there was room for improvement.  (7)\nA Bass is still robust and powerful. Fans should be \npleased with this presentation. (8)\nE The deleted scenes were quite lengthy, but only \nshelled out a few extra laughs. (4) \n(a) Sample labeled text from the multi-aspect sentiment corpus\n[R Big multifunction remote] with [R easy-to-\nread keys].   The on-screen menu is [M easy to \nuse] and you [M can rename the inputs] to one \nof several options (DVD, Cable, etc.).\nR = Remote\nM = Menu\nI = Inputs\nE = Economy\nV = Video\nS = Sound\nA = Appearance\nF = Features\nI bought this TV because the [V overall picture \nquality is good] and it's [A unbelievably thin].\n[I Plenty of inputs], including [I 2 HDMI ports], \nwhich is [E unheard of in this price range].\n(b) Sample labeled text from the Amazon multi-aspect summa-\nrization corpus\n[F All the ingredients are fresh], [V the sizes are \nhuge] and [V the price is cheap]. \nF = Food\nA = Atmosphere\nV = Value\nS = Service\nO = Overall\n[O This place rocks!]  [V Pricey, but worth it] .\n[A The place is a pretty good size] and\n[S the staff is super friendly].\n(c) Sample labeled text from the Yelp multi-aspect summarization\ncorpus\nFigure 3: Excerpts from the three corpora with the\ncorresponding labels. Note that sentences from the\nmulti-aspect summarization corpora generally focus\non only one or two aspects. The multi-aspect senti-\nment corpus has labels per paragraph rather than per\nsentence.\nModel (JointCM) setting refers to our full model de-\nscribed in Section 3, where content and task compo-\nnents are learned jointly.\nEvaluation Metrics\nFor multi-aspect sentiment\nranking, we report the average L2 (squared differ-\nence) and L1 (absolute difference) between system\nprediction and true 1-10 sentiment rating across test\ndocuments and aspects.\nFor the multi-aspect summarization task, we mea-\nsure average token precision and recall of the label\nassignments (Multi-label). For the Amazon corpus,\nwe also report a coarser metric which measures ex-\ntraction precision and recall while ignoring labels\n(Binary labels) as well as ROUGE (Lin, 2004). To\ncompute ROUGE, we control for length by limiting\nL1\nL2\nNoCM\n1.37\n3.15\nIndepCM\n1.28†*\n2.80†*\nJointCM\n1.25†\n2.65†*\nGold\n1.18†*\n2.48†*\nTable 3: The error rate on the multi-aspect sentiment\nranking. We report mean L1 and L2 between system\nprediction and true values over all aspects. Marked\nresults are statistically signiﬁcant with p < 0.05: *\nover the previous model and † over NoCM.\nF1\nF2\nPrec.\nRecall\nNoCM\n28.8%\n34.8%\n22.4%\n40.3%\nIndepCM\n37.9%\n43.7%\n31.1%†*\n48.6%†*\nJointCM\n39.2%\n44.4%\n32.9%†*\n48.6%†\nTable 4: Results for multi-aspect summarization on\nthe Yelp corpus. Marked precision and recall are\nstatistically signiﬁcant with p < 0.05: * over the\nprevious model and † over NoCM.\neach system to predict the same number of tokens as\nthe original labeled document.\nOur metrics of statistical signiﬁcance vary by\ntask.\nFor the sentiment task, we use Student’s t-\ntest. For the multi-aspect summarization task, we\nperform chi-square analysis on the ROUGE scores\nas well as on precision and recall separately, as\nis commonly done in information extraction (Fre-\nitag, 2004; Weeds et al., 2004; Finkel and Manning,\n2009).\n5\nResults\nIn this section, we present the results of the methods\non the tasks described above (see Tables 3, 4, and 5).\nBaseline Comparisons\nAdding a content model\nsigniﬁcantly outperforms the NoCM\nbaseline on\nboth tasks. The highest F1 error reduction – 14.7%\n– is achieved on multi-aspect summarization on the\nYelp corpus, followed by the reduction of 11.5% and\n8.75%, on multi-aspect summarization on the Ama-\nzon corpus and multi-aspect sentiment ranking, re-\nspectively.\nWe also observe a consistent performance boost\nwhen comparing against the IndepCM baseline.\nThis result conﬁrms our hypothesis about the ad-\n",
  "speaker": "SYSTEM",
  "uuid": "07ae8f04-c9ee-4e3a-b340-1048932de496"
}