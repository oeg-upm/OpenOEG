{
  "message": "SYSTEM: 2216-LT2013_Report_20130408.pdf: Página 78\n76\nwww.lt‐innovate.eu ‐ contact@lt‐innovate.eu\npay up front for site licenses anymore, and would prefer to pay based on usage. Machine translation lends\nitself nicely to this business model, as translations can be billed on a per‐word or per‐character basis. Google,\nfor example, charges $20 per million characters for the use of their Translate API (version 2.0). Pricing mod‐\nels vary. Some vendors charge on a per word basis, others per language, but in general expect some combi‐\nnation of the two for most offerings.\nMT Interoperability and Standards\nInteroperability between machine translation systems is desirable, but the systems are so similar in the way\nthey present themselves to outside users that transitioning from one system to another, from a system inte‐\ngration standpoint, isn’t that difficult. The APIs they expose to external systems all perform similar functions\n(request a translation, post‐edit a translation, etc.), and are not terribly complicated. Moreover, many ven‐\ndors have committed to implementing the TAUS web services reference API, as a way of providing a stan‐\ndard API that developers can use to interact with a variety of services. The reference API defines how to\nmake a variety of requests that are common to all human and machine translation systems.\nTranslation memory and corpora, if they are stored separately from the machine translation engine, in a\nstandard localisation file format (e.g. TMX), can easily be ported from system to system. Changing from one\ntranslation engine to another shouldn’t prevent customers from using the corpora they’ve built up over the\nyears.\nMeasuring and Benchmarking MT Quality\nMeasuring and benchmarking quality using an automated process remains a difficult challenge. While there\nare quality scales, such as the BLEU scale, they only provide a comparative measure of quality. This is impor‐\ntant because what’s really needed is an automated way to identify problem texts so they can be routed for\nhuman review and post‐edit. At present, the standard practice is to have human reviews look at a certain\npercentage of texts, or spend an assigned about of time reviewing a subset of a project. As the volume of\nmaterial being translated grows, it becomes easier for reviewers to miss defective translations. An automat‐\ned process that could identify problem texts without generating a large number of false positives would be\nhighly useful. Several companies are working on this problem and have fielded beta products. \nCost of MT Customisation\nThe cost of building custom or adapted translation engines remains quite high, mostly due to the cost of\nobtaining and pre‐processing high quality parallel texts with which to train the translation engine. Globally\nshared translation memories will encourage translation vendors to pool translations so they can be com‐\nbined to create large, high quality training corpora. While the technical challenge to building a shared trans‐\nlation memory has been solved, the primary challenge going forward is to encourage translation vendors to\nshare their translations by default. LSPs often resist doing this, so it will take time to make this a standard\npractice.\n4.11 Opportunities and Challenges for Translation Technology\nCloud Computing\nAs with other technology segments, the client/server model for Translation Technology is being quickly\nreplaced with a cloud (SaaS) based service model. The advantages of hosted services compared to customer\npremise software are extensive, and include:\n• Continual upgrades, with a continuous software release cycle (agile development)\n• Greatly reduced system administration and IT costs\n• High availability with highly redundant storage\n• Subscription based cost model, versus perpetual licenses, reduces up front capital cost\n",
  "speaker": "SYSTEM",
  "uuid": "21489512-f2f2-4dfc-942d-f114e1221c46"
}