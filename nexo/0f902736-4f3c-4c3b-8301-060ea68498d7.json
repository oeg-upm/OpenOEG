{
  "message": "SYSTEM: Science-2015-HirschbergManning-NLP.pdf: Página 3\nthis approach is also being attempted in spoken-\nchat systems designed to provide a sense of com-\npanionship for target audiences such as the elderly\nor individuals with dementia (22). In spoken dia-\nlogue, information about the speaker’s mental\nstate inferred from multimodal information can\nbe used to supplement the system’s knowledge of\nwhat the user is saying.\nThere are many challenges in building SDSs,\nin addition to the primary challenge of improv-\ning the accuracy of the basic ASR, DM, and TTS\nbuilding blocks and extending their use into less-\nrestricted domains. These include basic problems\nof recognizing and producing normal human\nconversational behaviors, such as turn-taking\nand coordination. Humans interpret subtle cues\nin speakers’ voices and facial and body gestures\n(where available) to determine when the speaker\nis ready to give up the turn versus simply pausing.\nThese cues, such as a filled pause (e.g., “um” or\n“uh”), are also used to establish when some\nfeedback from the listener is desirable, to indi-\ncate that he or she is listening or working on a\nrequest, as well as to provide “grounding” (i.e.,\ninformation about the current state of the con-\nversation). Non–humanlike latency often makes\nSDS burdensome, as users must wait seconds to\nreceive a system response. To address this, re-\nsearchers are exploring incremental processing\nof ASR, MT, and TTS modules, so that systems\ncan respond more quickly to users by beginning\nthese recognition, translation, and generation\nprocesses while the user is still speaking. Hu-\nmans can also disambiguate words such as “yeah”\nand “okay,” which may have diverse meanings—\nincluding agreement, topic shift, and even\ndisagreement—when spoken in different ways.\nIn successful and cooperative conversations, hu-\nmans also tend to entrain to their conversational\npartners, becoming more similar to each other in\npronunciation, word choice, acoustic and pro-\nsodic features, facial expressions, and gestures.\nThis tendency has long been used to subtly in-\nduce SDS users to employ terms that the system\ncan more easily recognize. Currently, research-\ners are beginning to believe that systems (parti-\ncularly embodied agents) should entrain to their\nusers in these different modalities, and some ex-\nperimental results have shown that users prefer\nsuch systems (23) and even think they are more\nintelligent (24). Open issues for DM have long been\nthe determination of how to architect the appro-\npriate dialogue flow for particular applications,\nwhere existing experimental data may be sparse\nand some aspects of the dialogue state may not yet\nhave been observed or even be observable from the\ndata. Currently, the most widely used approach is\nthe POMDP (partially observable Markov decision\nprocess), which attempts to identify an optimal\nsystem policy by maintaining a probability distri-\nbution over possible SDS states and updating this\ndistribution as the system observes additional\ndialogue behavior (25). This approach may make\nuse of the identification of dialogue acts, such as\nwhether the user input represents a question,\nstatement, or indication of agreement, for example.\nMachine reading\nThe printed word has great power to enlighten.\nMachine reading is the idea that machines could\nbecome intelligent, and could usefully integrate\nand summarize information for humans, by read-\ning and understanding the vast quantities of text\nthat are available.\nIn the early decades of artificial intelligence, many\nresearchers focused on the approach of trying to\nenable intelligent machines by manually building\nlarge structured knowledge bases in a formal logi-\ncal language and developing automated reasoning\nmethods for deriving further facts from this knowl-\nedge. However, with the emergence of the mod-\nern online world, what we mainly have instead is\nhuge repositories of online information coded in\nhuman languages. One place where this is true is\nin the scientific literature, where findings are still\nreported almost entirely in human language text\n(with accompanying tables and diagrams). How-\never, it is equally true for more general knowledge,\nwhere we now have huge repositories of infor-\nmation such as Wikipedia (26). The quantity of\nscientific literature is growing rapidly: For example,\nthe size of the U.S. National Library of Medicine’s\nMedline index has grown exponentially (27). At\nsuch a scale, scientists are unable to keep upwith\nthe literature, even in their narrow domains of\nexpertise. Thus, there is an increased need for\nmachine reading for the purposes of comprehend-\ning and summarizing the literature, as well as ex-\ntracting facts and hypotheses from this material.\nAn initial goal is to extract basic facts, most\ncommonly a relation between two entities, such\nas “child of” (for instance, Bill Clinton, Chelsea\nClinton). This is referred to as relation extrac-\ntion. For particular domain-specific relations,\nmany such systems have been successfully built.\nOne technique is to use handwritten patterns\nthat match the linguistic expression of relations\n(e.g., <PERSON>’s daughter, <PERSON>). Bet-\nter results can be obtained through the use of\nSCIENCE sciencemag.org\n17 JULY 2015 • VOL 349 ISSUE 6245\n263\ninformation access\nDialogue\nManagement\nSpeech in\nText-to-Speech\nSynthesis\nAutomatic Speech\nRecognition\nSp\nee\nch\n o\nut\nFig. 3. A spoken dialogue system.The three main\ncomponents are represented by rectangles; arrows\ndenote the flow of information.\n0.1\n0.3\n0.1\n0.4\n0.2\n-\n0.2\n0.2\n0.1\n0.1\n0.1\n-\n-\n0.2\n0.6\n0.1\n0.7\n0.1\n-\n-\nDie\nproteste\n0.2\n0.6\n0.1\n0.7\n0.1\n-\n-\n0.2\n0.6\n0.1\n0.7\n0.1\n-\n-\n0.4\n0.6\n0.2\n0.3\n0.4\n-\n-\nwaren\n0.4\n0.4\n0.3\n0.2\n0.3\n-\n-\n0.1\n0.3\n0.1\n0.7\n0.1\n-\n-\n0.2\n0.3\n0.1\n0.4\n0.2\n-\n-\nam\n0.5\n0.5\n0.9\n0.3\n0.2\n-\n-\n0.2\n0.6\n0.1\n0.4\n0.1\n-\n-\n0.2\n0.4\n0.1\n0.5\n0.2\n-\n-\nwochenende\n0.2\n0.6\n0.1\n0.5\n0.1\n-\n-\n0.2\n0.8\n0.1\n0.5\n0.1\n-\n-\n-\n0.4\n0.2\n0.3\n0.4\n0.2\n-\n-\n-\n-\neskaliert\n0.1\n0.6\n0.1\n0.7\n0.1\n-\n-\n-\n0.2\n0.6\n0.1\n0.7\n0.1\n-\n-\n0.2\n0.6\n0.1\n0.7\n0.1\n-\n-\n<EOS>\nThe\n0.2\n0.6\n0.1\n0.7\n0.1\n-\n-\n0.2\n0.1\n0.1\n0.7\n0.1\n-\n-\n-\n0.2\n0.6\n0.1\n0.7\n0.1\n-\n-\nThe\nprotests\n0.2\n0.6\n0.1\n0.7\n0.1\n-\n-\n0.4\n0.6\n0.1\n0.7\n0.1\n-\n-\n-\n0.2\n0.6\n0.1\n0.7\n0.1\n-\n-\nprotests\nescalated\n0.3\n0.6\n0.1\n0.7\n0.1\n-\n-\n0.2\n0.6\n0.1\n0.3\n0.1\n-\n0.2\n0.6\n0.1\n0.7\n0.1\n-\n-\nescalated\nover\n0.4\n0.4\n0.1\n0.7\n0.1\n-\n-\n0.1\n0.6\n0.1\n0.3\n0.1\n-\n0.1\n0.3\n0.1\n0.7\n0.1\n-\n-\nover\nthe\n0.2\n0.6\n0.1\n0.7\n0.1\n-\n-\n-\n0.2\n0.4\n0.1\n0.2\n0.1\n-\n0.2\n0.6\n0.1\n0.3\n0.1\n-\nthe\nweekend\n0.4\n0.6\n0.1\n0.7\n0.1\n-\n-\n-\n0.3\n0.6\n0.1\n0.5\n0.1\n-\n-\n0.4\n0.5\n0.5\n0.4\n0.1\n-\n-\nweekend\n<EOS>\n0.3\n0.5\n0.1\n0.7\n0.1\n-\n-\n-\n0.2\n0.6\n0.1\n0.7\n0.1\n-\n-\n0.2\n0.6\n0.1\n0.7\n0.1\n-\n-\nFig. 2. A deep, recurrent neural MT system (10). Initially trained on parallel sentences that translate each\nother, the model learns a representation of each word as a real-valued vector and internal parameter matrices so\nas to optimize translation quality.The trained network can then translate new sentences. Each arrow represents\na computation unit of a matrix multiplication followed by a nonlinear transformation; the small vectors shown in\nthe illustration might really be 1000-dimensional.The recurrent network first encodes the meaning of the source\nsentence (left side, blue). It maintains an internal state representing a partial sentence, which is updated after\neach new word is read (horizontal arrows). Having the upper network layers [mapped to by additional (upper\nvertical) computation arrows] makes this a deep recurrent network. Adding depth improves the ability of the\nmodel to learn, generalize, and remember. Once the end of the sentence (denoted by <EOS>) is reached\n(middle, dark blue), the network additionally starts to produce a word of translated output at each step from its\ninternal state (using a multiclass logistic regression–style model). During translation generation (right side,\ngreen), the last generated word is fed in as the input at each step. From the stored hidden state and this input,\nthe model calculates the next word of the translation. This process repeats until <EOS> is generated.\n",
  "speaker": "SYSTEM",
  "uuid": "0f902736-4f3c-4c3b-8301-060ea68498d7"
}