{
  "message": "SYSTEM: Barzilay2004.pdf: Página 4\ntion and multi-document summarization; While account-\ning for the full range of discourse and stylistic factors that\ninﬂuence the ordering process is infeasible in many do-\nmains, probabilistic content models provide a means for\nhandling important aspects of this problem. We demon-\nstrate this point by utilizing content models to select ap-\npropriate sentence orderings: we simply use a content\nmodel trained on documents from the domain of interest,\nselecting the ordering among all the presented candidates\nthat the content model assigns the highest probability to.\n4.2\nExtractive Summarization\nContent models can also be used for single-document\nsummarization. Because ordering is not an issue in this\napplication5, this task tests the ability of content models\nto adequately represent domain topics independently of\nwhether they do well at ordering these topics.\nThe usual strategy employed by domain-speciﬁc sum-\nmarizers is for humans to determine a priori what types\nof information from the originating documents should be\nincluded (e.g., in stories about earthquakes, the number\nof victims) (Radev and McKeown, 1998; White et al.,\n2001). Some systems avoid the need for manual anal-\nysis by learning content-selection rules from a collec-\ntion of articles paired with human-authored summaries,\nbut their learning algorithms typically focus on within-\nsentence features or very coarse structural features (such\nas position within a paragraph) (Kupiec et al., 1999).\nOur content-model-based summarization algorithm com-\nbines the advantages of both approaches; on the one\nhand, it learns all required information from un-annotated\ndocument-summary pairs; on the other hand, it operates\non a more abstract and global level, making use of the\ntopical structure of the entire document.\nOur algorithm is trained as follows. Given a content\nmodel acquired from the full articles using the method de-\nscribed in Section 3, we need to learn which topics (rep-\nresented by the content model’s states) should appear in\nour summaries. Our ﬁrst step is to employ the Viterbi al-\ngorithm to tag all of the summary sentences and all of the\nsentences from the original articles with a Viterbi topic\nlabel, or V-topic — the name of the state most likely to\nhave generated them. Next, for each state\n\u0001\nsuch that\nat least three full training-set articles contained V-topic\n\u0001 , we compute the probability that the state generates\nsentences that should appear in a summary. This prob-\nability is estimated by simply (1) counting the number\nof document-summary pairs in the parallel training data\nsuch that both the originating document and the summary\ncontain sentences assigned V-topic\n\u0001 , and then (2) nor-\nmalizing this count by the number of full articles con-\ntaining sentences with V-topic\n\u0001 .\n5Typically, sentences in a single-document summary follow\nthe order of appearance in the original document.\nDomain\nAverage Standard Vocabulary Token/\nLength Deviation\nSize\ntype\nEarthquakes\n10.4\n5.2\n1182\n13.2\nClashes\n14.0\n2.6\n1302\n4.5\nDrugs\n10.3\n7.5\n1566\n4.1\nFinance\n13.7\n1.6\n1378\n12.8\nAccidents\n11.5\n6.3\n2003\n5.6\nTable 1: Corpus statistics. Length is in sentences. Vo-\ncabulary size and type/token ratio are computed after re-\nplacement of proper names, numbers and dates.\nTo produce a length-\n\u0000 summary of a new article, the al-\ngorithm ﬁrst uses the content model and Viterbi decoding\nto assign each of the article’s sentences a V-topic. Next,\nthe algorithm selects those\n\u0000 states, chosen from among\nthose that appear as the V-topic of one of the article’s\nsentences, that have the highest probability of generating\na summary sentence, as estimated above. Sentences from\nthe input article corresponding to these states are placed\nin the output summary.6\n5\nEvaluation Experiments\n5.1\nData\nFor evaluation purposes, we created corpora from ﬁve\ndomains: earthquakes, clashes between armies and rebel\ngroups, drug-related criminal offenses, ﬁnancial reports,\nand summaries of aviation accidents.7 Speciﬁcally, the\nﬁrst four collections consist of AP articles from the North\nAmerican News Corpus gathered via a TDT-style docu-\nment clustering system. The ﬁfth consists of narratives\nfrom the National Transportation Safety Board’s database\npreviously employed by Jones and Thompson (2003) for\nevent-identiﬁcation experiments. For each such set, 100\narticles were used for training a content model, 100 arti-\ncles for testing, and 20 for the development set used for\nparameter tuning. Table 1 presents information about ar-\nticle length (measured in sentences, as determined by the\nsentence separator of\nReynar and Ratnaparkhi (1997)),\nvocabulary size, and token/type ratio for each domain.\n5.2\nParameter Estimation\nOur training algorithm has four free parameters: two that\nindirectly control the number of states in the induced con-\ntent model, and two parameters for smoothing bigram\nprobabilities.\nAll were tuned separately for each do-\nmain on the corresponding held-out development set us-\ning Powell’s grid search (Press et al., 1997). The parame-\nter values were selected to optimize system performance\n6If there are more than\n\u0001 sentences, we prioritize them by\nthe summarization probability of their V-topic’s state; we break\nany further ties by order of appearance in the document.\n7http://www.sls.csail.mit.edu/˜regina/struct\n",
  "speaker": "SYSTEM",
  "uuid": "d77721e5-3683-47fd-91be-4e2f47e4b9cd"
}