{
  "message": "SYSTEM: Sauper2007.pdf: Página 4\nT\ns\ny∗\nθ\nφ\nContent\nParameters\nTask\nParameters\nTask Labels\nText\nContent\nStructure\nFigure 2: A graphical depiction of the generative\nprocess for a labeled document at training time (See\nSection 3); shaded nodes indicate variables which\nare observed at training time. First the latent un-\nderlying content structure T is drawn. Then, the\ndocument text s is drawn conditioned on the content\nstructure utilizing content parameters θ. Finally, the\nobserved task labels for the document are modeled\ngiven s and T using the task parameters φ. Note that\nthe arrows for the task labels are undirected since\nthey are modeled discriminatively.\nwhere fN(·) and fE(·) are feature functions associ-\nated with CRF nodes and transitions respectively.\nAllowing the CRF to condition on the sentence\ntopic Ti permits predictions to be more sensitive to\ncontent. For instance, using the example from Ta-\nble 1, we could have a feature that indicates the word\n“pleased” conjoined with the segment topic (see Fig-\nure 1). These topic-speciﬁc features serve to disam-\nbiguate word usage.\nThis joint process, depicted graphically in Fig-\nure 2, is summarized as:\nP(T , s, y∗) = Pθ(T , s)Pφ(y∗|s, T )\n(3)\nNote that this probability decomposes into a\ndocument-level HMM term (the content component)\nas well as a product of CRF terms (the task compo-\nnent).\n3.3\nLearning\nDuring learning,\nwe would like to ﬁnd the\ndocument-level HMM parameters θ and the summa-\nrization task CRF parameters φ which maximize the\nlikelihood of the labeled documents. The only ob-\nserved elements of a labeled document are the docu-\nment text s and the aspect labels y∗. This objective\nis given by:\nLL(φ, θ) =\nX\n(s,y∗)∈DL\nlog P(s, y∗)\n=\nX\n(s,y∗)∈DL\nlog\nX\nT\nP(T , s, y∗)\nWe use the EM algorithm to optimize this objec-\ntive.\nE-Step\nThe E-Step in EM requires computing the\nposterior distribution over latent variables. In this\nmodel, the only latent variables are the sentence top-\nics T . To compute this term, we utilize the decom-\nposition in Equation (3) and rearrange HMM and\nCRF terms to obtain:\nP(T , s, y∗) = Pθ(T , s)Pφ(y∗|T , s)\n=\n n\nY\ni=1\nPθ(Ti|Ti−1)\nY\nw∈si\nPθ(w|Ti)\n!\n·\n n\nY\ni=1\nPφ(y∗\ni |si, Ti)\n!\n=\nn\nY\ni=1\nPθ(Ti|Ti−1)·\n Y\nw∈si\nPθ(w|Ti)Pφ(y∗\ni |si, Ti)\n!\nWe note that this expression takes the same form as\nthe document-level HMM, except that in addition to\nemitting the words of a sentence, we also have an\nobservation associated with the sentence sequence\nlabeling. We treat each Pφ(y∗\ni |si, Ti) as part of the\nnode potential associated with the document-level\nHMM. We utilize the Forward-Backward algorithm\nas one would with the document-level HMM in iso-\nlation, except that each node potential incorporates\nthis CRF term.\nM-Step\nWe perform separate M-Steps for content\nand task parameters. The M-Step for the content pa-\nrameters is identical to the document-level HMM\n",
  "speaker": "SYSTEM",
  "uuid": "f0210cfe-44f7-47b2-ac07-94d6893e9fdb"
}