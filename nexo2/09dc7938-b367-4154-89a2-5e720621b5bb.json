{
  "message": "SYSTEM: Deep_learning_applications_OEG.pdf: Página 10\nAlphaGo 2\nAlphaGo\nstudied 30 million positions from expert games  \nMonte-Carlo tree search with DNN trained by supervised learning.\nplayed against itself across 50 computers\nDRL\nUses two DNNs, both containing many layers with millions of neuron-like connections \n“policy network”, predicts the next move, and is used to narrow the search to \nconsider only the moves most likely to lead to a win \n(suggests intelligent moves to play)\n“value network”, reduces the depth of the search tree estimating the winner in each \nposition in place of searching all the way to the end of the game\n\n(evaluates the position that is reached)\nWhy is interesting a DL research on  games?\nDeep Blue beat Kasparov in 1997 being explicitly programmed to win at the game\nAlphaGowasn’t not preprogrammed to play Go\nSimilar techniques could be applied to other AI domains that require recognition of \ncomplex patterns, long-term planning and decision-making\nExamples are using medical images to make diagnoses or treatment plans, and \nimproving climate-change models\n10\nThe computer that mastered Go\n",
  "speaker": "SYSTEM",
  "uuid": "09dc7938-b367-4154-89a2-5e720621b5bb"
}