{
  "message": "SYSTEM: Wiki: Contents\n[\nhide\n]\n1\nT3.1 SEALS Benchmarking Methodology\n2\nKnowledge Web Benchmark Methodology (KWBM)\n3\nGuidelines for designing, executing and analysing Evaluations\n3.1\nMotivation\n3.2\nRelated work\n3.3\nTasks to perform\n3.4\nOpen issues\n4\nGuidelines for planning and executing SEALS evaluation campaigns\n4.1\nMotivation\n4.2\nRelated work\n4.3\nAnalysis of other evaluation campaigns\n4.4\nEvaluation campaign profiles\n4.5\nTasks to perform\n4.6\nOpen issues\n5\nPlanning and expected results\n5.1\nExpected results\n6\nOpen issues\nT3.1 SEALS Benchmarking Methodology\nThe Knowledge Web Benchmark Methodology (KWBM) is unsuitable for supporting the needs of SEALS. Due to the lack of time and resources, we cannot re-develop all the aspects of the KWBM. Thus, we will focus on two main features that we think are fundamental for carrying out the evaluations in SEALS:\nHow evaluations should be designed, executed and analysed (Experiment phase in KW Benchmark Process)\nHow the evaluation campaigns should be planned and executed (Organizational aspects for all phases of the KW Benchmark Process).\nKnowledge Web Benchmark Methodology (KWBM)\n\nKWBM is not suitable for the SEALS Evaluation Campaigns (SEALS-ECs). Some of the aspects that have to be revised:\nScope: KWBM was designed to support the organization, design and execution of benchmarks inside of an organization whereas SEALS-ECs require the participation of several organizations\nGrounding: The phases and processes of KWBM have been selected for existing bibliography from general benchmarking, experiemental software engineering and software measurement. Applied benchmarking in areas like databases or information retrieval has not been considered. In addition, some other parts of the specification of the KWBM, like the definition of the actors, are not based in existing bibliography or past experiences.\nActors: KWBM identify several groups’ profiles. Some of the suggested groups’ profiels should be revised and their obligations should be specified in more detail. Some issues already identify:\nSplit the benchmarking team in five teams: a team for organizing benchmark campaigns, a team for designing benchmarks, a team for executing benchmarks and a team for validating benchmarks, and a team for disseminating benchmark results\nTool developers (providers) should not be part of the teams for executing benchmarks and for validating benchmarks\nCompetences for each kind of participant should be specified in more detail.\nWe should define the distribution of SEALS main actors (end-users, domain-experts and tool-providers) in the different groups’ profiles of SEALS-BM\nModel: KWBM follows waterfall model in which the design process is embedded in each of the steps of each phase. However, this model is not flexible enough when the requirements for the evaluations are unclear. For WPs 10, 11 and 14, this is not a suitable model because they have to start the design of the evaluations or/and the organization of the campaigns from scratch and there are not good references available to be followed. For those reasons, we have to propose an alternative KWBP that follows an iterative prototype model\nConcreteness: Current version of the KWBM is not sufficiently well specified. Inputs, outputs and steps for each of the phases identified in the KW Benchmark Process (KWBP) need more detail in their specification. For instance the input/outputs, “benchmark proposal”, “experiment report”, “benchmarking report” and “monitoring report” are not defined. The same happens with the steps: “experiment definition”, “experiment execution” and “experiment analysis”\nGuidelines for designing, executing and analysing Evaluations\n\nMotivation\n\nOAEI is based on KWBM. However, OAEI has serious problems that have to be revised and the current KWBM does not help in the redesign of the future OAEIs. The design and execution of SEALS evaluations should ensure:\ncompatibility with SEALS Platform\nrelevance\nobjectiveness\nconsensus\nfairness\ntransparency\ncredibility\n...\nRelated work\n\nAnalyse practical evaluations in other areas:\nDatabases\nInformation retrieval\nAnalyse practical evaluations of semantic technologies\nOAEI\nSWS-Challenge\nInteroperability Benchmarking\nReasoning\nTasks to perform\n\nFind Bibliography\nAnalyse practical evaluations of semantic-based, Database-based and Information retrieval-based applications\nIdentify common \"best practices\" and design a first version of the SEALS Benchmark Methodology for the design, execution and analysis of SEALS evaluations\nApply the process in the first evaluation campaign\nImprove the methodology with lessons learnt in the first evaluation campaign\nValidate the methodology with:\nExisting approaches in the Software Engineering Area\nApplication of the methodology\nQuestionnaires to users\nOpen issues\n\nWhich criteria will be followed to choose between the evaluations in other areas?\nWhich criteria will be followed to choose between the evaluations of semantic technologies?\nGuidelines for planning and executing SEALS evaluation campaigns\n\nMotivation\n\nIn the SEALS project we will face the organization and execution of evaluation campaigns with certain characteristics:\nThere are different types of evaluation campaigns.\nWe need to cover the following needs of the SEALS project:\nInitiated by a consortium\nSupervised evaluations and results\nExecution of experiments using the SEALS Platform\nEnsure the effectiveness of the benchmarking activities\nMaximize relevance\nMaximize participation\nEnsure sustainability\nEnsure objectiveness\nEnsure consensus\nEnsure fairness\nEnsure transparency\nEnsure credibility\n...\nNowadays, it does not exist any methodology or guidelines that cover the different types of evaluation campaigns along these dimensions.\nRelated work\n\nThere are plenty of evaluation campaigns in different areas: databases, information retrieval (TREC, MUC, CLEF), Semantic Web (OAEI, SWS-C, IB).\n[FRM] I have doubts about \"the plenty amount of bibliography\". Usually, people do not report organizational aspects or if they do, they only provide a brief overview.\n[Raúl & FRM] We must look not just for research bibliography but from other sources: web pages, people, calls for participation, etc.\nAnalysis of other evaluation campaigns\n\nStart with the description of experiences related with the interoperability evaluation campaigns (Raúl: DONE)\nContinue with other evaluation campaigns.\nEvaluation campaign profiles\n\nThere are different evaluation campaign profiles:\nComparison\nImprovement\nChallenge\n...\nDifferent profiles will require different processes, actors, etc.\nTasks to perform\n\nFind research bibliography, web pages, people, calls for participation, etc\nAnalyse existing evaluation campaign approaches, both theoretical and practical. We will analyse all the approaches that we find.\nIdentify the different profiles of evaluation campaigns.\nPropose a methodology (process, roles, metrics, risks, code of conduct, etc.) that covers SEALS requirements, including both how to reach the goals and how to recover/which are the consequences if goals are not met.\nApply the methodology in the first evaluation campaign.\nImprove the methodology with lessons learnt in the first evaluation campaign.\nValidate the methodology by measuring its outcomes using a set of metrics.\nOpen issues\n\nUp to what extent can we cover the different evaluation campaign profiles under one methodology? We will see it when we have identified the different profiles of evaluation campaigns.\nPlanning and expected results\n\n(Include delivery dates of internal results and delivery dates of related SEALS results)\nExpected results\n\nWhat are we going to provide?\nWhat are we going to cover?\nWhat do we really need to provide to evaluation campaign organizers?\nOpen issues\n\nThe SEALS DoW does not explicitly mention the study of methodology aspects related with the design, execution and analysis of the SEALS evaluations. However, the improving of the design, execution and analysis of the OAEI's evaluations requires the support of the SEALS methodology\nWhich is the research value of the two methodologies? Which are the expected research outcomes?\nIs it feasible to develop the two methodologies with the expected effort in time? ([FRM]: Yes if we acknowledge that it is not possible to cover all methodology aspects in detail) ([Raúl] What are we going to cover?)\nHow will we ensure that the methodologies are used in the consortium? [FRM]: Being pragmatic in the design of the SEALS methodology and taking into account inputs from the SEALS consortium and the participants in the 1st evaluation campaign)",
  "speaker": "SYSTEM",
  "uuid": "18d8be7a-b29a-44b8-a6e1-bfa348806ccd"
}