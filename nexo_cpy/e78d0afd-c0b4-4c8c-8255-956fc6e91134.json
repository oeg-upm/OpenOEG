{
  "message": "SYSTEM: Sauper2007.pdf: Página 5\ncontent model: topic emission and transition dis-\ntributions are updated with expected counts derived\nfrom E-Step topic posteriors.\nThe M-Step for the task parameters does not have\na closed-form solution. Recall that in the M-Step,\nwe maximize the log probability of all random vari-\nables given expectations of latent variables. Using\nthe decomposition in Equation (3), it is clear that\nthe only component of the joint labeled document\nprobability which relies upon the task parameters is\nlog Pφ(y∗|s, T ). Thus for the M-Step, it is sufﬁcient\nto optimize the following with respect to φ:\nET |s, y∗log Pφ(y∗|s, T )\n=\nn\nX\ni=1\nETi|si, y∗\ni log Pφ(y∗\ni |si, Ti)\n=\nn\nX\ni=1\nK\nX\nk=1\nP(Ti = k|si, y∗\ni ) log Pφ(y∗\ni |si, Ti)\nThe ﬁrst equality follows from the decomposition\nof the task component into independent CRFs (see\nEquation (2)). Optimizing this objective is equiva-\nlent to a weighted version of the conditional likeli-\nhood objective used to train the CRF in isolation. An\nintuitive explanation of this process is that there are\nmultiple CRF instances, one for each possible hid-\nden topic T. Each utilizes different content features\nto explain the sentence sequence labeling. These in-\nstances are weighted according to the posterior over\nT obtained during the E-Step. While this objective\nis non-convex due to the summation over T, we can\nstill optimize it using any gradient-based optimiza-\ntion solver; in our experiments, we used the LBFGS\nalgorithm (Liu et al., 1989).\n3.4\nInference\nWe must predict a label sequence y for each sen-\ntence s of the document. We assume a loss function\nover a sequence labeling y and a proposed labeling\nˆy, which decomposes as:\nL(y, ˆy) =\nX\nj\nL(yj, ˆyj)\nwhere each position loss is sensitive to the kind of\nerror which is made. Failing to extract a token is\npenalized to a greater extent than extracting it with\nan incorrect label:\nL(yj, ˆyj) =\n\n\n\n\n\n0\nif ˆyj = yj\nc\nif yj ̸= NONE and ˆyj = NONE\n1\notherwise\nIn this deﬁnition, NONE represents the background\nlabel which is reserved for tokens which do not cor-\nrespond to labels of interest. The constant c repre-\nsents a user-deﬁned trade-off between precision and\nrecall errors. For our multi-aspect summarization\ntask, we select c = 4 for Yelp and c = 5 for Amazon\nto combat the high-precision bias typical of condi-\ntional likelihood models.\nAt inference time, we select the single labeling\nwhich minimizes the expected loss with respect to\nmodel posterior over label sequences:\nˆy = min\nˆy\nEy|sL(y, ˆy)\n= min\nˆy\nX\nj=1\nEyj|sL(yj, ˆyj)\nIn our case, we must marginalize out the sentence\ntopic T:\nP(yj|s) =\nX\nT\nP(yj, T|s)\n=\nX\nT\nPθ(T|s)Pφ(yj|s, T)\nThis minimum risk criterion has been widely used in\nNLP applications such as parsing (Goodman, 1999)\nand machine translation (DeNero et al., 2009). Note\nthat the above formulation differs from the stan-\ndard CRF due to the latent topic variables. Other-\nwise the inference task could be accomplished by\ndirectly obtaining posteriors over each yj state using\nthe Forward-Backwards algorithm on the sentence\nCRF.\nFinding ˆy can be done efﬁciently. First, we ob-\ntain marginal token posteriors as above. Then, the\nexpected loss of a token prediction is computed as\nfollows:\nX\nˆyj\nP(yj|s)L(yj, ˆyj)\nOnce we obtain expected losses of each token pre-\ndiction, we compute the minimum risk sequence la-\nbeling by running the Viterbi algorithm. The po-\ntential for each position and prediction is given by\n",
  "speaker": "SYSTEM",
  "uuid": "e78d0afd-c0b4-4c8c-8255-956fc6e91134"
}