{
  "message": "SYSTEM: ISWC2010_Andres_SemGroCrosLingFol.pdf: Página 10\n10\nAndr´es Garc´ıa-Silva et al.\nengine, and the search engine returns a ranked list of documents. In this list,\nnot all the documents are really relevant. It is then desired that the returned\nlist would contain as many relevant documents as possible. Moreover, since users\nusually focus their attention on the ﬁrst results, it is also desired that relevant\ndocuments would appear in the top positions of the ranked list.\nIn the conducted experiment, evaluators identiﬁed which DBpedia resources\nwere (highly) related to a given tag within the corresponding semantic context,\ni.e. for the annotated picture, and the presented approaches attempt to retrieve\nsuch resources. Based on the above IR problem deﬁnition, a tag (within a speciﬁc\nsemantic context) can be considered as a query, and the set of DBpedia resources\nrelated to such tag can be interpreted as the set of relevant documents.\nAiming to empirically compare the performance of the investigated approaches,\nwe made use of metrics widely used in the IR ﬁeld. The ﬁrst utilized metric is\nprecision. In our context, for a given approach and tag, we deﬁne precision as\nthe fraction of the DBpedia resources retrieved by the approach that are re-\nlated to the tag. Since, in general, our ﬁnal goal is to obtain a single related\nresource, we measure average precision values taking into account only the ﬁrst\nresults returned by the approaches. In the literature, this measure is called pre-\ncision at one or P@1 [22]. For more exhaustive comparisons, we also compute\nP@N, with N = 2, 3, 4, 5. Furthermore, averaging the sum of P@N values by\nthe number of related resources, we deﬁne the mean average precision or MAP.\nThe second utilized metric is recall. For a given approach and tag, we deﬁne\nrecall as the fraction of DBpedia resources related to the tag that are suc-\ncessfully retrieved by the approach. Similarly to precision, we also take into\nconsideration recall at N or R@N, with N = 1, 2, 3, 4, 5. Precision and recall\nare metrics that capture diﬀerent aspects of the set of retrieved documents. In\nmany situations, the use of a single measure combining precision and recall is\nappropriate. Thus, as our third metric, we propose to use the well known F\nmeasure, which is basically the weighted harmonic mean of precision and recall:\nF = 2 · precision · recall/(precision + recall).\nThe previous metrics have been (and are being) used extensively by the IR\ncommunity. However, they have limitations, and should be complemented by\nother metrics. Hence, for example, precision and recall do not take into account\nthe usefulness of a document based on its position in the result list. To address\nthis issue, we also compute NDCG [23] and MRR [24]. NDCG (Normalized\nDiscounted Cumulative Gain) penalizes relevant DBpedia resources appearing\nlower in a result list. This penalization is based on a relevance reduction loga-\nrithmically proportional to the position of the relevant resources. MRR (Mean\nReciprocal Rank), on the other hand, is the average of the ”reciprocal ranks”\nfor a sample of tags, where the reciprocal rank of a result list is the multiplica-\ntive inverse of the position of the ﬁrst relevant DBpedia resource retrieved. To\nconclude, note that all explained metrics have range [0, 1], and that the higher\nthe metric value, the better performance of the retrieval approach.\n",
  "speaker": "SYSTEM",
  "uuid": "60226f1a-b8e6-4ea9-84eb-1e06333a2b3b"
}