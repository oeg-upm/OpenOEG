{
  "message": "SYSTEM: Barzilay2004.pdf: Página 1\nCatching the Drift: Probabilistic Content Models, with Applications to\nGeneration and Summarization\nRegina Barzilay\nComputer Science and AI Lab\nMIT\nregina@csail.mit.edu\nLillian Lee\nDepartment of Computer Science\nCornell University\nllee@cs.cornell.edu\nAbstract\nWe consider the problem of modeling the con-\ntent structure of texts within a speciﬁc do-\nmain, in terms of the topics the texts address\nand the order in which these topics appear.\nWe ﬁrst present an effective knowledge-lean\nmethod for learning content models from un-\nannotated documents, utilizing a novel adap-\ntation of algorithms for Hidden Markov Mod-\nels. We then apply our method to two com-\nplementary tasks: information ordering and ex-\ntractive summarization. Our experiments show\nthat incorporating content models in these ap-\nplications yields substantial improvement over\npreviously-proposed methods.\n1\nIntroduction\nThe development and application of computational mod-\nels of text structure is a central concern in natural lan-\nguage processing. Document-level analysis of text struc-\nture is an important instance of such work.\nPrevious\nresearch has sought to characterize texts in terms of\ndomain-independent rhetorical elements, such as schema\nitems (McKeown, 1985) or rhetorical relations (Mann\nand Thompson, 1988; Marcu, 1997). The focus of our\nwork, however, is on an equally fundamental but domain-\ndependent dimension of the structure of text: content.\nOur use of the term “content” corresponds roughly\nto the notions of topic and topic change.\nWe desire\nmodels that can specify, for example, that articles about\nearthquakes typically contain information about quake\nstrength, location, and casualties, and that descriptions\nof casualties usually precede those of rescue efforts. But\nrather than manually determine the topics for a given\ndomain, we take a distributional view, learning them\ndirectly from un-annotated texts via analysis of word\ndistribution patterns.\nThis idea dates back at least to\nHarris (1982), who claimed that “various types of [word]\nrecurrence patterns seem to characterize various types of\ndiscourse”. Advantages of a distributional perspective in-\nclude both drastic reduction in human effort and recogni-\ntion of “topics” that might not occur to a human expert\nand yet, when explicitly modeled, aid in applications.\nOf course, the success of the distributional approach\ndepends on the existence of recurrent patterns. In arbi-\ntrary document collections, such patterns might be too\nvariable to be easily detected by statistical means. How-\never, research has shown that texts from the same domain\ntend to exhibit high similarity (Wray, 2002). Cognitive\npsychologists have long posited that this similarity is not\naccidental, arguing that formulaic text structure facilitates\nreaders’ comprehension and recall (Bartlett, 1932).1\nIn this paper, we investigate the utility of domain-\nspeciﬁc content models for representing topics and\ntopic shifts.\nContent models are Hidden Markov\nModels (HMMs) wherein states correspond to types\nof information characteristic to the domain of in-\nterest (e.g., earthquake magnitude or previous earth-\nquake occurrences), and state transitions capture possible\ninformation-presentation orderings within that domain.\nWe ﬁrst describe an efﬁcient, knowledge-lean method\nfor learning both a set of topics and the relations be-\ntween topics directly from un-annotated documents. Our\ntechnique incorporates a novel adaptation of the standard\nHMM induction algorithm that is tailored to the task of\nmodeling content.\nThen, we apply techniques based on content models to\ntwo complex text-processing tasks. First, we consider in-\nformation ordering, that is, choosing a sequence in which\nto present a pre-selected set of items; this is an essen-\ntial step in concept-to-text generation, multi-document\nsummarization, and other text-synthesis problems. In our\nexperiments, content models outperform Lapata’s (2003)\nstate-of-the-art ordering method by a wide margin — for\none domain and performance metric, the gap was 78 per-\ncentage points. Second, we consider extractive summa-\n1But “formulaic” is not necessarily equivalent to “simple”,\nso automated approaches still offer advantages over manual\ntechniques, especially if one needs to model several domains.\n",
  "speaker": "SYSTEM",
  "uuid": "f2d65a5c-d10d-462f-8c67-12e6b7333474"
}