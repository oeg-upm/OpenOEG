{
  "message": "SYSTEM: Wiki: Contents\n[\nhide\n]\n1\nLugar para anunciar proyectos fin de carrera\n2\nPropuesta prácticum OREX (Oficina Relaciones Externas)\n3\nOntologies\n4\nSemantic Web\n5\nSemantics and Big Data\n6\nSemantic e-Science\n6.1\nTítulo del trabajo: Creación de un framework para la monitorización y adopción de buenas prácticas de Ciencia Abierta en repositorios software\n6.2\nTítulo del trabajo: Creación grafos de conocimiento a partir de código de software científico en distintos lenguajes\n6.3\nTítulo del trabajo: Evaluación de reconocimiento de entidades nombradas de Software a partir de archivos readme\n6.4\nTítulo del trabajo:  Creación de un framework para comparar proyectos de software científico similares\n6.5\nTítulo del trabajo: Creación de APIs para consultar datos de grafos de conocimientos de manera intuitiva\n6.6\nTítulo del trabajo: Creación de grafos de conocimiento a partir de contenedores para facilitar su monitorización\n6.7\nTítulo del trabajo: Creación de un servicio de enriquecimiento de metadatos de software a partir de repositorios de código científico\n6.8\nTítulo del trabajo: Creación de grafos de conocimiento de metadatos de software a partir de múltiples fuentes de datos\n6.9\nTítulo del trabajo: Creación de un framework para el mantenimiento, curación e integración continua de catálogos de software\n6.10\nTítulo del trabajo: Detección de agradecimientos en proyectos software\n7\nCitizen Science\n8\nLinguistic Linked Data\n9\nOld projects\n9.1\nOntologies\n9.2\nSemantic Web\n9.3\nSemantics and Big Data\n9.4\nSemantics and Big Data\n9.5\nSemantic e-Science\n9.6\nCitizen Science\n9.7\nLinguistic Linked Data\nLugar para anunciar proyectos fin de carrera\n\nhttp://www.fi.upm.es/intranet_p/?pagina=3\nUso de\nhttps://github.blog/2020-10-01-keeping-your-data-pipelines-healthy-with-the-great-expectations-github-action/\nen la asignatura de Web Semántica y Linked Data para la corrección de assignments basadso en datos.\nPropuesta prácticum OREX (Oficina Relaciones Externas)\n\nRediseño y mejora de la usabiliad del sistema online de evaluación de ontologías OOPS!\nOOPS! (OntOlogy Pitfall Scanner!\nhttp://oops.linkeddata.es\n) es un sistema online para la evaluación de ontologías. Este sistema es una herramienta independiente del editor de ontologías empleado y realiza un mayor número de comprobaciones automáticas que el resto de herramientas existentes (33 de las 41 pitfalls definidas en el catálogo). La independencia de editores específicos así como su sencilla interfaz hacen del sistema una herramienta accesible así como fácil de utilizar y comprender para usuarios no expertos en tecnologías semánticas. Además, las funcionalidades de OOPS! pueden ser integradas en sistemas externos a través del servicio web disponible. Este proyecto consiste en realizar un rediseño y mejora de la interfaz de usuario añadiendo nuevas funcionalidades como guías para resolver pitfalls o edición de informes de evaluación.\nLos objetivos concreto del trabajo son:\nMejora del diseño de la interfaz web actual\nIntegración de nueva información (resolver pitfalls)\nImplementación de nuevas funcionalidades (editar informes de evaluación)\nImplementación y despliegue del servicio web mejorado\nFunciones a desarrollar:\nAnálisis de la web actual de OOPS! y sus funcionalidades\nAnálisis de las nuevas funcionalidades\nDiseño de nueva interfaz e interacción con el usuario incluyendo nuevas funcionalidades\nImplementación de la interfaz de usuario\nPruebas\nDocumentación\nRequisitos:\nDiseño web\nHTML + CSS\nJAVA\nJ2SE/J2EE\nJSP\nOntologies\n\nSemantic Web\n\nSemantics and Big Data\n\nSemantic e-Science\n\nTítulo del trabajo: Creación de un framework para la monitorización y adopción de buenas prácticas de Ciencia Abierta en repositorios software\n\nFecha propuesta: Sept 2022.\nTipo: TFG\nResumen general del trabajo: Actualmente es práctica común para los científicos agruparse en organizaciones, que desarrollan herramientas en distintos proyectos de investigación. Sin embargo, estos proyectos no suelen seguir buenas prácticas de Ciencia Abierta (declarar licencia, cómo citar el software, documentación, etc.). Este TFG utilizará trabajo existente para crear catálogos software con el fin de crear analíticas de las prácticas actuales en una organización, y creará sugerencias para guiar a los autores hacia su adopción.\nLista de objetivos concretos:\nCreación de validadores que verifiquen cómo se cumplen las buenas prácticas de Ciencia Abierta en una organización.\nCreación de un portal para visualizar los resultados de manera resumida.\nCreación de sugerencias para aquellos repositorios que se han analizado de acuerdo a las guías propuestas por la UPM.\nCreación de portales de 2 organizaciones distintas.\nDesglose de la dedicación total del trabajo en horas (324 horas en los Grados)\nFamiliarización con SOCA (\nhttps://github.com/oeg-upm/soca\n) (20%)\nFamiliarización con SOMEF (\nhttps://github.com/KnowledgeCaptureAndDiscovery/somef/\n) (20%).\nDiseño de la visualización y relación con las guías UPM (20%)\nProgramación del framework y su validación (40%)\nDocumentación y memoria(20%)\nConocimientos previos recomendados para hacer el trabajo\nPython\nRDF\nJSON\nTítulo del trabajo: Creación grafos de conocimiento a partir de código de software científico en distintos lenguajes\n\nFecha propuesta: Sept 2022.\nTipo: TFG\nEstado:No empezado, propuesto\nResumen general del trabajo: El software que se desarrolla en investigaciones científicas es cada vez más importante para reproducir y reutilizar los resultados descritos en artículos académicos. Dicho software se desarrolla en distintos lenguajes de programación (C, Java, Python, etc.), que se describen con metadatos a la hora de crear paquetes para ser reutilizados. En este TFG se desarrollarán métodos para interpretar los metadatos de los lenguajes más utilizados, reutilizando herramientas existentes, y se integrarán en un grafo de conocimiento.\nLista de objetivos concretos:\nIdentificación de los lenguajes de programación a tratar, prácticas comunes de empaquetado y sus metadatos correspondientes\nIdentificación de herramientas que sirvan para leer cada uno de los metadatos de dichos lenguajes\nIntegración de las herramientas en el software metadata extraction framework (SOMEF)\nhttps://github.com/KnowledgeCaptureAndDiscovery/somef/\nCreación de trazas de provenance para indicar cómo se han extraído cada uno de los metadatos.\nIncorporación de los metadatos a los grafos de conocimientos generados por la herramienta SOMEF.\nDesglose de la dedicación total del trabajo en horas (324 horas en los Grados)\nIdentificación de prácticas comunes en distintos lenguajes de programación (incluyendo Python, Java, C, Javascript, C++, etc.)\nIntegración de las herramientas y parsers en SOMEF (40%)\nExportar resultados en un grafo de conocimiento (10%)\nDocumentación y memoria (20%)\nConocimientos previos recomendados para hacer el trabajo\nPython\nRDF\nTítulo del trabajo: Evaluación de reconocimiento de entidades nombradas de Software a partir de archivos readme\n\nTipo: TFG\nResumen general del trabajo:  El software que se desarrolla en investigaciones científicas es cada vez más importante para reproducir y reutilizar los resultados descritos en artículos académicos. Dicho software se suele describir en archivos README y documentación dedicada, de manera fragmentada, heterogénea y no accesible programáticamente. Este TFG extenderá trabajos de otros alumnos y definirá métodos para la evaluación de técnicas existentes de reconocimiento de entidades nombradas sobre READMEs para generar grafos de conocimiento.\nLista de objetivos concretos:\nCreación de un framework de evaluación de entidades nombradas en software\nExtensión del framework para evaluar los grafos de conocimiento generados\nCreación de un gold standard con 40-50 repositorios característicos\nEvaluación de las herramientas de extracción existentes con el framework\nIdentificación de puntos de mejora de las herramientas existentes\nDesglose de la dedicación total del trabajo en horas (324 horas en los Grados):\nFamiliarización con Ner4Soft y despliegue del servicio (20%)\nConstrucción de un corpus de hasta 40-50 repositorios para evaluar los resultados (20%)\nDefinición de grafos de conocimiento para comparar resultados (20%)\nCreación de resúmenes de las evaluaciones para comparar resultados (20%).\nMemoria y documentación (20%)\nConocimientos previos recomendados para hacer el trabajo:\nPython\nRDF\nProcesamiento de Lenguaje Natural\nTítulo del trabajo:  Creación de un framework para comparar proyectos de software científico similares\n\nTipo: TFG\nEstado: No empezado, propuesto\nResumen general del trabajo: Repositorios de código como GitHub tienen millones de herramientas de software libre para diversas actividades. Sin embargo, es difícil encontrar y comparar herramientas con un propósito similar. En este TFG se aplicarán distintas métricas de similitud de software científico para facilitar su comparación basados en su documentación, código o metadatos.\nLista de objetivos concretos:\nAplicaciones de métricas de similitud basados en metadatos.\nAplicaciones de métricas de similitud basados en texto (embeddings)\nAplicaciones de métricas de similitud basados en código (embeddings)\nAplicaciones de métricas de similitud basados en métodos híbridos\nConstrucción de un corpus de repositorios para demostrar los resultados\nCreación de una página web para visualizar los resultados\nDesglose de la dedicación total del trabajo en horas (324 horas en los Grados):\nFamiliarización con técnicas de extracción automática de grafos de conocimiento de metadatos (SOMEF) (10%)\nFamiliarización con técnicas de similitud de texto basado en embeddings con modelos pre-entrenados (10%)\nFamiliarización con técnicas de similitud de código basados en embeddings con modelos pre-entrenados (10%)\nCreación de un framework que extraiga similitudes dados dos o más herramientas (20%)\nCreación de un corpus y demostración (30%)\nDocumentación y memoria (20%)\nConocimientos previos recomendados para hacer el trabajo:\nPython\nRDF\nJSON\nHuggingface y procesamiento del lenguaje natural\nTítulo del trabajo: Creación de APIs para consultar datos de grafos de conocimientos de manera intuitiva\n\nTipo: TFG\nEstado: No empezado, propuesto\nResumen general del trabajo: Los grafos de conocimiento se han vuelto una tecnología popular entre instituciones y empresas para representar y agregar datos de distintos dominios. Sin embargo, una gran cantidad de desarrolladores no conocen las tecnologías semánticas como SPARQL. Este TFG utilizará una herramienta para crear APIs sobre grafos de conocimiento, y la actualizará según la última versión de OpenAPI para que se generen servidores de manera automática.\nLista de objetivos concretos:\nActualización de la última versión de OBA con OpenAPI de manera manual, incluyendo la realización de queries.\nConstrucción del servidor de manera automática\nDespliegue con contenedores Docker\nDesglose de la dedicación total del trabajo en horas (324 horas en los Grados)\nFamiliarización con OBA (\nhttps://github.com/KnowledgeCaptureAndDiscovery/OBA/\n) (10%)\nFamiliarización con OpenAPI (5%)\nFamiliarización con JSON-LD y JSON-LD framing (5%)\nActualización de OBA con la última versión de OpenAPI (50%)\nComprobación del funcionamiento del servidor (10%)\nDocumentación y memoria (20%)\nConocimientos previos recomendados para hacer el trabajo:\nPython\nRDF\nJSON\nJava\nOpenAPI\nTítulo del trabajo: Creación de grafos de conocimiento a partir de contenedores para facilitar su monitorización\n\nTipo: TFG\nEstado: No empezado, propuesto\nResumen general del trabajo: Los contenedores software (por ejemplo, Docker) se han convertido en una herramienta indispensable para el despliegue de aplicaciones en distintas infraestructuras. Sin embargo, monitorizar los contenedores desplegados en una organización es una tarea que conlleva esfuerzo para los administradores de sistemas. En este TFG se extenderá trabajo anterior sobre la descripción de dependencias en contenedores para monitorizar los contenedores desplegados en una organización.\nLista de objetivos concretos:\nAnálisis para representar de contenedores software.\nExtensión de c2t (\nhttps://github.com/osoc-es/c2t\n)  para:\n1) enlazar con repositorios de vulnerabilidades comunes en paquetes software\n2) exportar los contenedores desplegados en un servidor.\nCreación de una página web que utilice el grafo de conocimiento para visualizar la monitorización producida.\nDesglose de la dedicación total del trabajo en horas (324 horas en los Grados):\nFamiliarización con c2t (10%)\nRepresentación de todos los contenedores en una organización (10%)\nExtensión del paquete c2t para describir y exportar grafos de conocimiento de contenedores. (40%)\nCreación de un portal de visualización de monitorización (20%)\nDocumentación y memoria (20%)\nConocimientos previos recomendados para hacer el trabajo:\nPython\nRDF\nJSON\nTítulo del trabajo: Creación de un servicio de enriquecimiento de metadatos de software a partir de repositorios de código científico\n\nTipo: TFG\nEstado:No empezado, propuesto\nResumen general del trabajo: Software Heritage es un repositorio de software científico cuyo objetivo es almacenar de manera sistemática todo el código existente. Sin embargo, dicho código raramente está descrito con metadatos que faciliten su descubrimiento o reutilización. En este TFG, el alumno explorará las APIs de software Heritage para crear un servicio de enriquecimiento de metadatos software que derive nuevo conocimiento sobre software almacenado en la plataforma.\nLista de objetivos concretos:\nExplorar la API de Software Heritage, y cómo recuperar contenidos dependiendo de los releases e identificadores utilizados\nIntegrar los servicios de descripción de software desarrollados en UPM para enriquecer los registros de Software Heritage.\nDefinición de un marco de evaluación para medir el enriquecimiento producido por el servicio.\nCorrección de fallos en las herramientas desarrolladas en la UPM\nCreación de un grafo de conocimiento de ejemplo a partir de 100 repositorios de Software Heritage.\nDesglose de la dedicación total del trabajo en horas (324 horas en los Grados):\nFamiliarización con la API de Software Heritage (10%)\nFamiliarización con SOMEF (\nhttps://github.com/KnowledgeCaptureAndDiscovery/somef/\n)  (10%)\nIntegración de resultados con Morph-KGC (20%)\nCreación de un grafo de conocimiento de 100 repositorios (20%)\nEvaluación de los resultados (10%)\nCreación de consultas de ejemplo y explotación de los resultados (10%)\nDocumentación y memoria (20%)\nConocimientos previos recomendados para hacer el trabajo:\nJSON\nPython\nRDF\nTítulo del trabajo: Creación de grafos de conocimiento de metadatos de software a partir de múltiples fuentes de datos\n\nTipo: TFG\nEstado:No empezado, propuesto\nResumen general del trabajo:  El software que se desarrolla en investigaciones científicas es cada vez más importante para reproducir y reutilizar los resultados descritos en artículos académicos. Sin embargo, los metadatos de los repositorios software (cruciales para su reutilización) se encuentran desperdigados en READMEs, wikis, notebooks e imágenes de contenedores. En este TFG se propone el diseño y la creación de un grafo de conocimiento de software asociado a repositorios de código científico. El TFG incluirá herramientas existentes para capturar metadatos, personas, licencias, notebooks, publicaciones y hasta imágenes de contenedores.\nLista de objetivos concretos:\nDiseño de representación del conocimiento en grafos de software científico\nDiseño de las preguntas a contestar por el grafo.\nImplementación de un paquete en Python para integrar todas las fuentes de conocimiento usando Morph-KGC.\nCreación de grafos de conocimiento de dos organizaciones.\nDesglose de la dedicación total del trabajo en horas (324 horas en los Grados):\nFamiliarización con inspect4py (10%)\nFamiliarización con SOMEF (\nhttps://github.com/KnowledgeCaptureAndDiscovery/somef/\n)  (10%)\nFamiliarización con c2t (10%)\nDiseño del grafo a integrar, incluyendo personas, licencias, publicaciones, etc. (20%)\nImplementación del framework de integración usando Morph-KGC (30%)\nMemoria y documentación (20%)\nConocimientos previos recomendados para hacer el trabajo:\nRDF\nPython\nTítulo del trabajo: Creación de un framework para el mantenimiento, curación e integración continua de catálogos de software\n\nTipo: TFG\nEstado: No empezado, propuesto\nResumen general del trabajo: Mantener catálogos de software de manera actualizada es un proceso costoso. En TFGs anteriores se ha desarrollado un método para crear portales y grafos de conocimiento personalizados a partir de una organización de GitHub. Sin embargo, el proceso se repite de principio a fin en cada ocasión, incluso en aquellos repositorios que no han cambiado. En este TFG se desarrollará un framework para monitorización de catálogos de software, optimizando el proceso existente y actualizando solo aquellos repositorios que hayan cambiado o tengan diferencias. Además, se tendrán en cuenta los requisitos del usuario en cuanto a los repositorios a incluir en el portal final.\nLista de objetivos concretos:\nCreación de una metodología para actualizar entradas sólo en caso de que haya diferencias.\nCreación de un método para detectar entradas que necesiten actualización.\nCuracion de datos basada en usuarios\nDesglose de la dedicación total del trabajo en horas (324 horas en los Grados)\nFamiliarización con SOCA (\nhttps://github.com/oeg-upm/soca\n) (20%)\nFamiliarización con SOMEF (\nhttps://github.com/KnowledgeCaptureAndDiscovery/somef/\n)  (10%)\nExtensión de SOCA basada en diferencias (50%)\nDocumentación y memoria (20%)\nConocimientos previos recomendados para hacer el trabajo\nPython\nJSON\n—-------------\nTítulo del trabajo: Detección de agradecimientos en proyectos software\n\nTipo: TFG\nEstado: No empezado, propuesto\nResumen general del trabajo (a mejorar): Multitud de repositorios de software científico se generan gracias a la financiación de proyectos de distintas organizaciones. En artículos científicos es común encontrar una lista de agradecimientos con los proyectos y organizaciones financiadoras, pero es difícil detectarlas automáticamente.  En este TFG se propone encontrar la lista de organizaciones y proyectos que financiaron un software científico, partiendo de su repositorio de código.\nLista de objetivos concretos:\nEncontrar las citas asociadas a un repositorio software, o los agradecimientos en caso de que existan.\nExplorar artículos asociados al software para extraer agradecimientos\nEncontrar los proyectos y entidades que han financiado el software mediante extracción de entidades nombradas.\nEvaluación de resultados.\nRepresentar los resultados en RDF\nDesglose de la dedicación total del trabajo en horas (324 horas en los Grados)\nFamiliarización con SOMEF (\nhttps://github.com/KnowledgeCaptureAndDiscovery/somef/\n) (10%)\nExtractor de PDF (10%)\nReconocimiento de entidades nombradas (40%)\nProgramación del framework y su evaluación (20%)\nMemoria y documentación (20%)\nConocimientos previos recomendados para hacer el trabajo\nPython\nRDF\nJSON\nCitizen Science\n\nLinguistic Linked Data\n\nOld projects\n\nBelow are the projects defined on or before 2020\n2.\nCorrector de texto para Twitter\nUno de los principales problemas a la hora de analizar textos procedentes de las redes sociales son los errores gramaticales que suelen contener, así como la presencia de elementos propios de este tipo de foros que requieren de un procesamiento especial (i.e. hashtags, formas de mencionar a otros usuarios o emoticonos y expresiones habituales en las redes). Además, la limitación en el número de caracteres existente en Twitter la convierte en un caso singular dentro de las redes sociales, ya que los usuarios tienden a adaptar su forma de escribir a dicha limitación, omitiendo palabras y creando acortaciones que dificultan el uso de herramientas genéricas de procesamiento del lenguaje, especialmente a la hora de realizar tareas como el Análisis de Sentimientos. El objetivo de este practicum será la creación de un corrector que \"normalice\" tweets en español.\nEl objetivo del trabajo es la creación de un módulo software que:\nCorrija tweets (deletreo, faltas de ortografía...)\nProcese emoticonos y deduzca su significado en el contexto del Análisis de Sentimientos.\nExpanda hashtags.\nCorrija gramaticalmente los tweets (palabras omitidas, acortadas...).\nAdicionalmente, podrían procesarse las conversaciones a las que pertenecen los tweets, así como las URLs o las imágenes que contienen, para enriquecer el texto y dotarlo de un contexto.\nFunciones a desarrollar:\nAnálisis de otros correctores existentes.\nDiseño e implementación del corrector (en base a una versión básica ya implementada o partiendo de cero).\nPruebas.\nDocumentación.\nAdicionalmente, se podría crear un servicio web que corrija tweets en línea usando el módulo implementado.\nRequisitos:\nJava, especialmente parseado de cadenas de texto.\nConocimiento previo de NLP y de Twitter y su API es bienvenido pero no imprescindible.\n3.\nMapeador de elementos urbanos\nEl objetivo de este practicum es mapear automáticamente los elementos urbanos de una ciudad dada una ruta. Para ello, se propone el uso de las siguientes herramientas:\nGoogle Directions para obtener el recorrido de la ruta\nGoogle Street View para obtener las imágenes de la ruta\nGoogle Vision para identificar el tipo de elemento urbano.\nEl alumno podrá hacer uso de otras herramientas si lo considera necesario\nEl resultado de este prácticum será un mapa con las posiciones (latitud, longitud) de los elementos identificados a lo largo de la ruta\nComo caso de uso nos centraremos en las farolas.\nFunciones a desarrollar:\nMódulo para el cálculo de ruta de ruta\nMódulo con imágenes de la ruta\nClasificador de imágenes para reconocimiento de elementos urbanos\nRequisitos:\nDiseño Web\nHTML/CSS\nUso de API\nUso de librerías de Machine Learning (opcional)\n[Esteban González Guardia]\n4.\nClasificador de Farolas\nEl objetivo de este practicum es el desarrollo de una aplicación basada en el juego NightKnights (\nhttp://www.nightknights.eu\n) con el objetivo de reconocer los distintos tipos de farolas. Para ello se hará uso de una plantilla para generar este tipo de juegos disponible en [\n[1]\n]\nFunciones a desarrollar:\nIdentificación de los distintos tipos de farolas\nConstrucción de un banco de imágenes de farolas\nAdaptar la plantilla existente al juego que se quiere desarrollar.\nRequisitos:\nHTML+CSS+Javascript\nPHP\n[Esteban González Guardia]\n5.\nMapas de contaminación lumínica\nEl objetivo es desarrollar una aplicación web para visualizar mapas de contaminación lumínica basados en datos procedentes de sensores, farolas, mediciones tomadas por ciudadanos con aplicaciones como Loss of the Night [\n[2]\n], etc ...\nFunciones a desarrollar:\nIdentificar fuentes de datos relacionadas con Contaminación lumínica\nDesarrollo de un módulo para acceder a los datos de dichas fuentes, siempre que sean de acceso público.\nDesarrollo de un mapa con cada una de las fuentes de datos y la combinación de ellas (incluyendo gráficas si es aplicable)\nRequisitos:\nHTML + CSS\nLeaflet (opcional)\n[Esteban González Guardia]\n6.\nIdentificación de núcleos urbanos mediante el uso de mapas de cielo\nNixnox es un proyecto de STARS4ALL (\nhttp://nixnox.stars4all.eu\n) para la generación de mapas de brillo de cielo. Estos mapas se basan en las mediciones de la bóveda celeste tomadas por un aparato llamado SQM (Sky Quality Meter) [\n[3]\n]. Los objetivos de este proyecto son:\nEl desarrollo de una pequeña aplicación móvil o web en la que se introduzcan los datos del sensor mediante un formulario.\nA partir del mapa generado, identificar las distintas fuentes de luz,  principalmente núcleos urbanos, que están produciendo el brillo en el cielo. Además se adjuntará información relevante del núcleo urbano como número de habitantes, renta per cápita, etc … Para ello, el alumno podrá hacer uso de herramientas como Open Street Map, DBpedia, Wikidata, etc\nUn ejemplo de mapa de cielo generado por nixnox puede verse aquí: [\n[4]\n]\nFunciones a desarrollar:\nDesarrollar un módulo para la extracción de datos de DBPedia / Wikidata / OSM\nDesarrollar un script que genere mapas de cielo. Una versión de este script ya ha sido generada. El alumno deberá modificarla para incluir los datos procedentes de DBPedia o Wikidata.\nMódulo para almacenar los mapas y los datos generados en el Portal de datos de STARS4ALL [\n[5]\n] y/o Zenodo.\nRequisitos:\nPython\nSPARQL (en caso de usar DBPedia o Wikidata)\n[Esteban González Guardia]\n7.\nTesting y visualización de resultados de la aplicación Dark Sky Meter\nDark Sky Meter es una aplicación para Android cuya finalidad es medir el brillo de cielo nocturno con la cámara de un iPhone.\nFunciones a desarrollar:\nPublicar los datos de la aplicación en un CKAN\nVisualizar los datos en forma de mapa de contaminación lumínica\nTestear la aplicación siguiendo la misma técnica que se emplea en NixNox (Practicum 6) que consiste en explorar toda la bóveda celeste. Los datos obtenidos se compararán tanto con un Sky Quality Meter (SQM - fotómetro)[\n[6]\n] como con un TESS (fotómetro desarrollado por STARS4ALL) [\n[7]\n]\nRequisitos:\nPython\n8.\nNamed entity recognition and disambiguation in the legal domain\nWork description: Legal documents (EU directives, national laws, judgements) contain references to companies and individuals, to public entities and to other legal documents. These textual references are not made explicit, hampering the navigation of the documents, preventing the development of advanced search engines and decreasing the performance of information analysis algorithms.\nThe person taking this assignment would design and develop algorithms and methods to identify and disambiguate the key named entities in legal documents and would create a graph with the cross references.\nDesired skills: The person taking this assignment would ideally have interest in NLP technologies and a fair knowledge of either Java or Python. Specific skills on NLP toolkits are not required but would be a plus.\n[Víctor Rodríguez Doncel]\n9.\nHarvesting of terminological resources in the legal domain and its conversion to RDF\nWork description: Domain dependent vocabularies or terminologies are collections of terms used in specific domains. In the case of the legal domain, those terms are the ones used by experts (lawmakers, lawyers, judges…) in their daily communication and also the ones that appear in directives, laws, norms, standards, etc. Such terminologies are normally scattered, monolingual or multilingual, and in many different formats. This makes it difficult to reuse and integrate them in NLP systems.\nThe person taking this assignment would harvest legal terminologies, analyse them, and transform them, if feasible, into RDF.\nDesired skills: The person taking this assignment would ideally have interest in NLP technologies and a fair knowledge of RDF. Specific skills on NLP toolkits are not required but they would be a plus.\n[Elena Montiel Ponsoda]\nOntologies\n\nExtension de VoID y desarrollo de un generador de descripciones VoID\n. Responsable: Mari Carmen. El trabajo consta de dos partes: (1) extension del vocabulario VoID para representar conocimiento relacionado con criterios de reutilizacion de ontologías (VoID-Re) y (2) desarrollo de un generador de descripciones VoID-Re a partir de datos en formato tabular.\n[Propuesto en Septiembre de 2012].\nDesarrollo de una herramienta de especificación de requisitos de una ontología\n. Responsable: Mari Carmen. El trabajo consistirá en crear un plug-in (para Protege y/o NeOn toolkit) que permita especificar los requisitos de una ontologia. [Revisado Junio 2014]\nDesarrollo de una aplicación web para seleccionar las ontologías más adecuadas en base a una serie de criterios\n. Responsable: Mari Carmen [Propuesto Junio 2014]\nDesarrollo de ontología de competencias educativas\n. Responsable: Mari Carmen [Propuesto Junio 2014]\nDesarrollo de ontología de títulos de grado\n. Responsable: Mari Carmen [Propuesto Junio 2014]\nDesarrollo de ontología de títulos de posgrado (máster)\n. Responsable: Mari Carmen [Propuesto Junio 2014]\nDesarrollo de ontología de títulos de posgrado (doctorado)\n. Responsable: Mari Carmen [Propuesto Junio 2014]\nMejora de OOPS! con técnicas lingüisticas\n. Responsables: Mari Carmen y María [Propuesto Junio 2014]\nAplicación de escritorio para OOPS!\n. Responsables: Mari Carmen y María [Propuesto Junio 2014]\nIntegración de OOPS! en Protege\n. Responsables: Mari Carmen y María [Propuesto Junio 2014] --> Primera versión ya desarrollada (Septiembre 2017)\nGestor web de políticas y licencias en RDF\n. Responsable: Víctor + X [Propuesto Noviembre 2014]\nSemantic Web\n\nImplement RDFHarvester\n(Responsable: Raúl García Castro) RDFHarvester is a tool that crawls documents and extracts the RDF content in them. Documents can be structured or non-structured, online or offline. [Propuesto Junio 2014]\nCreación de graphical interface para R2RML generation\n(Responsable: Oscar Corcho). [Propuesto en Septiembre 2013]\nMultilingual MIRROR\n(una herramienta que genera una ontologia que refleja el structure de un BBDD y genera default R2RML mappings con possibilidad de definir las idiomas de BBDD y la ontologia).\nHerramienta de creación de RDFa\n(Responsable: Oscar Corcho). [Propuesto en Septiembre 2013]\nPlataforma de comercio de datos con tecnologías semánticas\n(Responsable: Víctor Rodríguez) [Propuesto en Septiembre 2013]\nEn los últimos años, se han publicado en la red conjuntos de datos\nexpresados en RDF cuyo valor es explotable comercialmente, pero que han\ncarecido de las plataformas adecuadas para su intercambio.  La\ntraslación de las licencias actuales (CreativeCommons etc.) a su forma\nen RDF, junto con su extension para soportar modelos de negocio no\nexclusivamente abiertos podría crear un ecosistema de datos donde se\ncreen, publiquen, consulten y consuman datos en confianza y beneficiando\na todas las partes. El proyectista deberá construir algunos de los servicios para la\nimplementación de una plataforma que encarne esta idea, habiendo de ser\ncapaz de generar e interpretar expresiones de derechos en RDF, y\ndesarrollar prototipos de servicios web. Requisitos:\n- Creatividad e interés por aprender tecnologías nuevas\n- Familiaridad con los conceptos de RDF y la web semántica\n- Capacidad para desarrollar servicios web, escritos en Java\nGeneración de RDF desde OpenStreetMap (creación automática consultas OverpassQL) dado un vocabulario (OWL)\n(Responsables: Carlos Badenes-Olmedo, Esteban Gonzalez, Fernando Serena, Nandana Mihindukulasooriya y Oscar Corcho). [Propuesto en Octubre 2016]\nDado un vocabulario, el sistema construiría y ejecutaría consultas OverpassQL optimizadas sobre OpenStreetMap para extraer todas las posibles instancias que identifique en una región dada, y transformarlas en RDF (siguiendo el vocabulario dado).\nNosotros identificamos dos partes diferenciadas en esta propuesta: la primera construiría las consultas OverpassQL, y la segunda generaría RDF a partir de los resultados de las consultas. Siendo el vocabulario dado el factor común en ambas partes.\nAprendizaje de ontologías desde tipologías de Nodo en OpenStreetMap\n(Responsables: Carlos Badenes-Olmedo, Esteban Gonzalez, Fernando Serena, Nandana Mihindukulasooriya y Oscar Corcho). [Propuesto en Octubre 2016]\nEn OpenStreetMap se manejan recursos georeferenciados en forma de nodos + atributos.  La propuesta busca aprender un modelo conceptual a partir un conjunto nodos  que pueden ser identificados por una serie de atributos o filtrados previamente por un area. No debería quedarse únicamente en transformar los atributos del nodo en propiedades del concepto en la ontología, debería consensuar un modelo a partir de todas las anotaciones.  Por ejemplo, construir una ontología que describa los nodos etiquetados con `highway=streetlight`.  El sistema podría apoyarse en las recomendaciones descritas en la wiki de OpenStreetMap (\nhttp://wiki.openstreetmap.org/wiki/Key:highway\n).\nAplicación Web de Visualización de Corpus documentales basado en Tecnologías Semánticas\n(Responsable: Carlos Badenes-Olmedo).[Propuesto Julio 2018]\nCreación de un frontal web que permita explorar el contenido de grandes colecciones de documentos mediante la carga online de archivos en múltiples formatos (inicialmente PDF). Se integrará con servicios externos mediante peticiones HTTP-Rest y tendrá respaldo en un sistema de almacenamiento de documentos (Solr o Elasticsearch).\nSemantics and Big Data\n\nExtension of morph-streams with streaming systems like Spark, Google Big Query, Apache Samza, etc.\n(Oscar Corcho) [Propuesto en septiembre 2013]\nDevelopment of an RDF Stream processing framework that combines static and dynamic data. In this thesis, existing frameworks for RDF Stream processing that are being described in the context of the W3C RDF Stream Processing community group will be analysed, so as to select the most suitable framework to start with for the consideration of the combination of static and dynamic RDF data that can be useful for the development of applications in the context of Smart Cities.\nExploiting REST interfaces for query federation over Linked Data streams. In this thesis, existing work on RDF query processing over Linked Data (such as the Linked Data Fragments framework) will be enhanced with the implementation of physical query operators that respect the REST nature of Linked Data interfaces while at the same time provide efficient query processing for static and dynamic RDF data.\nLinked Connections for more efficient public transport routing. Linked Connections is a joint effort with the University of Ghent that has recently received the best demo award at the ISWC2015 conference, and which aims at exploiting REST interfaces for the provision of information about connections that can be done in multimodal transport routes. A set of utilities are expected to be developed in the context of this thesis in order to improve the current framework and extend it to more places in the world where transport data is available (normally in the form of GTFS files, but also as real-time feeds).\nPerfiles de investigadores a partir de sus publicaciones\n(Carlos Badenes-Olmedo): Uso de técnicas de modelado de tópicos junto con PLN y Word Embeddings para la construcción de modelos que permitan caracterizar a los investigadores a partir de sus publicaciones científicas.\nVisualización de tópicos, tendencias y evoluciones en corpus de gran tamaño\n(Carlos Badenes-Olmedo): Construcción de un sistema web que permita explorar las anotaciones derivadas de los modelos de tópicos generados a partir de corpus de gran tamaño.\nSemantics and Big Data\n\nExtension of morph-streams with streaming systems like Spark, Google Big Query, Apache Samza, etc.\n(Oscar Corcho) [Propuesto en septiembre 2013]\nDevelopment of an RDF Stream processing framework that combines static and dynamic data. In this thesis, existing frameworks for RDF Stream processing that are being described in the context of the W3C RDF Stream Processing community group will be analysed, so as to select the most suitable framework to start with for the consideration of the combination of static and dynamic RDF data that can be useful for the development of applications in the context of Smart Cities.\nExploiting REST interfaces for query federation over Linked Data streams. In this thesis, existing work on RDF query processing over Linked Data (such as the Linked Data Fragments framework) will be enhanced with the implementation of physical query operators that respect the REST nature of Linked Data interfaces while at the same time provide efficient query processing for static and dynamic RDF data.\nLinked Connections for more efficient public transport routing. Linked Connections is a joint effort with the University of Ghent that has recently received the best demo award at the ISWC2015 conference, and which aims at exploiting REST interfaces for the provision of information about connections that can be done in multimodal transport routes. A set of utilities are expected to be developed in the context of this thesis in order to improve the current framework and extend it to more places in the world where transport data is available (normally in the form of GTFS files, but also as real-time feeds).\nPerfiles de investigadores a partir de sus publicaciones\n(Carlos Badenes-Olmedo): Uso de técnicas de modelado de tópicos junto con PLN y Word Embeddings para la construcción de modelos que permitan caracterizar a los investigadores a partir de sus publicaciones científicas.\nVisualización de tópicos, tendencias y evoluciones en corpus de gran tamaño\n(Carlos Badenes-Olmedo): Construcción de un sistema web que permita explorar las anotaciones derivadas de los modelos de tópicos generados a partir de corpus de gran tamaño.\nSemantic e-Science\n\nCitizen Science\n\nCiencia ciudadana sobre contaminación lumínica. Reponsable: Óscar Corcho [Duración: Septiembre 2016 -> Enero 2017]. El alumno adaptará la aplicación Dark Skies ISS a formato móvil, implementando, si es posible, alguna técnica de gamificación.\nSTARS4ALL: Herramientas de colaboración sobre contaminación lumínica. Reponsable: Óscar Corcho [Duración: Septiembre 2016 -> Enero 2017]. El alumno desplegará e integrará en STARS4ALL alguna herramienta de generación de ideas o colaboración entre ciudadanos. Actualmente está estudiando la herramienta utilizada por el Ayuntamiento de Madrid.\nCreación de contenidos educativos relacionados con eventos astronómicos para estudiantes de primaria y secundaria. Reponsable: Mari Carmen [Duración: Septiembre 2016 -> Enero 2017]. El alumno desarrollará una aplicación móvil para medir la actividad durante las lluvias de estrellas (Gemínidas, Perseidas, etc ...) desde su localización o viendo nuestra retransmisión por Youtube. Esta actividad está inspirada en una propuesta del IMO (International Meteor Organizationg)\n[8]\n.\nLinguistic Linked Data\n\n1.\nConstrucción de una interfaz de acceso a datos enlazados ligüísticos\n. La Terminoteca RDF es un dataset de terminologías multilingües (Terminesp, Termcat) enlazadas según el paradigma de los Datos Enlazados (Linked Data). Este proyecto pretende integrar distintas terminologías en un único grafo para así ofrecer un único punto de entrada a los datos. De esta manera, se puede navegar y buscar información terminológica procedente de distintas fuentes y desarrollada de manera independiente. Los datos se encuentran modelados en RDF siguiendo el modelo Lemon-OntoLex. Terminoteca RDF tiene una interfaz web y un SPARQL-endpoint con algunas consultas de ejemplo. La interfaz web es muy básica y no permite visualizar todos los datos disponibles en Terminoteca RDF, no es intuitiva, y no permite hacer búsquedas complejas al usuario que no conoce el lenguaje de consulta SPARQL. El proyecto consistirá en construir la interfaz de consulta de la Terminoteca RDF de forma que permita al usuario consultar toda la información disponible en el grafo (traducciones, definiciones, fuentes, notas de uso, normas terminológicas). Algunas de las funcionalidades de la nueva interfaz serán\nla ejecución de consultas complejas con SPARQL en el backend a partir de los filtros que proporcione el usuario en el front-end. Se proporcionará a alumno una serie de pautas y de conjuntos de combinaciones posibles.\nla visualización de los resultados de estas consulta en formato de tabla o grafo\nObjetivos:\nAdquisición de conocimientos básicos de RDF y SPARQL\nFamiliarización con el modelado de contenido lingüístico como Linguistic Linked Data mediante estándares de-facto\nCreación de la nueva interfaz\nImplementación y evaluación\nSe aconseja conocimientos de: Diseño web, HTML + CSS, JAVA, J2SE/J2EE, JSP, PHP.\nLos conocimientos básicos de SPARQL y Datos Enlazados resultarán igualmente muy útiles, aunque no son necesarios a priori.\nEl alumno ganará experiencia en el uso de tecnologías de la Web Semántica y adquirirá las principales nociones de los datos lingüísticos enlazados (Linguistic Linked Data).\n[Julia Bosque Gil]\n2.\nConstrucción del portal de Datos Lingüísticos Enlazados del Ontology Engineering Group\n. El portal de Datos Lingüísticos Enlazados del Ontology Engineering Group reúne una serie recursos lingüísticos pasados por el grupo de investigación al formato Resource Description Framework de la Web Semántica. No obstante, el portal está anticuado: la búsqueda de recursos no es intuitiva, existen páginas huérfanas, hay problemas gráficos y el mantenimiento resulta difícil. Este proyecto consiste en la construcción del portal para albergar, dar acceso y visualizar todo el trabajo del Ontology Engineering Group en la línea de Datos Enlazados Lingüísticos. Se proporcionarán al alumno todos los datos necesarios relacionados con el contenido, su organización/disposición, y las funcionalidades requeridas. La principal de ellas es el fácil mantenimiento: el portal debe permitir añadir contenido (referencias bibliográficas, artículos en PDF) y la creación de nuevas páginas de manera intuitiva. Se aconsejan conocimientos de: Diseño web,HTML, CSS, Java, etc.\n[Julia Bosque Gil]\n​",
  "speaker": "SYSTEM",
  "uuid": "5308bbe1-fccd-48b5-93b7-234ddc67aca5"
}