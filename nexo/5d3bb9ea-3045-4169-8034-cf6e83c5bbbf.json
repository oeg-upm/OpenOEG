{
  "message": "SYSTEM: Wiki: Contents\n[\nhide\n]\n1\nCourse 23/24\n1.1\nMariano Rico\n2\nCourse 22/23\n2.1\nAhmad Alobaid\n2.2\nMariano Rico\n3\nCourse 21/22\n3.1\nCarlos Badenes\n3.2\nOscar Corcho\n3.3\nDaniel Garijo\n3.4\nMariano Rico\n3.5\nMaria Poveda\n3.6\nMaria S. Perez\n3.7\nAna Iglesias\n3.8\nJulián Arenas-Guerrero\n3.9\nPablo Calleja\n4\nCourse 20/21\n4.1\nOscar Corcho\n4.2\nMariano Rico\n4.3\nMaría Poveda - Pablo Calleja\n4.4\nVíctor Rodríguez Doncel\n4.4.1\nMaría Poveda - Serge Chávez\n4.5\nCarlos Badenes-Olmedo\n5\nCourse 19/20\n5.1\nMariano Rico\n5.2\nEsteban González\n5.3\nMaría Poveda-Villalón\n5.3.1\nMaría Poveda - Pablo Calleja\n5.4\nCarlos Badenes-Olmedo\n5.5\nOscar Corcho\n5.6\nDavid Chaves\n5.7\nAhmad Alobaid\n5.8\nPablo Calleja - Patricia Martín\nCourse 23/24\n\nMariano Rico\n\nTitle: \"Herramienta interactiva para consensuar expertos en terminologías\"\n- Description: En el contexto del proyecto TERESIA, centrado en la gestión y enlazado de terminologías, es necesaria una aplicación web que permita una colaboración eficiente entre los terminólogos. La aplicación aplicará métricas de consenso, así como métricas para la clasificación y ordenación de términos.\n- Funding schema: Si hay buenos resultados en los 3 primeros meses, hay posibilidad de beca por proyecto. Una buena ocasión para trabajar en un grupo de investigación (OEG) aunando ciencia y tecnología.\nTitle: \"Sistema de alerta temprana para la detección de neologismos en DBpedia del español\"\n- Description: En el contexto del proyecto TERESIA, centrado en la gestión y enlazado de terminologías, y aprovechando que la DBpedia del español se actualiza semanalmente, se propone la creación de un sistema que detecte la aparición de neologismos (potenciales elementos de una terminología) en Wikipedia, mediante un análisis inteligente de los textos y un eficiente uso de la información. Una ocasión única para conocer en detalle el funcionamiento de DBpedia.\n- Funding schema: perhaps GSoC (Google Summer of Code) proposal.\n- Funding schema: if good results are achieved in the first 3 months of the TFM, is it possible to achieve a student grant in our research group (OEG).\nTitle: Smart Domain-oriented crawling: AI for corpora creation -\n- Description: Traditional crawlers are having serious problems to retrieve web pages, among them: dynamic generation of contents, cookies dialogs, just to name a few. Additionally, we need domain-oriented crawlers because downloading without a specific criteria is a nonsense in a constant growing Web. Therefore, this proposal is focused on: (1) analyzing the state-of-the-art, (2) providing an initial prototype using head-less crawlers like Selenium with a (3) binary document classifier trained on a set of documents describing the domain.\n- Funding schema: if good results are achieved in the first 3 months of the TFM, is it possible to achieve a student grant in our research group (OEG).\nTitle: A multi-domain multi-lingual neural classifier for effective crawling -\n- Description: In the context of parallel corpora crawling, we have developed domain-oriented and domain-oriented crawlers. However, we have to use classifying models per language pairs (e.g. \"Spanish and English\") and domain (e.g. \"public administration\"). This results in a long number of classifiers. Our research hypothesis is that we can create a unique multi-domain multi-lingual classifier without sacrificing performance.\n- Funding schema: if good results are achieved in the first 3 months of the TFM, is it possible to achieve a student grant in our research group (OEG).\nCourse 22/23\n\nAhmad Alobaid\n\nTitle: Torrent Federated query with HDT\n- Description: The Torrent protocol allows downloading and uploading data between clients. HDT is a binary format used to compress knowledge graphs. If multiple clients want to use HDT locally, they need to download the whole HDT.  Even though it is compressed and much smaller than the typical formats (e.g., turtle), it is still large (+100GB for wikidata for example). So instead of downloading the whole HDT on the client’s machine, multiple clients can share the HDT. For example, each client can have a portion of that HDT. Also redundancy should be taken into account. Query federation can be applied utilising the torrent protocol.\nMariano Rico\n\nTitle: \"Applying latest Deep Learning techniques to terminology extraction in specific domains\"\n- Description: The recent application of Deep Learning techniques to keyphrase extraction (Sahrawat 2019, Grootendorst 2020, Abulaish 2022) by using deep contextual language models have enhanced the performance in the domain of Automatic Term Extraction. This TFM is aimed at applying these techniques to specific domains such as technology, bio or law.\n- Funding schema: if good results are achieved in the first 3 months of the TFM, is it possible to achieve a student grant in our research group (OEG).\nTitle: esDBTypes: Inferring resource type(s) in a Knowledge Graph  -\n- Description: In a Knowledge Graph (KG), like DBpedia, the challenge \"Entity Typing\" aims at inferring the type of the untyped resource in the KG. This challenge has been faced in previous works with good results. However, recent papers show a big performance enhancement by the combination of NLP and KG techniques, the so-named RDF2vec approaches. This work is focused on applying these new techniques to the Spanish DBpedia.\n- Funding schema: perhaps GSoC (\nGoogle Summer of Code\n)\nTitle: Text2RDF:Language generation to generate RDF from DBpedia abstracts -\n- Description: Generating triples from natural language text has had recently a big push due to the rise of relation extraction by using generative language models (like BART or T5). The objective of this work will be to use such advancements to create a system that can generate DBpedia triples given a Wikipedia abstract (or even the complete page).\n- Funding schema: perhaps GSoC (\nGoogle Summer of Code\n)\nTitle: TUChecker:Advances techniques to validate Translation Units -\n- Description: Translation Units (TUs) are a cornerstone in Machine Translation (MT). Each TU is a pair of sentences from one source language to a target language. MT systems require millions of these TUs for training and testing translation models. However, these TUs use to have many kinds of errors that down-perform the translation model. The objective of this work will be to apply several Machine Learning & Natural Language Processing techniques to detect such problematic TUs and remove them from the TUs datasets.\n- Funding schema: if good results are achieved in the first 3 months of the TFM, is it possible to achieve a student grant in our research group (OEG).\nTitle: MachineHelper: Humans teach machines the subtle details of languages -\n- Description: Idioms (\nmodismos\nor\nclichés\nin Spanish) are a nightmare for language students. Also machines can get confused by the subtle meaning of these words. The objective of this work is to provide a web application in which users can contribute and validate these special sets of words. Additionally, a REST API will provide an efficient communication with external applications. We will use NLP techniques and Machine Learning / Statistics, and we will apply this tool to different ongoing research projects.\n- Funding schema: if good results are achieved in the first 3 months of the TFM, is it possible to achieve a student grant in our research group (OEG).\nTitle: SummaryValidator: Humans teach machines how to summarize -\n- Description: Generative models like T5 allow us to summarize texts beyond extractive summarization (selection of most relevant sentences in a text), allowing the so called abstractive summarization (creating new sentences not found in the text). However, the training of these neural models depends on high-quality training and validation datasets. The objective of this work is to create a web application (ideally, a reactive application using the shiny package) to \"lint\" (clean up) these datasets. You can re-use a tool created by other TFM in the course 21-22, enhance it and include new functionality (like data exchange with an ecosystem of services). A perfect TFM for people skilled on language models and curious about latest techniques in software development.\n- Funding schema: if good results are achieved in the first 3 months of the TFM, is it possible to achieve a student grant in our research group (OEG).\nTitle: Smart Focused Crawling: IA for parallel corpus creation -\n- Description: In the context of Neural Machine Translation (NMT), Prabha's 2021 work shows a hybrid focused crawler that improves the performance of focused web crawlers. This system integrates an approach based on semantic similarity with the probabilistic similarity model and takes various text characteristics as features. Experimental results show that this algorithm outperforms previous algorithms, like Breadth-First-Crawler (BFS), Vector Space Model Crawler (VSM), among others, in terms of harvest rate, precision rate and irrelevance ratio. This TFM will study these techniques in the context of an active research project.\n- Funding schema: if good results are achieved in the first 3 months of the TFM, is it possible to achieve a student grant in our research group (OEG).\nTitle: On Feature Decay Algorithms: Filtering parallel corpus -\n- Description: The Feature Decay algorithms have proven to be the most efficient in the selection of data for the training of neural translators in the selection of corpora for small training sets (between 2 and 4 million sentences pairs). This TFM will determine which dataset is optimal to reach the best results using the least time and computational resources for the sake of quality and efficiency of the process. This TFM will study these algorithms in the context of an active research project.\n- Funding schema: if good results are achieved in the first 3 months of the TFM, is it possible to achieve a student grant in our research group (OEG).\nTitle: On cleaning algorithms for Spanish -\n- Description: This TFM will review the SotA in corpus cleaning for Spanish, from BETO to MarIA models, applied in the context of Neural Machine Translation.\n- Funding schema: if good results are achieved in the first 3 months of the TFM, is it possible to achieve a student grant in our research group (OEG).\nCourse 21/22\n\nCarlos Badenes\n\nTitle: Relation extraction between biomedical entities from scientific texts\n- Description: The CORD-19 collection contains scientific publications on coronaviruses. From a fine tuning on the BioBERT language model we have created a named entity identifier that has annotated the scientific publications with the drugs, diseases, proteins and genes mentioned in their text. The goal of this TFM is to infer relationships between these entities, using both the content of scientific publications and external knowledge bases.\n- No funding available\nOscar Corcho\n\nTitle: Knowledge discovery from the combination of mobility, meteorological and traffic data in the region of Madrid\n- Description: Since 2017 CRTM (Consorcio Regional de Transportes de Madrid) is accumulating data related to the usage of the public transport card in the region of Madrid. We have been doing several MSc theses in the last three years analysing several aspects related to persons with reduced mobility and senior citizens, and now in this thesis we will focus on understanding whether there are any relationships between public transport usage patterns and meteorology and traffic, using open data sources available for the region of Madrid.\n- No funding available\nDaniel Garijo\n\n[ONGOING] Title: Relation extraction from README software documentation (with Pablo Calleja)\n- Description: Software developed as part of scientific research is increasingly important for reproducing and reusing existing work. Software documentation usually contains valuable instructions for running and reusing software, but it is heterogeneous and difficult to interpret by intelligent systems. In this work, the student will have to leverage and adapt relation extraction models to find how different named entities in software documentation are related to each other. As a result, a knowledge graph will be built with the resultant metadata.\n- Objectives:\n-Review state of the art on relation extraction using Transformer-based models (such as Bert, T5 or Pegasus)\n-Identify commonly available relationships in readme files to extract\n-Generate a relation extraction corpus for software documentation\n-Create a robust model for identifying these common relationships\n- Recommended knowledge: Python, Natural Language Processing Frameworks.\n- Funding: Beatriz Galindo (a explorar)\n[ONGOING] Title: Topic modeling for software comparison\n- Description: An increasing number of researchers develop scientific software to carry out their experiments. However, finding and comparing related software is a complex task. Authors rely on existing surveys that rely on significant manual effort, search engines and coworkers to find similar tools. In this project, the student will develop topic models for scientific software, with the purpose of detecting commonalities among different tools based on their documentation\n- Recommended knowledge: Python, unsupervised techniques for topic modeling, knowledge representation\n- Funding: Beatriz Galindo (a explorar)\n[ONGOING] Title: A framework for evaluating Research Object FAIRness (with Esteban)\n- Description: Research Objects define a framework for aggregating the resources associated with a given publication or research hypothesis. While Research Objects help preserve the context of an investigation, it is currently difficult to check whether they comply with the Findable, Accessible, Interoperable and Reusable (FAIR) principles promoted by the scientific community. In this project, the student will analyze what are the common practices for assessing the FAIRness of research artifacts (data, software, ontologies, methods, etc.); and will create a framework to create informative and explainable FAIR metrics for Research Objects.\n- Recommended knowledge: Python/Java, knowledge representation\n- Funding: Reliance?\nTitle: Knowledge extraction for research artifacts (With Esteban)\n- Description: Modern computational research often requires the analysis or integration of multiple data inputs, software components and the postprocessing of their corresponding output. Thanks to the increasing adoption of Open Science policies, authors are depositing some of these research artifacts in public repositories. However,without the author it is still challenging to determine the relationship between all these resources.  In this project, the student will analyze and assess common patterns for finding relationships between research artifacts used within the context of an investigation. In order to do so, the student will explore their metadata and their corresponding papers.\n- Python/Java, knowledge representation\n- Recommended knowledge: Python/Java, knowledge representation\n- Funding: Reliance?\nMariano Rico\n\nTitle MR1: \"Implementing a smart Query Prefetching System for SPARQL endpoints\".\n- Description MR1: Recent works point out machine learning based techniques to create more effective SPARQL endpoints. These techniques are based on predicting the structure of the incoming queries as well as catching augmented triple patterns. The experiments show that these techniques can provide an effective mechanism to avoid the execution of the incoming SPARQL queries and a fast response exploiting the cached information. This \"TFM\" would implement these techniques in order to achieve a prefetching layer for current SPARQL endpoint engines like Virtuoso or Fuseki.\n- Funding schema: not funded\nTitle MR2: \"Analysis on new features for a smart Query Prefetching System for SPARQL endpoints\".\n- Description MR2: Recent works point out machine learning based techniques to create more effective SPARQL endpoints. These techniques are based on predicting the structure of the incoming queries as well as catching augmented triple patterns. The experiments show that these techniques can provide an effective mechanism to avoid the execution of the incoming SPARQL queries and a fast response exploiting the cached information. This \"TFM\" would study new features (like time between queries) [see\nhere\nhere\nand\nhere\n] that could enhance the current results.\n- Funding schema: not funded\nTitle MR3: \"Detection of types in DBpedia based on machine learning techniques\"\n- Description MR3: Recent works have pointed out the benefits of machine learning approaches to predict the type of DBpedia resources. This \"TFM\" will focus on analyzing new approaches to improve the current state of the art. The supervisor will provide some suggestions but the student will have the chance to propose new approaches.\n- Funding schema: not funded\nTitle MR4: \"Generation of semantic information from natural language sources\"\n- Description MR4: With the recent advances (e.g. SyntaxNet) in the analysis of texts in natural language, the conversion of texts into RDF triples is becoming a real possibility. This \"TFM\" will analyze the state of the art and its application to a real use case: DBpedia. From the textual information about a given resource, many new triples will be generated. This new information must be analyzed to validate its correctness before being added to the DBpedia. This work can apply to the Google Summer of Code (GSoC) 2020 to get prestige and additional funding. GSoC 2019 funded this project.\n- Funding schema: perhaps GSoC (\nGoogle Summer of Code\n)\nTitle MR5: \"Extended Spanish DBpedia\"\n- Description MR5: the Spanish DBpedia is the source of semantic data for the Spanish language. Aimed at linking this valuables source with data from other sources, the DBpedia foundation has created\nDBpedia Data Bus\naimed at providing a common place where users can provide and retrieve data. This TFM will be focused on this software engineering problem and the solutions provided, initially for the Spanish DBpedia but, potentially, for any DBpedia.\n- Funding schema: perhaps DBpedia Foundation\nTitle MR6: \"Data Analytics applied to DBpedia error detection\"\n- Description MR6: gamification techniques are a valuable method to detect errors in DBpedia (or any other knowledge graph). This TFM will use Data Analytics techniques over the logs of\nan existing mobile application\nto extract potential errors. These potential errors will feed a web application in which human users will verify these issues. Download the app\nhere\n- Funding schema: perhaps DBpedia Foundation\nTitle MR7: \"A new visualization paradigm to visualize RDF data\"\n- Description MR7: RDF data can be visualized as a graph. Most approaches focus on visualizing the concepts (T-box) or the instances (A-box), or both\nAntoniazzi 2018\n. However, in day-to-day activities, in which we create knowledge graphs (typically from external sources), we miss a visualization paradigm that show us simplified visualizations of the RDF graph. This TFM will be focused on providing this new visualization paradigm by using RDF data from\nSlideWiki\n, an European research project. Proficient knowledge of JavaScript language and web development is a must.\n- Funding schema: perhaps GSoC (\nGoogle Summer of Code\n)\nTitle MR8: \"User evaluation of a web tool to assist users on creating terminologies\"\n- Description MR8: There is a lack of research on the user interaction carried out on the tools used for terminology extraction. This project is aimed at overcoming these deficiencies with a user interaction study on a specific tool developed in our research group.\n- Funding schema: perhaps AInnoSpace.\nTitle MR9: \"Applying Deep Learning techniques to terminology extraction in specific domains\"\n- Description MR9: The recent application of Deep Learning techniques to keyphrase extraction (\nSahrawat 2019\n) by using deep contextual language models such as ELMo and BERT have enhanced the performance in the domain of Automatic Term Extraction. This TFM is aimed at applying these techniques to specific domains such as technology, bio or law.\n- Funding schema: perhaps AInnoSpace .\nTitle MR10: \"A user experience evaluation of multi-term searching techniques in specific domain corpora\"\n- Description MR10: Using the latest UX (user experience) methods we will evaluate a search engine based developed by our research group in the AInno Space research group (funded by Accenture). A comparison to classical keyword search engines will be carried out paying attention to specific domains such as technology, bio or law.\n- Funding schema: perhaps AInnoSpace.\nTitle MR11: \"Allergies and Contamination: an study based on Open Data and Machine Learning\"\n- Description MR11: The Open Data initiative is increasing the number of data sources. In this TFM we will focus on extending the data sources beyond pollutant levels (PL) provided by several local administrations. We plan to extend it with data about temperature, barometric pressure, wind direction, or polen concentration data. These data will be added to our model in order to increase the predictive model created for each user based on his/her historic episodes (itching, coughing, migraine headaches, etc.). A mid-level background on data science and/or machine learning is required.\n- Funding schema: perhaps AInnoSpace.\nTitle MR12: \"esDBpedia data analytics and error reporter tool\"\n- Description MR12: esDBpedia (the Spanish chapter of DBpedia) needs an error reporter tool. Anybody who detects a wrong data in esDBpedia could use the proposed web tool (integrated inside the web site of esDBpedia) to report the error. This error will be labeled as \"possible error\" and the tool will have to analyze the error reported in order to verify it. This problem can be approached in several ways, but we propose initially a \"collaborative\" approach, by using other users to validate statistically the error. The student should have solid skills on web development.\n- Funding schema: perhaps GSoC (\nGoogle Summer of Code\n)\nTitle MR13: \"esDBpedia SPARQL query analytics dashboard\"\n- Description MR13: esDBpedia (the Spanish chapter of DBpedia) stores the queries made to its SPARQL endpoint during one year. It would get very useful for the DBpedia community to have a dashboard (web application) capable of analyzing the queries in terms of numbers (number of queries per hour/day/week/month), type (the type of a query is described\nhere\n) and other metrics. As the number of queries per week are in the range of several million, this is a great dataset for \"big data\" analysis and visualization. The recommended language is R and the\nwebreadr\npackage for reading apache logs.\nAs this work can be easily extend to any DBpedia chapter, this project is suitable for GSoC funding.\n- Funding schema: perhaps GSoC (\nGoogle Summer of Code\n)\nTitle MR14: \"Semantic technologies applied to an efficient management of Research Groups\"\n- Description MR14: This work will apply semantic technologies (RDF, SPARQL, mappings) to the document flow (papers, reports, requests, etc.) of a real research group. This work will provide a migration guideline that could be applied to any traditional organization interested in adopting semantic technologies. The student should have a basic knowledge of semantic web technologies.\n- Funding schema: perhaps OEG funding.\nMaria Poveda\n\nTitle MP1: \"Semantic modelling for the touristic sector: the Tourist Apartments use case \".\n- Description MP!: En los últimos años las viviendas turísticas se han vuelto más y más populares frente a los alojamientos tradicionales. Además de la información proporcionada en los distintos portales ofertando viviendas turísticas, los colegios registradores siguen su propia estructura. El objetivo de esta línea de acción es construir un modelo común para armonizar e integrar la información compartida para los usuarios en los portales como la generada y mantenida en los colegios registradores.\n- Funding schema: not funded\n- Asignado a: Esther Gómez\nMaria S. Perez\n\nTitle MSP1: “Escalabilidad de modelos híbridos de Deep Learning para la mejora de la predicción”.\n- Description MSP1:  El objetivo global del proyecto es la integración de los paradigmas de HPC (High Performance Computing) y Big Data.  Se pretende partir de la investigación de modelos híbridos de Deep Learning (Redes Neuronales Recurrentes, que permiten incorporar información temporal, y Redes Neuronales Convolucionales, que son apropiadas para variables espaciales) para la mejora de la predicción de determinadas variables de diferentes casos de uso, que se caracterizan por ser modelos robustos, flexibles, extensibles, integrables y portables. El objetivo del TFM es la mejora de la característica de escalabilidad de dichos modelos.\n- Funding schema: Proyecto CABAHLA\n- Asignado a: Persona contratada por el proyecto (si es del máster de IA)\nAna Iglesias\n\nTitle AI1: “Translating between mapping languages for Knowledge Graph construction”.\n- Description AI1: The increasing use of RDF-based Knowledge Graphs (KG) in the last decades has lead the community to come up with a wide variety of ways to transform data and generate KGs. One way is based on the use of mapping languages, that represent the rules to transform data into RDF. Nowadays, there are many different languages available, but not interoperability among them. The objective of this thesis consists of developing a solution that enable translations between mapping languages, to increase interoperability and potentiate the use of this technologies to generate KGs.\n[ONGOING] Title AI2: “Generating and enriching the Drugs4Covid Knowledge Graph”.\n- Description AI2: In this work, the student will build a Knowledge Graph with relevant concepts mentioned papers related to COVID-19 (the CORD-19 dataset): chemicals, genes, proteins and diseases, associations among them and the evidence that they are based on. This work will not only serve to be a biomedical knowledge base, but for promoting evidence-based research and the discovery of novel associations, such as potential drug repurposing or genetic predisposition to diseases.\nJulián Arenas-Guerrero\n\nTitle JAG1: ”Fine-grained Analysis of SQL Queries Generated by Virtual Knowledge Graph Systems”.\n- Description JAG1: Virtual Knowledge Graph systems allow to query relational databases as if they were a knowledge graphs. This involves the translation of SPARQL queries to their equivalents in SQL. However, the resulting SQL queries in this process are often inneficient, for instance they contain unnecesary cross joins, DISTINCT clauses, correlated subqueries, etc. This work consists in performing a fine-grained analysis that could be used as a reference to optimize Virtual Knowledge Graph systems.\nTitle JAG2: ”Inference in Knowledge Graph Materialization with RML”.\n- Description JAG2: RML is used to map heterogeneous data sources to RDF knowledge graphs. In the presence of an ontology, it is possible to do reasoning. To achieve this in materialization (instantiation of the entire RDF), there are two options: saturating the RML mappings with additional rules that generate the inferred triples, or afterwards with mechanical forward chaining. This work consists in developing a simple module in Python to saturate RML mappings (that can then be used by tools such as Morph-KGC) and comparing it to mechanical forward chaining (e.g. owlrl library).\nPablo Calleja\n\nTitle: Language Model adaptation measuring the impact of NER, coreference and sentence transformation in small corpora available scenarios.\n- Description: The generation of language models such as BERT and GPT-3 requires a huge amount of corpus and a huge amount of hardware requisites. However, language models can be adapted to particular scenarios with enough data. Moreover, this data is usually comprised of sentences (e.g, extracted from books) and representative of the language. The objective of this work is to adapt language models in those scenarios in which the available data is low and in which the presence of named entities and bad sentence constructions can degenerate the adapting process of the model through processes that could clean and generalize the available data.\n- No funding available\nCourse 20/21\n\nOscar Corcho\n\nTitle OC1: \"An analysis and proposal of metrics for Explainable AI\"\n- Description OC1: Recent literature on Explanaible AI (XAI) proposes different dimensions that need to be explored when analysing the explainability capabilities of machine learning models. The objective of this MSc thesis is to make a systematic analysis of the current state of the art in this area, identifying the metrics that are being used in different papers related to this topic, and make a proposal of the metrics to be used, and the types of evaluations to be made, for different types of machine learning models\"\nTitle OC2: \"Combining ontologies and machine learning models for Explainable AI\"\n- Description: Some initial papers have started appearing on the last two years on the usage of ontologies for providing explanations of machine learning models that have been generated with different types of algorithms. Some of them focus on using classes and properties, as general as possible and as specific as needed, identified in ontologies in order to explain these models. The work done in this MSc thesis will be focused on understanding the current state of the art in this area, and propose alternative ways to use ontologies for such explanation.\nTitle OC3: \"Using ontologies and description logics to represent and reason around probabilistic graphical models\"\n- Description: Several works have been proposed in the past for the representation of probabilistic graphical models using ontology languages like OWL, or extending them, with the corresponding definition of the semantics of the proposed language and of reasoning procedures. This MSc thesis will focus on advancing the state of the art in this area by testing and providing new tools that implement those reasoning procedures, and applying them in the context of autonomous networks.\nTitle OC4: \"Creating a network of ontologies and declarative mappings to generate a knowledge graph for Open Data for universities\"\n- Description: In this work we will create a network of ontologies that allow describing some of the core datasets that are being considered for the publication of open data for universities across Spain (and applicable elsewhere as well), together with the corresponding declarative mappings for the generation of a knowledge graph. This work will have a lot of impact in a real use case.\nMariano Rico\n\nTitle MR1: \"Implementing a smart Query Prefetching System for SPARQL endpoints\".\n- Description MR1: Recent works point out machine learning based techniques to create more effective SPARQL endpoints. These techniques are based on predicting the structure of the incoming queries as well as catching augmented triple patterns. The experiments show that these techniques can provide an effective mechanism to avoid the execution of the incoming SPARQL queries and a fast response exploiting the cached information. This \"TFM\" would implement these techniques in order to achieve a prefetching layer for current SPARQL endpoint engines like Virtuoso or Fuseki.\n- Funding schema: not funded\nTitle MR2: \"Analysis on new features for a smart Query Prefetching System for SPARQL endpoints\".\n- Description MR2: Recent works point out machine learning based techniques to create more effective SPARQL endpoints. These techniques are based on predicting the structure of the incoming queries as well as catching augmented triple patterns. The experiments show that these techniques can provide an effective mechanism to avoid the execution of the incoming SPARQL queries and a fast response exploiting the cached information. This \"TFM\" would study new features (like time between queries) [see\nhere\nhere\nand\nhere\n] that could enhance the current results.\n- Funding schema: not funded\nTitle MR3: \"Detection of types in DBpedia based on machine learning techniques\"\n- Description MR3: Recent works have pointed out the benefits of machine learning approaches to predict the type of DBpedia resources. This \"TFM\" will focus on analyzing new approaches to improve the current state of the art. The supervisor will provide some suggestions but the student will have the chance to propose new approaches.\n- Funding schema: not funded\nTitle MR4: \"Generation of semantic information from natural language sources\"\n- Description MR4: With the recent advances (e.g. SyntaxNet) in the analysis of texts in natural language, the conversion of texts into RDF triples is becoming a real possibility. This \"TFM\" will analyze the state of the art and its application to a real use case: DBpedia. From the textual information about a given resource, many new triples will be generated. This new information must be analyzed to validate its correctness before being added to the DBpedia. This work can apply to the Google Summer of Code (GSoC) 2020 to get prestige and additional funding. GSoC 2019 funded this project.\n- Funding schema: perhaps GSoC (\nGoogle Summer of Code\n)\nTitle MR5: \"Extended Spanish DBpedia\"\n- Description MR5: the Spanish DBpedia is the source of semantic data for the Spanish language. Aimed at linking this valuables source with data from other sources, the DBpedia foundation has created\nDBpedia Data Bus\naimed at providing a common place where users can provide and retrieve data. This TFM will be focused on this software engineering problem and the solutions provided, initially for the Spanish DBpedia but, potentially, for any DBpedia.\n- Funding schema: perhaps DBpedia Foundation\nTitle MR6: \"Data Analytics applied to DBpedia error detection\"\n- Description MR6: gamification techniques are a valuable method to detect errors in DBpedia (or any other knowledge graph). This TFM will use Data Analytics techniques over the logs of\nan existing mobile application\nto extract potential errors. These potential errors will feed a web application in which human users will verify these issues. Download the app\nhere\n- Funding schema: perhaps DBpedia Foundation\nTitle MR7: \"A new visualization paradigm to visualize RDF data\"\n- Description MR7: RDF data can be visualized as a graph. Most approaches focus on visualizing the concepts (T-box) or the instances (A-box), or both\nAntoniazzi 2018\n. However, in day-to-day activities, in which we create knowledge graphs (typically from external sources), we miss a visualization paradigm that show us simplified visualizations of the RDF graph. This TFM will be focused on providing this new visualization paradigm by using RDF data from\nSlideWiki\n, an European research project. Proficient knowledge of JavaScript language and web development is a must.\n- Funding schema: perhaps GSoC (\nGoogle Summer of Code\n)\nTitle MR8: \"User evaluation of a web tool to assist users on creating terminologies\"\n- Description MR8: There is a lack of research on the user interaction carried out on the tools used for terminology extraction. This project is aimed at overcoming these deficiencies with a user interaction study on a specific tool developed in our research group.\n- Funding schema: perhaps AInnoSpace (Accenture).\nTitle MR9: \"Applying Deep Learning techniques to terminology extraction in specific domains\"\n- Description MR9: The recent application of Deep Learning techniques to keyphrase extraction (\nSahrawat 2019\n) by using deep contextual language models such as ELMo and BERT have enhanced the performance in the domain of Automatic Term Extraction. This TFM is aimed at applying these techniques to specific domains such as technology, bio or law.\n- Funding schema: perhaps AInnoSpace (Accenture).\nTitle MR10: \"A user experience evaluation of multi-term searching techniques in specific domain corpora\"\n- Description MR10: Using the latest UX (user experience) methods we will evaluate a search engine based developed by our research group in the AInno Space research group (funded by Accenture). A comparison to classical keyword search engines will be carried out paying attention to specific domains such as technology, bio or law.\n- Funding schema: perhaps AInnoSpace (Accenture).\nTitle MR11: \"Allergies and Contamination: an study based on Open Data and Machine Learning\"\n- Description MR11: The Open Data initiative is increasing the number of data sources. In this TFM we will focus on extending the data sources beyond pollutant levels (PL) provided by several local administrations. We plan to extend it with data about temperature, barometric pressure, wind direction, or polen concentration data. These data will be added to our model in order to increase the predictive model created for each user based on his/her historic episodes (itching, coughing, migraine headaches, etc.). A mid-level background on data science and/or machine learning is required.\n- Funding schema: perhaps AInnoSpace (Accenture).\nTitle MR12: \"esDBpedia data analytics and error reporter tool\"\n- Description MR12: esDBpedia (the Spanish chapter of DBpedia) needs an error reporter tool. Anybody who detects a wrong data in esDBpedia could use the proposed web tool (integrated inside the web site of esDBpedia) to report the error. This error will be labeled as \"possible error\" and the tool will have to analyze the error reported in order to verify it. This problem can be approached in several ways, but we propose initially a \"collaborative\" approach, by using other users to validate statistically the error. The student should have solid skills on web development.\n- Funding schema: perhaps GSoC (\nGoogle Summer of Code\n)\nTitle MR13: \"esDBpedia SPARQL query analytics dashboard\"\n- Description MR13: esDBpedia (the Spanish chapter of DBpedia) stores the queries made to its SPARQL endpoint during one year. It would get very useful for the DBpedia community to have a dashboard (web application) capable of analyzing the queries in terms of numbers (number of queries per hour/day/week/month), type (the type of a query is described\nhere\n) and other metrics. As the number of queries per week are in the range of several million, this is a great dataset for \"big data\" analysis and visualization. The recommended language is R and the\nwebreadr\npackage for reading apache logs.\nAs this work can be easily extend to any DBpedia chapter, this project is suitable for GSoC funding.\n- Funding schema: perhaps GSoC (\nGoogle Summer of Code\n)\nTitle MR14: \"Semantic technologies applied to an efficient management of Research Groups\"\n- Description MR14: This work will apply semantic technologies (RDF, SPARQL, mappings) to the document flow (papers, reports, requests, etc.) of a real research group. This work will provide a migration guideline that could be applied to any traditional organization interested in adopting semantic technologies. The student should have a basic knowledge of semantic web technologies.\n- Funding schema: perhaps OEG funding.\nMaría Poveda - Pablo Calleja\n\nTitle MP1: Explotation of Semantic Graphs for Entity Linking and Word Sense Disambiguation\n- Description MP1: El objetivo de este proyecto es desarrollar un algoritmo capaz de explotar grafos semánticos para tareas de Entity Linking y Word Sense Disambiguation. El proyecto partirá de los grafos BabelNet y WordNet para dichas tareas pero el objetivo final es que dicho algoritmo pueda ser exportado y adaptado (con cambios mínimos) a otros grafos semánticos (e.g., SNOMED-CT). El proyecto parte de algoritmos ya implementados para ambas tareas y se pretende mejorar su precisión y adaptabilidad mediante nuevas técnicas para el procesamiento del lenguaje natural y navegación por grafos que han aparecido en los últimos años.\n- Funding schema: not funding available so far\nVíctor Rodríguez Doncel\n\nTitle VRD1: Description of datasets in terms of their personal data status and possibles purposes\n- Description VRD1: El objetivo de este proyecto es desarrollar los vocabularios y ontologías que sean necesarios para describir un conjunto de datos en términos del impacto jurídico, ético y social que su explotación pudiera desencadenar. En particular, los vocabularios y ontologías deberán ser capaces de describir si el conjunto de datos tiene datos personales, de qué carácter son (si los tiene) y si suponen un riesgo para la reidentificación (si no los tiene). Asimismo, deberán ser capaces de describir los usos potenciales que el conjunto de datos podría tener, el impacto que dicho uso supondría, y los riesgos de un uso malicioso de los mismos. Gracias a esta descripción, se facilitará la \"evaluación del impacto en protección de datos\" (DPIA) y la evaluación del impacto ético, jurídico y social de proyectos derivados.\n- Funding schema: not funding available so far\nMaría Poveda - Serge Chávez\n\nTitle MPSC1: Explotation of Semantic Graphs for Entity Linking and Word Sense Disambiguation\n- Description MPSC1: El objetivo de este proyecto es realizar un estado del arte de los modelos de datos y ontologías sobre BPMN y generar un conjunto de mappings RML para la anotación sémantica de datos BPMN. Business Process Model Notation (BPMN) is a standard for business process modeling. It provides a common visual language so the steps and workflow of a process can be communicated seamlessly to all the stakeholders involved in a project or activity.\n- Funding schema: not funding available so far\nCarlos Badenes-Olmedo\n\nTitle CB1: \"Smart Bibtex\"\n- Description CB1: Enrichment of bibliographic references through natural language processing techniques. The student will build an intelligent system that expands the content of a reference list, in bibtex format, using external data sources.\n- Funding schema: not funded\nTitle CB2: \"Explainable QA over KG\"\n- Description CB2: Creation of an Extractive Question/Answering (EQA) system that continuously updates its knowledge from a KG. The student will use language models (e.g. BERT) to build a RESTful service that responds to natural language queries.\n- Funding schema: not funded\nTitle CB3: \"Unsupervised Topic Labelling\"\n- Description CB3: Most text mining algorithms represent documents in a common feature space that abstracts away from the specific sequence of words used in them. Probabilistic Topic Models reduce that feature space by annotating documents with thematic information. However, this information is expressed as probability distributions over the complete vocabulary, making it more difficult to interpret. The student will design an algorithm that summarizes the meaning of a topic through multi-lingual concepts.\n- Funding schema: not funded\nCourse 19/20\n\nMariano Rico\n\nTitle MR1: \"Implementing a smart Query Prefetching System for SPARQL endpoints\".\n- Description MR1: Recent works point out machine learning based techniques to create more effective SPARQL endpoints. These techniques are based on predicting the structure of the incoming queries as well as catching augmented triple patterns. The experiments show that these techniques can provide an effective mechanism to avoid the execution of the incoming SPARQL queries and a fast response exploiting the cached information. This \"TFM\" would implement these techniques in order to achieve a prefetching layer for current SPARQL endpoint engines like Virtuoso or Fuseki.\n- Funding schema: not funded\nTitle MR2: \"Analysis on new features for a smart Query Prefetching System for SPARQL endpoints\".\n- Description MR2: Recent works point out machine learning based techniques to create more effective SPARQL endpoints. These techniques are based on predicting the structure of the incoming queries as well as catching augmented triple patterns. The experiments show that these techniques can provide an effective mechanism to avoid the execution of the incoming SPARQL queries and a fast response exploiting the cached information. This \"TFM\" would study new features (like time between queries) that could enhance the current results.\n- Funding schema: not funded\nTitle MR3: \"Detection of types in DBpedia based on machine learning techniques\"\n- Description MR3: Recent works have pointed out the benefits of machine learning approaches to predict the type of DBpedia resources. This \"TFM\" will focus on analyzing new approaches to improve the current state of the art. The supervisor will provide some suggestions but the student will have the chance to propose new approaches.\n- Funding schema: not funded\nTitle MR4: \"Generation of semantic information from natural language sources\"\n- Description MR4: With the recent advances (e.g. SyntaxNet) in the analysis of texts in natural language, the conversion of texts into RDF triples is becoming a real possibility. This \"TFM\" will analyze the state of the art and its application to a real use case: DBpedia. From the textual information about a given resource, many new triples will be generated. This new information must be analyzed to validate its correctness before being added to the DBpedia. This work can apply to the Google Summer of Code (GSoC) 2020 to get prestige and additional funding. GSoC 2019 funded this project.\n- Funding schema: perhaps GSoC (\nGoogle Summer of Code\n)\nTitle MR5: \"Live Spanish DBpedia\"\n- Description MR5: the Spanish DBpedia is the second biggest DBpedia after the English one. Aimed at keeping its data updated, the DBpedia foundation has created\nDBpedia Data Bus\n, an alternative to\nDBpedia Live\n. This TFM will be focused on this software engineering problem and the solutions provided, initially for the Spanish DBpedia but, potentially, for any DBpedia.\n- Funding schema: perhaps DBpedia Foundation\nTitle MR6: \"Data Analytics applied to DBpedia error detection\"\n- Description MR6: gamification techniques are a valuable method to detect errors in DBpedia (or any other knowledge graph). This TFM will use Data Analytics techniques over the logs of\nan existing mobile application\nto extract potential errors. These potential errors will feed a web application in which human users will verify these issues. Download the app\nhere\n- Funding schema: perhaps DBpedia Foundation\nTitle MR7: \"A new visualization paradigm to visualize RDF data\"\n- Description MR7: RDF data can be visualized as a graph. Most approaches focus on visualizing the concepts (T-box) or the instances (A-box), or both\nAntoniazzi 2018\n. However, in day-to-day activities, in which we create knowledge graphs (typically from external sources), we miss a visualization paradigm that show us simplified visualizations of the RDF graph. This TFM will be focused on providing this new visualization paradigm by using RDF data from\nSlideWiki\n, an European research project. Proficient knowledge of JavaScript language and web development is a must.\n- Funding schema: perhaps GSoC (\nGoogle Summer of Code\n)\nTitle MR8: \"A POS-pattern based tool to assist users on creating terminologies\"\n- Description MR8: There is a lack of user interaction in the current syntactic systems used for terminology extraction. This project is aimed at coping these deficiencies with a friendly user interface that allows users to provide fine-grain information to assist terminologist with powerful but ease-of-use online tools.\n- Funding schema: perhaps AInnoSpace (Accenture).\nTitle MR9: \"Applying Deep Learning techniques to terminology extraction in specific domains\"\n- Description MR9: The recent application of Deep Learning techniques to keyphrase extraction (\nSahrawat 2019\n) by using deep contextual language models such as ELMo and BERT have enhanced the performance in the domain of Automatic Term Extraction. This TFM is aimed at applying these techniques to specific domains such as technology or law.\n- Funding schema: perhaps AInnoSpace (Accenture).\nTitle MR10: \"A user experience evaluation of multi-term searching techniques in specific domain corpora\"\n- Description MR10: Using the latest UX (user experience) methods we will evaluate search technologies based in phrasal nouns. A comparison to classical keyword search will be carried out paying attention to specific domains such as technology or law.\n- Funding schema: perhaps AInnoSpace (Accenture).\nTitle MR11: \"Allergies and Contamination: an study based on Open Data and Machine Learning\"\n- Description MR11: The Open Data initiative is increasing the number of data sources. In this TFM we will focus on pollutant levels (PL) provided by several local administrations. When some PL exceed the recommended thresholds, the users of the proposed system will get a notification of their mobiles (based on their geolocation). Additionally, if the user indicates to the app some type of symptom (itching, coughing, etc.) we could predict allergic episodes and what is the pollutant that prejudices him/her. The student should have basic skills on mobile app development.\n- Funding schema: perhaps AInnoSpace (Accenture).\nTitle MR12: \"esDBpedia data analytics and error reporter tool\"\n- Description MR12: esDBpedia (the Spanish chapter of DBpedia) needs an error reporter tool. Anybody who detects a wrong data in esDBpedia could use the proposed web tool (integrated inside the web site of esDBpedia) to report the error. This error will be labeled as \"possible error\" and the tool will have to analyze the error reported in order to verify it. This problem can be approached in several ways, but we propose initially a \"collaborative\" approach, by using other users to validate statistically the error. The student should have solid skills on web development.\n- Funding schema: perhaps GSoC (\nGoogle Summer of Code\n)\nEsteban González\n\nTitle EG1: Generación de mapas de contaminación lumínica\n- Description EG1: En este proyecto el alumno deberá construir una API REST y un pequeño formulario web responsive que permita el envío de un fichero con mediciones realizadas por el fotómetro  (\nTESS-P\n) o SQM. Una vez recibido el fichero, el sistema deberá generar el mapa de forma automática mediante un script que se le proporcionará al alumno. El sistema deberá permitir gestión de usuarios, de forma que se puedan visualizar los mapas generados por cada uno de ellos. Para más información\nNIXNOX Project\n- Funding schema: not funded\nTitle EG2: Caracterización de mapas de contaminación lumínica\n- Description EG2: En este proyecto el alumno deberá,  partiendo de mapas de contaminación lumínica generados en el proyecto\nNIXNOX Project\n, identificar las fuentes que están produciendo dicha contaminación, especialmente ciudades y polígonos industriales. Para ello hará uso de fuentes externas tal como DBPedia, Wikidata u Open Street Map.\n- Funding schema: not funded\nTitle EG3: Plataforma para homogeneizar la información de elementos urbanos\n- Description EG3: El objetivo de este proyecto es desarrollar una plataforma que permita analizar y visualizar la información disponible de un elemento urbano desde distintas fuentes de información. El caso de uso se centrará en las farolas y la información disponible de ellas en OpenStreetMap, Portales de Datos Abiertos y Epicollect5.\n- Funding schema: not funded\nTitle EG4: Generador de informes medioambientales\n- Description EG4: El objetivo de este proyecto es crear una serie de plantillas para generar informes medioambientales en PDF de forma automática. Para ello, se hará uso de los datos procedentes de una red de sensores de contaminación lumínica disponibles en la plataforma (\nTESS\n)).\n- Funding schema: not funded\nTitle EG5: Bot de telegram para caracterizar farolas.\n- Description EG5: El objetivo de este proyecto es desarrollar un bot para telegram que permita registrar observaciones de los usuarios sobre un determinado elemento urbano. El usuario podrá adjuntar imágenes a dichas observaciones. El ámbito de aplicación serán fuentes de iluminación. Con la ayuda del bot, el usuario será capaz de identificar las farolas y proporcionar información relevante sobre ella como color, horas de funcionamiento, estado, altura, etc ...\n- Funding schema: not funded\nMaría Poveda-Villalón\n\nTitle MP1: Ontology visual representation analysis and library generation\n- Description MP1: El objetivo de este proyecto es analizar las distintas formas de visualización de ontologías comúnmente utilizadas analizando su adecuación al lenguaje de representación de ontolgoías OWL.  A partir de dicho análisis se generará una librería de plantillas de vidualización de ontologías siguiendo distintos sistemas de representación gráfica de conocimiento. Finalmente, estas librerias se integrarán para su ejecución y generación de documentación en el sistema de gestión colaborativa para el desarrollo de ontologías OnToology.\n- Funding schema: not funding available so far\nMaría Poveda - Pablo Calleja\n\nTitle MP2: Explotation of Semantic Graphs for Entity Linking and Word Sense Disambiguation\n- Description MP2: El objetivo de este proyecto es desarrollar un algoritmo capaz de explotar grafos semánticos para tareas de Entity Linking y Word Sense Disambiguation. El proyecto partirá de los grafos BabelNet y WordNet para dichas tareas pero el objetivo final es que dicho algoritmo pueda ser exportado y adaptado (con cambios mínimos) a otros grafos semánticos (e.g., SNOMED-CT). El proyecto parte de algoritmos ya implementados para ambas tareas y se pretende mejorar su precisión y adaptabilidad mediante nuevas técnicas para el procesamiento del lenguaje natural y navegación por grafos que han aparecido en los últimos años.\n- Funding schema: not funding available so far\nCarlos Badenes-Olmedo\n\nTitle CB1: Unsupervised Cross-lingual Conceptualization\n- Description CB1: Most text mining algorithms represent documents in a common feature space that abstracts away from the specific sequence of words used in them. Probabilistic Topic Models reduce that feature space by annotating documents with thematic information. However, this information is expressed as probability distributions over the complete vocabulary, making it more difficult to interpret. This TFM will design an algorithm that summarizes the meaning of a topic through multi-lingual concepts.\n- Funding schema: perhaps TheyBuyForYou\nTitle CB2: Measuring the Human Perception of Textual Similarity\n- Description CB2: Finding or synthesizing value from a subset of closely related documents can be especially difficult when navigation between documents is based on metadata such as a timestamp of last update, rather than topics contained within the documents. Many of topic-based technologies depend on measures of similarity between documents without clear validation that the algorithmic measures being used match human perceptions of similarity. This TFM will build a benchmark to measure how well various similarity measures match human perceptions when they are being used to help humans navigate through a large set of documents.\n- Funding schema: perhaps TheyBuyForYou\nTitle CB3: Semantic-based Navigation through Documents\n- Description CB3: Searching for similar documents and exploring major themes covered across groups of documents are common actions when browsing collections of scientific papers. This TFM will provide a web-based solution to explore large collections of documents from their semantic annotations.\n- Funding schema: perhaps TheyBuyForYou\nOscar Corcho\n\nTitle OC1: \"Characterisation of senior citizen mobility in the region of Madrid\".\n- Description OC1: The public transport card is used widely in the region of Madrid. The data about card validations is managed by CRTM (Consorcio Regional de Transportes de Madrid), with a predefined structure and data attributes that ensure anonimicity and at the same time may allow for further exploitation. One of the interests of the accessbility office at CRTM is to understand the behaviour of senior citizens with respect to their usage of public transport. Therefore, they provide us data to make this analysis, which has already been translated to a TFM in the course 2018-2019. The objective of this master thesis is to continue with the work done in that thesis and apply new techniques plus reproduce already done experiments with new data from this year.\n- Funding schema: not funded\nTitle OC2: \"Ontology modelling from XML Schema documents and/or UML models\"\n- Description OC2: There are several situations where the ontology development process may be done almost directly from other existing models, such as XML Schema or UML models. While the automatic translation from one model to another may seem simple and intuitive (a UML class will be normally transformed into an OWL class, or an XML Schema complexType will probably be transformed also as a class) there are lots of cornercases that need to be considered when performing such translation so that the final result is clean enough to be provided to an ontology engineer to continue working on the ontology development process. This TFM will focus on analysing the outputs of existing tools that perform this activity, identifying the errors and situations where this does not work, based on real developments done in the context of the transport domain, and design and implement algorithms for a more robust and clean transformation process. The work done may have the opportunity to be published in a knowledge engineering conference like EKAW.\n- Funding: SPRINT\nDavid Chaves\n\nTitle DC1: \"Physical design of Knowledge Graphs\".\n- Description DC1: A día de hoy existen infinidad de datasets almacenados en bases de datos relaciones. En el contexto de la creación de Knowledge Graphs se ha propuesto la transformación de los datos de esas RDB a RDF pero también la traducción de las consultas de SPARQL a SQL. Se ha demostrado que en un contexto de federación de consultas,  dependiendo del tipo de subquery generado, la ejecución sobre RDB es más eficiente que sobre el RDF generado y viceversa. El objetivo de este proyecto es la aplicación de técnicas de Machine Learning en un contexto de consultas federadas para, en base a las descripciones de las fuentes de un Knowledge Graph (RDF-MT), decidir que partes deben ser transformadas a RDF y que partes deben mantenerse en la base de datos para la ejecución eficiente de consultas SPARQL.\n- Funding schema: perhaps SPRINT\nTitle DC2: \"SPARQL 1.1 in SPARQL-to-SQL translations\".\n- Description DC2: El objetivo de este proyecto es la mejora en la traducción de consultas SPARQL-to-SQL de la herramienta morph-RDB. Este software, capaz de crear un Knowledge Graph Virtual sobre una base de datos relacional utilizando el lenguaje de mapping R2RML, no tiene en cuenta todos los operadores existentes de SPARQL en su última versión. El fin es formalizar e implementar la traducción de estos operadores de SPARQL a SQL e integrarlo en el desarrollo actual de la herramienta.\n- Funding schema: not funded\nTitle DC3: \"Virtual Knowledge Graph Access over Open Data\"\n- Description DC2: El objetivo de este proyecto la creación de una capa virtual de consultas con SPARQL sobre los datos abiertos en bruto (CSV, JSON, XML) de diversas fuentes (e.g. ayuntamientos, transporte, biología). Para ello, se deberá extender y formalizar la traducción de consultas en SPARQL haciendo uso de reglas de mappeo a comandos en bash, para generar unos resultados de forma eficiente.\n- Funding schema: not funded\nAhmad Alobaid\n\nTitle AA1: \"Online R2RML Mapping Editor\".\n- Description AA1: Writing Mappings to represent heterogenous tabular data is cumbersome, it is time consuming and subject to human errors. Some efforts in the semantic web community to generate such mapping in a (semi-)automatic way are still on early stages (experimental). Even though they can be used to generate mappings, they are not designed to include editing functionalities. This is the technological problem to be addressed in this work. The OEG group developed a simple prototype of a mapping editor without this editing functionality\nOME\n. It is also important to support autocomplete of concepts and properties from general ontologies (currently schema.org is supported) and user defined ontologies. It would be a very good advantage to have it online without any required plugins. Also including a way to use automatic semantic labeling systems (via APIs) like TADA, Karma, .. etc. would be very beneficial.\nTitle AA2: \"Semantic Reality\".\n- Description AA2: The idea is to represent the world semantically. To have a video camera that record everything. This feed will go into a system which identify objects. The objects are represented using semantic concepts. Then, the positioning of these objects will also be represented using ontologies.\nPablo Calleja - Patricia Martín\n\nTitle PP1: \"Improving Spanish and English terminology extraction processes over massive documents using distributed computation\".\n- Description PP1: El objetivo del proyecto es mejorar una herramienta de extracción terminológica desarrollada sobre Solr, implementar nuevas técnicas de extracción de terminología y mejorar su interfaz. Actualmente, la herramienta no explota las propiedades de Solr para realizar computación distribuida para trabajar con grandes volúmenes de documentos de forma ágil. De la misma forma, la extracción de términos sobre documentos en español ha de ser mejorada con nuevos modelos actuales: mejorar la precisión de las herramientas  mediante la creación de una aplicación de refinamiento con ténicas de procesamiento natural.\n- Funding schema: not funding available so far (but will be necessary in current european projects or others that will come)",
  "speaker": "SYSTEM",
  "uuid": "5d3bb9ea-3045-4169-8034-cf6e83c5bbbf"
}