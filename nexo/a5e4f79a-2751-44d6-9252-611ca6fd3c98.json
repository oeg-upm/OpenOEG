{
  "message": "SYSTEM: Sauper2007.pdf: Página 3\nevance learning with the target analysis task leads to\nfurther performance gains.\nFinally, our work relates to supervised topic mod-\nels in Blei and McAullife (2007). In this work, la-\ntent topic variables are used to generate text as well\nas a supervised sentiment rating for the document.\nHowever, this architecture does not permit the usage\nof standard discriminative models which condition\nfreely on textual features.\n3\nModel\n3.1\nProblem Formulation\nIn this section, we describe a model which incorpo-\nrates content information into a multi-aspect sum-\nmarization task.2\nOur approach assumes that at\ntraining time we have a collection of labeled doc-\numents DL, each consisting of the document text\ns and true task-speciﬁc labeling y∗. For the multi-\naspect summarization task, y∗consists of sequence\nlabels (e.g., value or service) for the tokens of a\ndocument.\nSpeciﬁcally, the document text s is\ncomposed of sentences s1, . . . , sn and the label-\nings y∗consists of corresponding label sequences\ny1, . . . , yn.3\nAs is common in related work, we model each yi\nusing a CRF which conditions on the observed doc-\nument text. In this work, we also assume a content\nmodel, which we ﬁx to be the document-level HMM\nas used in Barzilay and Lee (2004). In this content\nmodel, each sentence si is associated with a hidden\ntopic variable Ti which generates the words of the\nsentence. We will use T = (T1, . . . , Tn) to refer to\nthe hidden topic sequence for a document. We ﬁx\nthe number of topics to a pre-speciﬁed constant K.\n3.2\nModel Overview\nOur model, depicted in Figure 1, proceeds as fol-\nlows:\nFirst the document-level HMM generates\na hidden content topic sequence T for the sen-\ntences of a document. This content component is\nparametrized by θ and decomposes in the standard\n2In Section 3.6, we discuss how this framework can be used\nfor other text analysis applications.\n3Note that each yi is a label sequence across the words in si,\nrather than an individual label.\ny1\ni\nym\ni\ny2\ni\n. . .\nTi\nw1\ni\nwm\ni\nw2\ni\n. . .\nTi−1\nTi+1\n(w2\ni = pleased) ∧(Ti = 3)\nw2\ni = pleased...\nsi\nFigure 1: A graphical depiction of our model for\nsequence labeling tasks. The Ti variable represents\nthe content model topic for the ith sentence si. The\nwords of si, (w1\ni , . . . , wm\ni ), each have a task label\n(y1\ni , . . . , ym\ni ).\nNote that each token label has an\nundirected edge to a factor containing the words of\nthe current sentence, si as well as the topic of the\ncurrent sentence Ti.\nHMM fashion:4\nPθ(s, T ) =\nn\nY\ni=1\nPθ(Ti|Ti−1)\nY\nw∈si\nPθ(w|Ti)\n(1)\nThen the label sequences for each sentence in\nthe document are independently modeled as CRFs\nwhich condition on both the sentence features and\nthe sentence topic:\nPφ(y|s, T ) =\nn\nY\ni=1\nPφ(yi|si, Ti)\n(2)\nEach sentence CRF is parametrized by φ and takes\nthe standard form:\nPφ(y|s, T) ∝\nexp\n\n\n\nX\nj\nφT \u0002\nfN(yj, s, T) + fE(yj, yj+1)\n\u0003\n\n\n\n4We also utilize a hierarchical emission model so that each\ntopic distribution interpolates between a topic-speciﬁc distribu-\ntion as well as a shared background model; this is intended to\ncapture domain-speciﬁc stop words.\n",
  "speaker": "SYSTEM",
  "uuid": "a5e4f79a-2751-44d6-9252-611ca6fd3c98"
}