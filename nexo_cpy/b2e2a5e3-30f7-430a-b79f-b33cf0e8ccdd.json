{
  "message": "SYSTEM: Wiki: https://delicias.dia.fi.upm.es/wiki/index.php/Preguntas_de_investigaci%C3%B3n \n\nResearch questions\n\nLicenses are natural language documents with a legal value and referring to actual laws.\nCan a RDF representation of licenses help its understanding for both humans and machines?\nCan licenses for Linked Data and policy expressions for conditional access to Linked Data be more expressive if represented as ODRL rules whose evaluation depend on context acquired from the web of data?\nRelevance\n\nLicensing, provenance and securing technologies are enablers of Linked Data as an industry-ready resource. The adoption of licensed Linked Data in commercial settings may boost the amount of available resources in the web of data, boosting its usefulness and increasing its outreach.\nResearch hypotheses\n\nH1 - There is a need for rights expressions more complex than CreativeCommons, in particular, embracing non-open terms which can demand for payments.\nH2 - The RDF representation of a license, if properly authored, agreed upon and signed, may eventually have a legal value in case of legal conflict.\nH3 - Much as CreativeCommons licenses have gained widespread adoption, Linked Data publishers will adopt more complex rights expressions if provided with the correct tools.\nNandana Mihindukulasooriya\n\nResearch questions\n\nIs it possible to define a RESTful transaction model that is suitable for Linked Data based application integration scenarios?\nRelevance\n\nLinked Data-based application integration is getting the traction in the enterprise as a novel approach for integrating data intensive applications because of the benefits that Linked Data and the Semantic Web technologies brings.  Along these lines the W3C Linked Data Platform (LDP) initiative provides a RESTful protocol for accessing read/write Linked Data.\nDespite of the benefits it brings, one of the main barriers for this approach being adopted in the industry is the lack for support for some enterprise quality of service requirements such as transaction support. The objective of the thesis is bridge this gap by developing a transaction model for Linked Data based applications that can be used in Enterprise Application Integration scenarios.\nResearch hypotheses\n\nA transaction model can be defined that is RESTful and provide transactional gurantees required by the Linked Data based Enterprise Application Integration scenarios.\nMaría Poveda\n\nResearch questions\n\nWhere and how can vocabularies be found?\nWhich vocabularies or elements should be reused?\nHow much information should be reused?\nHow to reuse elements or vocabularies?\nHow to link elements or vocabularies?\nHow should terms be created according to LD and ontological principles?\nRelevance\n\nThe uptake of Linked Data (LD) has promoted the proliferation of datasets and their associated ontologies for describing different domains. \nIn this regard, rescribing data by means of vocabularies or ontologies is crucial for the Semantic Web and LD realization. Particular LD development characteristics such as agility and web-based architecture necessitate the revision, adaption, and lightening of existing methodologies for ontology development. This thesis proposes a lightweight method for ontology development in an LD context which will be based in data-driven agile de-velopments, existing resources to be reused, and the evaluation of the obtained products considering both classical ontological engineering principles and LD characteristics. This method is intended to guide LD practitioners through the process of creating a vocabulary to describe their data.\nResearch hypotheses\n\nH1 Tailoring current methodologies for ontology development by adapting them to the particular characteristic of Linked Data developments will improve the quality of the resulting ontologies as well as reduce the time needed for the development.\nH2 Reducing the number of intermediate products will reduce the time consumed during the ontnology development.\nH3 Reducing the number of interactions between ontological engineers and users will reduce the time consumed during the ontnology development.\nAssumptions\n\nThe ontology requirements taken as input for the method can be directly transformed into sparql queries.\nRestrictions\n\nI will only consider  the development of ontologies that are going to be published as or used to describe data that is going to be published in RDF as Linked Data. Covering particular requiremnts or characteristics out of the Linked Data use case are out of the scope of this thesis.\nThe method will only consider the steps of developing the ontology within the Linked Data life cycle, letting out of the scope of this thesis the rest of the activities involved in a Linked Data project, for example RDF data generation, linking and publishing activities.\nThe evaluation of the work is restricted to the use of the results in real cases, which provide feedback, and to the execution of controlled experiments that use the results of this thesis\nMariano Rico\n\nResearch questions\n\nQ1. An intuitive user interface would increase the interest of end-users and developers in Linked Open Data?\nQ1.1 What are the most frequent queries?\nQ1.1.1 Can we distinguish queries made by human beings of queries made by programmers?\nQ1.2 What are the most used properties/classes?\nQ2. Do SPARQL queries depend on the native language of the user?\nQ3. Do SPARQL queries increase their complexity over time?\nRelevance\n\nRQ1: If Q1 is true, an intuitive user interface would increase the development of LOD applications.\nRQ2: If Q2 is true, language-specific interfaces should be provided.\nRQ3: If Q3 is true, we would have an evidence of the increasing usage of the LOD cloud.\nResearch hypotheses\n\nRH1Q1: An intuitive user interface would increase the number of query types.\nRH1Q1.1 We can define \"query type\", compare query types and provide a metric.\nRH1Q1.1.1 We can find a common pattern for queries made by human beings.\nRH1Q1.2 We can extract the properties/classes used by a given SPARQL query.\nRH1Q2. We can see a significative different between the SPARQL queries types depending on the native language of the user.\nRH1Q3. We can see that the number of SPARQL query types (and/or some other complexity metric) increases over time.\nAssumptions\n\nAQ1. LOD and NLP is interesting to academia/companies.\nAQ1.1 We have a representative set of data, not only DBpedia.\nAQ1.1.1 We find representative features of timestamps and query content.\nAQ1.2 We have technology to solve this problem.\nAQ2. We have a significative set of multilingual SPARQL queries\nAQ3. We have a significative set of multilingual SPARQL queries\nAlejandro Llaves\n\nResearch questions\n\nWhat technologies are suited to optimize query processing over RDF data streams?\nWhich set of query operators are optimizable in the context of Linked Stream Data?\nAre there features of Linked Geospatial Data that make its processing more parallelizable?\nRelevance\n\nThere is a growing number of applications that depend on the usage of real-time spatio-temporal data for decision making. One example would be real-time geomarketing, where decisions on offering discount coupons to customers may be made on short time slots based on the combination of a set of spatio-temporal data streams coming from different providers, e.g. public transport card validations or weather information. Extracting information from these streams is complex because of the heterogeneity of the data, the rate of data generation, and the volume. To tap these data sources accordingly and get relevant information, scalable processing infrastructures are required, as well as approaches to enable data integration and fusion.\nResearch hypotheses\n\nH1 - Given a RDF streaming engine, a SPARQL query, a set of RDF data streams, and finite computing resources, it is possible to define a processing strategy that optimizes the time and resources used to provide a response in near real-time.\nH2 - Using an adaptive query processing strategy, a RDF streaming engine offers better performance against varying input data rates, requests, and system conditions.\nJavier D. Fernández\n\nResearch hypotheses",
  "speaker": "SYSTEM",
  "uuid": "b2e2a5e3-30f7-430a-b79f-b33cf0e8ccdd"
}