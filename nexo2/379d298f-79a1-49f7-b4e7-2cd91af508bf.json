{
  "message": "SYSTEM: Barzilay2004.pdf: Página 5\non the information-ordering task8. We found that across\nall domains, the optimal models were based on “sharper”\nlanguage models (e.g.,\n \n\t\n\u0017\u0001\u0000\u0011\n\u0000\u0002\u0000\u0003\u0000\u0002\u0000\u0003\u0000\u0002\u0000' ). The optimal\nnumber of states ranged from 32 to 95.\n5.3\nOrdering Experiments\n5.3.1\nMetrics\nThe intent behind our ordering experiments is to test\nwhether content models assign high probability to ac-\nceptable sentence arrangements. However, one stumbling\nblock to performing this kind of evaluation is that we do\nnot have data on ordering quality: the set of sentences\nfrom an\n\u0004\n-sentence document can be sequenced in\n\u0004\u0006\u0005\ndifferent ways, which even for a single text of moder-\nate length is too many to ask humans to evaluate. For-\ntunately, we do know that at least the original sentence\norder (OSO) in the source document must be acceptable,\nand so we should prefer algorithms that assign it high\nprobability relative to the bulk of all the other possible\npermutations. This observation motivates our ﬁrst evalu-\nation metric: the rank received by the OSO when all per-\nmutations of a given document’s sentences are sorted by\nthe probabilities that the model under consideration as-\nsigns to them. The best possible rank is 0, and the worst\nis\n\u0004\u0006\u0005\n(\n' .\nAn additional difﬁculty we encountered in setting up\nour evaluation is that while we wanted to compare our\nalgorithms against Lapata’s (2003) state-of-the-art sys-\ntem, her method doesn’t consider all permutations (see\nbelow), and so the rank metric cannot be computed for it.\nTo compensate, we report the OSO prediction rate, which\nmeasures the percentage of test cases in which the model\nunder consideration gives highest probability to the OSO\nfrom among all possible permutations; we expect that a\ngood model should predict the OSO a fair fraction of the\ntime. Furthermore, to provide some assessment of the\nquality of the predicted orderings themselves, we follow\nLapata (2003) in employing Kendall’s\n\u0007 , which is a mea-\nsure of how much an ordering differs from the OSO—\nthe underlying assumption is that most reasonable sen-\ntence orderings should be fairly similar to it. Speciﬁcally,\nfor a permutation\n\b\nof the sentences in an\n\u0004\n-sentence\ndocument,\n\u0007\n\u0015\t\b\n\u0018 is computed as\n\u0007\n\u0015\n\b\n\u0018\n\u0006\n')(\f\u000b\u000e\r\n\u0015\n\b\n\u0018\n\u000f\u0011\u0010\n\f\u0013\u0012\n\u000e\nwhere\n\r\r\u0015\t\b\n\u0018\nis the number of swaps of adjacent sen-\ntences necessary to re-arrange\n\b into the OSO. The metric\nranges from -1 (inverse orders) to 1 (identical orders).\n8See Section 5.5 for discussion of the relation between the\nordering and the summarization task.\n5.3.2\nResults\nFor each of the 500 unseen test texts, we exhaustively\nenumerated all sentence permutations and ranked them\nusing a content model from the corresponding domain.\nWe compared our results against those of a bigram lan-\nguage model (the baseline) and an improved version of\nthe state-of-the-art probabilistic ordering method of La-\npata (2003), both trained on the same data we used.\nLapata’s method ﬁrst learns a set of pairwise sentence-\nordering preferences based on features such as noun-verb\ndependencies. Given a new set of sentences, the latest\nversion of her method applies a Viterbi-style approxima-\ntion algorithm to choose a permutation satisfying many\npreferences (Lapata, personal communication).9\nTable 2 gives the results of our ordering-test compari-\nson experiments. Content models outperform the alterna-\ntives almost universally, and often by a very wide margin.\nWe conjecture that this difference in performance stems\nfrom the ability of content models to capture global doc-\nument structure. In contrast, the other two algorithms\nare local, taking into account only the relationships be-\ntween adjacent word pairs and adjacent sentence pairs,\nrespectively. It is interesting to observe that our method\nachieves better results despite not having access to the lin-\nguistic information incorporated by Lapata’s method. To\nbe fair, though, her techniques were designed for a larger\ncorpus than ours, which may aggravate data sparseness\nproblems for such a feature-rich method.\nTable 3 gives further details on the rank results for our\ncontent models, showing how the rank scores were dis-\ntributed; for instance, we see that on the Earthquakes do-\nmain, the OSO was one of the top ﬁve permutations in\n95% of the test documents. Even in Drugs and Accidents\n— the domains that proved relatively challenging to our\nmethod — in more than 55% of the cases the OSO’s rank\ndid not exceed ten. Given that the maximal possible rank\nin these domains exceeds three million, we believe that\nour model has done a good job in the ordering task.\nWe also computed learning curves for the different do-\nmains; these are shown in Figure 2. Not surprisingly, per-\nformance improves with the size of the training set for all\ndomains. The ﬁgure also shows that the relative difﬁculty\n(from the content-model point of view) of the different\ndomains remains mostly constant across varying training-\nset sizes. Interestingly, the two easiest domains, Finance\nand Earthquakes, can be thought of as being more for-\nmulaic or at least more redundant, in that they have the\nhighest token/type ratios (see Table 1) — that is, in these\ndomains, words are repeated much more frequently on\naverage.\n9Finding the optimal such permutation is NP-complete.\n",
  "speaker": "SYSTEM",
  "uuid": "379d298f-79a1-49f7-b4e7-2cd91af508bf"
}