{
  "message": "SYSTEM: Sauper2007.pdf: Página 9\nMulti-label\nBinary labels\nF1\nF2\nPrec.\nRecall\nF1\nF2\nPrec.\nRecall\nROUGE\nNoCM\n18.9%\n18.0%\n20.4%\n17.5%\n35.1%\n33.6%\n38.1%\n32.6%\n43.8%\nIndepCM\n24.5%\n23.8%\n25.8%†*\n23.3%†*\n43.0%\n41.8%\n45.3%†*\n40.9%†*\n47.4%†*\nJointCM\n28.2%\n31.3%\n24.3%†\n33.7%†*\n47.8%\n53.0%\n41.2%†\n57.1%†*\n47.6%†*\nTable 5: Results for multi-aspect summarization on the Amazon corpus. Marked ROUGE, precision, and\nrecall are statistically signiﬁcant with p < 0.05: * over the previous model and † over NoCM.\nvantages of jointly learning the content model in the\ncontext of the underlying task.\nComparison with additional context features\nOne alternative to an explicit content model is to\nsimply incorporate additional features into NoCM\nas a proxy for contextual information.\nIn the\nmulti-aspect summarization case, this can be accom-\nplished by adding unigram features from the sen-\ntences before and after the current one.6\nWhen testing this approach, however, the perfor-\nmance of NoCM actually decreases on both Ama-\nzon (to 15.0% F1) and Yelp (to 24.5% F1) corpora.\nThis result is not surprising for this particular task –\nby adding these features, we substantially increase\nthe feature space without increasing the amount of\ntraining data.\nAn advantage of our approach is\nthat our learned representation of context is coarse,\nand we can leverage large quantities of unannotated\ntraining data.\nImpact of content model quality on task per-\nformance\nIn the multi-aspect sentiment ranking\ntask, we have access to gold standard document-\nlevel content structure annotation. This affords us\nthe ability to compare the ideal content structure,\nprovided by the document authors, with one that is\nlearned automatically. As Table 3 shows, the manu-\nally created document structure segmentation yields\nthe best results. However, the performance of our\nJointCM model is not far behind the gold standard\ncontent structure.\nThe quality of the induced content model is de-\ntermined by the amount of training data. As Fig-\nure 4 shows, the multi-aspect summarizer improves\nwith the increase in the size of raw data available for\nlearning content model.\n6This type of feature is not applicable to our multi-aspect\nsentiment ranking task, as we already use unigram features from\nthe entire document.\n10\n20\n30\n0%\n50%\n100%\nMulti-label F1\nPercentage of unlabeled data\n22.8\n26.0\n28.2\nFigure 4: Results on the Amazon corpus using the\ncomplete annotated set with varying amounts of ad-\nditional unlabeled data.7\nCompensating for annotation sparsity\nWe hy-\npothesize that by incorporating rich contextual in-\nformation, we can reduce the need for manual task\nannotation. We test this by reducing the amount of\nannotated data available to the model and measur-\ning performance at several quantities of unannotated\ndata. As Figure 5 shows, the performance increase\nachieved by doubling the amount of annotated data\ncan also be achieved by adding only 12.5% of the\nunlabeled data.\n6\nConclusion\nIn this paper, we demonstrate the beneﬁts of incor-\nporating content models in text analysis tasks. We\nalso introduce a framework to allow the joint learn-\ning of an unsupervised latent content model with a\nsupervised task-speciﬁc model. On multiple tasks\nand datasets, our results empirically connect model\nquality and task performance, suggesting that fur-\n7Because we append the unlabeled versions of the labeled\ndata to the unlabeled set, even with 0% additional unlabeled\ndata, there is a small data set to train the content model.\n",
  "speaker": "SYSTEM",
  "uuid": "4a99fb73-c375-4d1a-88c0-72d655aa9099"
}