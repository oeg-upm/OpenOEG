{
  "message": "SYSTEM: Barzilay2004.pdf: Página 3\nThe Athens seismological institute said the temblor’s epi-\ncenter was located 380 kilometers (238 miles) south of\nthe capital.\nSeismologists in Pakistan’s Northwest Frontier Province\nsaid the temblor’s epicenter was about 250 kilometers\n(155 miles) north of the provincial capital Peshawar.\nThe temblor was centered 60 kilometers (35 miles) north-\nwest of the provincial capital of Kunming, about 2,200\nkilometers (1,300 miles) southwest of Beijing, a bureau\nseismologist said.\nFigure 1: Samples from an earthquake-articles sentence\ncluster, corresponding to descriptions of location.\n\u0000\u0013\u0001\u0003\u0002\n\t\n\u0002\n\u0003\u0016\u0015\n\b\n\u0001\u0005\u0004\n\b\n\u0001\u0007\u0006\n\t\n\u0018 . Estimating the state bigram proba-\nbilities\n\u0002\n\u0003\n\u0015\n\b\t\b\n\u0004\n\b\n\u0018 is described below.\nInitial topic induction\nAs in previous work (Florian\nand Yarowsky, 1999; Iyer and Ostendorf, 1996; Wu and\nKhudanpur, 2002), we initialize the set of “topics”, distri-\nbutionally construed, by partitioning all of the sentences\nfrom the documents in a given domain-speciﬁc collection\ninto clusters. First, we create\n\n clusters via complete-link\nclustering, measuring sentence similarity by the cosine\nmetric using word bigrams as features (Figure 1 shows\nexample output).4 Then, given our knowledge that docu-\nments may sometimes discuss new and/or irrelevant con-\ntent as well, we create an “etcetera” cluster by merging\ntogether all clusters containing fewer than\n\u000b\nsentences,\non the assumption that such clusters consist of “outlier”\nsentences. We use\n\f\nto denote the number of clusters\nthat results.\nDetermining states, emission probabilities, and transi-\ntion probabilities\nGiven a set\n\r\n\t\u000f\u000e\n\r\n\f\u0010\u000e\u0012\u0011\u0013\u0011\u0012\u0011\u0013\u000e\n\r\u0015\u0014\nof\n\f\nclus-\nters, where\n\r\u0012\u0014\nis the “etcetera” cluster, we construct a\ncontent model with corresponding states\n\u0001\n\t\n\u000e\n\u0001\n\f\n\u000e\u0012\u0011\u0013\u0011\u0012\u0011\u0013\u000e\n\u0001\n\u0014 ;\nwe refer to\n\u0001\n\u0014\nas the insertion state.\nFor each state\n\u0001\n\u0001\n,\n\u0016\u0018\u0017\u0019\f\n, bigram probabilities (which\ninduce the state’s sentence-emission probabilities) are es-\ntimated using smoothed counts from the corresponding\ncluster:\n\u0002\n\u0003\u001b\u001a$\u0015\n\b\n\b\n\u0004\n\b\n\u0018\n\u001a\u000b\u001c\n\u001d\n\u0006\n\u001c\u0010\u001d\n\u001a\u000b\u0015\n\b\n\b\t\b\n\u0018\u001f\u001e! \n\t\n\u001c\u0010\u001d\n\u001a\u000b\u0015\n\b\n\u0018\"\u001e! \n\t\n\u0004\n#$\u0004\n\u000e\nwhere\n\u001c\n\u001d\n\u001a\n\u0015\u0007%\n\u0018 is the frequency with which word sequence\n%\noccurs within the sentences in cluster\n\r\n\u0001\n, and\n#\nis the\nvocabulary. But because we want the insertion state\n\u0001\u0010\u0014\nto model digressions or unseen topics, we take the novel\nstep of forcing its language model to be complementary\nto those of the other states by setting\n\u0002\n\u0003\u001b&\n\u0015\n\b\n\b\n\u0004\n\b\n\u0018\n\u001a\u000b\u001c\u001e\u001d\n\u0006\n')(+*-,\u0010.\n\u00010/\n\u000121\n\u0014\n\u0002\n\u0003\n\u001a\u0011\u0015\n\b\t\b\n\u0004\n\b\n\u0018\n3547698\n\u0015\n':(+*-,\u0010.\n\u0001;/\n\u000121\n\u0014\n\u0002\n\u0003\u001b\u001a\u000b\u00152<\n\u0004\n\b\n\u0018\u001e\u0018\n\u0011\n4Following Barzilay and Lee (2003), proper names, num-\nbers and dates are (temporarily) replaced with generic tokens to\nhelp ensure that clusters contain sentences describing the same\nevent type, rather than same actual event.\nNote that the contents of the “etcetera” cluster are ignored\nat this stage.\nOur state-transition probability estimates arise from\nconsidering how sentences from the same article are dis-\ntributed across the clusters. More speciﬁcally, for two\nclusters\n\r and\n\r\n\b , let\n=\n\u0015>\r\n\u000e\n\r\n\b\n\u0018 be the number of documents\nin which a sentence from\n\r\nimmediately precedes one\nfrom\n\r\n\b , and let\n=\n\u00152\r\u0010\u0018 be the number of documents con-\ntaining sentences from\n\r . Then, for any two states\n\u0001\n\u0001\nand\n\u0001\u0015? ,\n\u0016\n\u000e\u001b@BA\n\f\n, we use the following smoothed estimate of\nthe probability of transitioning from\n\u0001\n\u0001\nto\n\u0001\u0005? :\n\u0002\n\u0015\n\u0001\u0015?\n\u0004\n\u0001\n\u0001\n\u0018\n\u0006\n=\n\u00152\r\n\u0001\n\u000e\n\r\n?\n\u0018\"\u001eC \n\f\n=\n\u0015>\r\n\u0001\n\u0018\"\u001eC \n\f\n\f\n\u0011\nViterbi re-estimation\nOur initial clustering ignores\nsentence order; however, contextual clues may indicate\nthat sentences with high lexical similarity are actually on\ndifferent “topics”. For instance, Reuters articles about\nearthquakes frequently ﬁnish by mentioning previous\nquakes. This means that while the sentence “The temblor\ninjured dozens” at the beginning of a report is probably\nhighly salient and should be included in a summary of it,\nthe same sentence at the end of the piece probably refers\nto a different event, and so should be omitted.\nA natural way to incorporate ordering information is\niterative re-estimation of the model parameters, since the\ncontent model itself provides such information through\nits transition structure. We take an EM-like Viterbi ap-\nproach (Iyer and Ostendorf, 1996): we re-cluster the sen-\ntences by placing each one in the (new) cluster\n\r\n\u0001\n,\n\u0016\nA\n\f\n,\nthat corresponds to the state\n\u0001\n\u0001\nmost likely to have gen-\nerated it according to the Viterbi decoding of the train-\ning data. We then use this new clustering as the input to\nthe procedure for estimating HMM parameters described\nabove. The cluster/estimate cycle is repeated until the\nclusterings stabilize or we reach a predeﬁned number of\niterations.\n4\nEvaluation Tasks\nWe apply the techniques just described to two tasks that\nstand to beneﬁt from models of content and changes in\ntopic: information ordering for text generation and in-\nformation selection for single-document summarization.\nThese are two complementary tasks that rely on dis-\njoint model functionalities: the ability to order a set of\npre-selected information-bearing items, and the ability\nto do the selection itself, extracting from an ordered se-\nquence of information-bearingitems a representative sub-\nsequence.\n4.1\nInformation Ordering\nThe information-ordering task is essential to many text-\nsynthesis applications, including concept-to-text genera-\n",
  "speaker": "SYSTEM",
  "uuid": "3aadf83d-1fe1-4c61-a9db-767688c47e65"
}