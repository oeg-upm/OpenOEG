{
  "message": "SYSTEM: Sauper2007.pdf: Página 6\nthe negative expected loss. The maximal scoring se-\nquence according to these potentials minimizes the\nexpected risk.\n3.5\nLeveraging unannotated data\nOur model allows us to incorporate unlabeled doc-\numents, denoted DU, to improve the learning of the\ncontent model. For an unlabeled document we only\nobserve the document text s and assume it is drawn\nfrom the same content model as our labeled docu-\nments. The objective presented in Section 3.3 as-\nsumed that all documents were labeled; here we sup-\nplement this objective by capturing the likelihood\nof unlabeled documents according to the content\nmodel:\nLU(θ) =\nX\ns∈DU\nlog Pθ(s)\n=\nX\ns∈DU\nlog\nX\nT\nPθ(s, T )\nOur overall objective function is to maximize the\nlikelihood of both our labeled and unlabeled data.\nThis objective corresponds to:\nL(φ, θ) =LU(θ) + LL(φ, θ)\nThis objective can also be optimized using the EM\nalgorithm, where the E-Step for labeled and unla-\nbeled documents is outlined above.\n3.6\nGeneralization\nThe approach outlined can be applied to a wider\nrange of task components.\nFor instance, in Sec-\ntion 4.1 we apply this approach to multi-aspect sen-\ntiment analysis. In this task, the target y consists of\nnumeric sentiment ratings (y1, . . . , yK) for each of\nK aspects. The task component consists of indepen-\ndent linear regression models for each aspect sen-\ntiment rating. For the content model, we associate\na topic with each paragraph; T consists of assign-\nments of topics to each document paragraph.\nThe model structure still decomposes as in Fig-\nure 2, but the details of learning are slightly differ-\nent. For instance, because the task label (aspect sen-\ntiment ratings) is not localized to any region of the\ndocument, all content model variables inﬂuence the\ntarget response. Conditioned on the target label, all\ntopic variables become correlated. Thus when learn-\ning, the E-Step requires computing a posterior over\nparagraph topic tuples T :\nP(T |y, s) ∝P(s, T )P(y|T , s)\nFor the case of our multi-aspect sentiment task, this\ncomputation can be done exactly by enumerating\nT tuples, since the number of sentences and pos-\nsible topics is relatively small. If summation is in-\ntractable, the posterior may be approximated using\nvariational techniques (Bishop, 2006), which is ap-\nplicable to a broad range of potential applications.\n4\nExperimental Set-Up\nWe apply our approach to two text analysis tasks that\nstand to beneﬁt from modeling content structure:\nmulti-aspect sentiment analysis and multi-aspect re-\nview summarization.\n4.1\nTasks\nIn the following section, we deﬁne each task in de-\ntail, explain the task-speciﬁc adaptation of the model\nand describe the data sets used in the experiments.\nTable 2 summarizes statistics for all the data sets.\nFor all tasks, when using a content model with a\ntask model, we utilize a new set of features which\ninclude all the original features as well as a copy\nof each feature conjoined with the content topic as-\nsignment (see Figure 1).\nWe also include a fea-\nture which indicates whether a given word was most\nlikely emitted from the underlying topic or from a\nbackground distribution.\nMulti-Aspect Sentiment Ranking\nThe goal of\nmulti-aspect sentiment classiﬁcation is to predict a\nset of numeric ranks that reﬂects the user satisfaction\nfor each aspect (Snyder and Barzilay, 2007). One of\nthe challenges in this task is to attribute sentiment-\nbearing words to the aspects they describe. Informa-\ntion about document structure has the potential to\ngreatly reduce this ambiguity.\nFollowing\nstandard\nsentiment\nranking\nap-\nproaches (Wilson et al., 2004; Pang and Lee, 2005;\nGoldberg and Zhu, 2006; Snyder and Barzilay,\n2007), we employ ordinary linear regression to\nindependently map bag-of-words representations\ninto predicted aspect ranks.\nIn addition to com-\nmonly used lexical features, this set is augmented\n",
  "speaker": "SYSTEM",
  "uuid": "7e47bb72-6565-46c7-aca9-ec8f3aa53ce5"
}