{
  "message": "SYSTEM: Barzilay2004.pdf: Página 2\nrization: the compression of a document by choosing\na subsequence of its sentences.\nFor this task, we de-\nvelop a new content-model-based learning algorithm for\nsentence selection. The resulting summaries yield 88%\nmatch with human-written output, which compares fa-\nvorably to the 69% achieved by the standard “leading\n\u0000sentences” baseline.\nThe success of content models in these two comple-\nmentary tasks demonstrates their ﬂexibility and effective-\nness, and indicates that they are sufﬁciently expressive to\nrepresent important text properties. These observations,\ntaken together with the fact that content models are con-\nceptually intuitive and efﬁciently learnable from raw doc-\nument collections, suggest that the formalism can prove\nuseful in an even broader range of applications than we\nhave considered here; exploring the options is an appeal-\ning line of future research.\n2\nRelated Work\nKnowledge-rich methods\nModels employing manual\ncrafting of (typically complex) representations of content\nhave generally captured one of three types of knowledge\n(Rambow, 1990; Kittredge et al., 1991): domain knowl-\nedge [e.g., that earthquakes have magnitudes], domain-\nindependent communication knowledge [e.g., that de-\nscribing an event usually entails specifying its location];\nand domain communication knowledge [e.g., that Reuters\nearthquake reports often conclude by listing previous\nquakes2]. Formalisms exemplifying each of these knowl-\nedge types are DeJong’s (1982) scripts, McKeown’s\n(1985) schemas, and Rambow’s (1990) domain-speciﬁc\nschemas, respectively.\nIn contrast, because our models are based on a dis-\ntributional view of content, they will freely incorporate\ninformation from all three categories as long as such in-\nformation is manifested as a recurrent pattern. Also, in\ncomparison to the formalisms mentioned above, content\nmodels constitute a relatively impoverished representa-\ntion; but this actually contributes to the ease with which\nthey can be learned, and our empirical results show that\nthey are quite effective despite their simplicity.\nIn recent work, Duboue and McKeown (2003) propose\na method for learning a content planner from a collec-\ntion of texts together with a domain-speciﬁc knowledge\nbase, but our method applies to domains in which no such\nknowledge base has been supplied.\nKnowledge-lean approaches\nDistributional models of\ncontent have appeared with some frequency in research\non text segmentation and topic-based language modeling\n(Hearst, 1994; Beeferman et al., 1997; Chen et al., 1998;\nFlorian and Yarowsky, 1999; Gildea and Hofmann, 1999;\n2This does not qualify as domain knowledge because it is\nnot about earthquakes per se.\nIyer and Ostendorf, 1996; Wu and Khudanpur, 2002). In\nfact, the methods we employ for learning content models\nare quite closely related to techniques proposed in that\nliterature (see Section 3 for more details).\nHowever, language-modeling research — whose goal\nis to predict text probabilities — tends to treat topic as a\nuseful auxiliary variable rather than a central concern; for\nexample, topic-based distributional information is gener-\nally interpolated with standard, non-topic-based\n\u0000 -gram\nmodels to improve probability estimates. Our work, in\ncontrast, treats content as a primary entity. In particular,\nour induction algorithms are designed with the explicit\ngoal of modeling document content, which is why they\ndiffer from the standard Baum-Welch (or EM) algorithm\nfor learning Hidden Markov Models even though content\nmodels are instances of HMMs.\n3\nModel Construction\nWe employ an iterative re-estimation procedure that al-\nternates between (1) creating clusters of text spans with\nsimilar word distributions to serve as representatives of\nwithin-document topics, and (2) computing models of\nword distributions and topic changes from the clusters so\nderived.3\nFormalism preliminaries\nWe treat texts as sequences\nof pre-deﬁned text spans, each presumed to convey infor-\nmation about a single topic. Specifying text-span length\nthus deﬁnes the granularity of the induced topics. For\nconcreteness, in what follows we will refer to “sentences”\nrather than “text spans” since that is what we used in our\nexperiments, but paragraphs or clauses could potentially\nhave been employed instead.\nOur working assumption is that all texts from a given\ndomain are generated by a single content model. A con-\ntent model is an HMM in which each state\n\u0001 corresponds\nto a distinct topic and generates sentences relevant to that\ntopic according to a state-speciﬁc language model\n\u0002\u0004\u0003 —\nnote that standard\n\u0000 -gram language models can there-\nfore be considered to be degenerate (single-state) content\nmodels. State transition probabilities give the probability\nof changing from a given topic to another, thereby cap-\nturing constraints on topic shifts. We can use the forward\nalgorithm to efﬁciently compute the generation probabil-\nity assigned to a document by a content model and the\nViterbi algorithm to quickly ﬁnd the most likely content-\nmodel state sequence to have generated a given docu-\nment; see Rabiner (1989) for details.\nIn our implementation, we use bigram language mod-\nels, so that the probability of an\n\u0000 -word sentence\n\u0005\u0007\u0006\n\b\n\t\u000b\b\r\f\u000f\u000e\u0010\u000e\u0011\u000e\u0012\b\u0014\u0013\nbeing generated by a state\n\u0001\nis\n\u0002\u0004\u0003\u0016\u0015\u0017\u0005\u0019\u0018\u001b\u001a\u000b\u001c\u001e\u001d\n\u0006\n3For clarity, we omit minor technical details, such as the use\nof dummy initial and ﬁnal states. Section 5.2 describes how the\nfree parameters\n\u001f ,\n ,\n!\u0016\" , and\n!$# are chosen.\n",
  "speaker": "SYSTEM",
  "uuid": "8e3f059c-61a2-4fed-bdfc-c5220f35dacb"
}