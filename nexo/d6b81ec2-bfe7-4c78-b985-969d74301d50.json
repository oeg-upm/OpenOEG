{
  "message": "SYSTEM: Wiki: https://delicias.dia.fi.upm.es/wiki/index.php/Evaluaci%C3%B3n_editores_de_ontolog%C3%ADas \n\nEvaluation features\nSeveral groups of persons (ontology editors) working on the development of an ontology in a very well known domain (e.g., if in biology, a set of biologists creating an ontology about plants, for instance). The ideal would be two groups per ontology edition tool selected in the comparison (e.g., if you select Protégé, NeOn toolkit and TopBraid, which is a fair comparison, then you would have 8 groups).\nThese groups have to be trained in a similar time in all the tools, and they should have no experience in ontology editing tools before.\nYou provide a fixed set of ontology editing tasks to them (how many concepts have to be generated, about what, which information sources they can use, etc.). In order to be fair and not to impose differences between tools given the order in which they are selected and used, a target ontology should be clearly explained to them, so that they can replicate it somehow.\nThen each group starts working, with a maximum time to be spent, and they record the time needed, and you assess the quality and heavyweightness of the ontologies generated. This has to be done for all groups for all tools. That is:\nStep 1: groups 1 and 2 (tool1), groups 3 and 4 (tool2), groups 5 and 6 (tool3), groups 7 and 8 (tool4).\nStep 2: groups 1 and 2 (tool2), groups 3 and 4 (tool3), groups 5 and 6 (tool4), groups 7 and 8 (tool1).\nEtc.\nThen you pass them questionnaires according to your research hypothesis (if you said the that Morphster is more usable, then you pass them a SUMI-related questionnaire, if you said that it is more efficient, then you don’t need questionnaires, since you can get everything from them, if you say that it is less error-prone, then you check, etc.).",
  "speaker": "SYSTEM",
  "uuid": "d6b81ec2-bfe7-4c78-b985-969d74301d50"
}