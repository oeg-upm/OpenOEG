{
  "message": "SYSTEM: Wiki: Fill your research question according to this structure:\nName:\nYour name\nResearch questions:\nWhat research questions are you addressing?\nRelevance:\nWhy are these questions relevant?\nResearch hypotheses:\nWhat are the hypotheses that are related to the research questions?\nContents\n[\nhide\n]\n1\nFilip Radulovic\n1.1\nResearch questions\n1.2\nRelevance\n1.3\nResearch hypotheses\n2\nIdafen Santana\n2.1\nResearch questions\n2.2\nRelevance\n2.3\nResearch hypotheses\n3\nOlga Ximena Giraldo Pasmín\n3.1\nResearch questions\n3.2\nRelevance\n3.3\nResearch hypotheses\n4\nDaniel Garijo Verdejo\n4.1\nResearch questions\n4.2\nRelevance\n4.3\nResearch hypotheses\n5\nFreddy Priyatna\n5.1\nResearch questions\n5.2\nRelevance\n5.3\nResearch hypotheses\n6\nVíctor Rodríguez-Doncel\n6.1\nResearch questions\n6.2\nRelevance\n6.3\nResearch hypotheses\n7\nNandana Mihindukulasooriya\n7.1\nResearch questions\n7.2\nRelevance\n7.3\nResearch hypotheses\n8\nMaría Poveda\n8.1\nResearch questions\n8.2\nRelevance\n8.3\nResearch hypotheses\n8.4\nAssumptions\n8.5\nRestrictions\n9\nMariano Rico\n9.1\nResearch questions\n9.2\nRelevance\n9.3\nResearch hypotheses\n9.4\nAssumptions\n10\nAlejandro Llaves\n10.1\nResearch questions\n10.2\nRelevance\n10.3\nResearch hypotheses\n11\nJavier D. Fernández\n11.1\nResearch hypotheses\n11.2\nResearch questions\n11.3\nRelevance\n12\nJorge Gracia\n12.1\nResearch questions\n12.2\nRelevance\n12.3\nResearch hypotheses\n13\nAlmudena Ruiz-Iniesta\n13.1\nResearch questions\n13.2\nRelevance\n13.3\nResearch hypotheses\n14\nAntonio Sánchez-Padial\n14.1\nResearch questions\n14.2\nRelevance\n14.3\nResearch hypotheses\n14.4\nResearch hypotheses\n15\nMaría Navas-Loro\n15.1\nResearch questions\n15.2\nRelevance\n15.3\nResearch hypotheses\nFilip Radulovic\n\nResearch questions\n\nIs it possible to automate the comparison of alternatives in a process of recommendation by exploiting the results of their evaluation\nIs it possible to adapt existing multiple criteria decision making method to automatically address user quality requirements in a process of recommendation\nRelevance\n\nWhen using a multiple criteria decision making method in a process of recommendation, alternatives to be recommended have to be compared against various criteria and this is usually done by users or experts, which can be expensive and time-consuming. In the cases when alternatives have been previously evaluated, their automatic comparison based on evaluation results can solve this problem.\nFurthermore, since different users have different requirements for the alternatives to be recommended, adapting multiple criteria decision making method to automatically address user quality requirements in a process of recommendation can lead to better recommendations tailored to users' needs.\nResearch hypotheses\n\nH1 - Recommendation framework to be developed in my thesis is not able to rank better those alternatives that satisfy more of user quality requirements\nH2 - Recommendations produced by the recommendation framework to be developed in my thesis are not in line with recommendations made by experts\nIdafen Santana\n\nResearch questions\n\nIs it possible to conserve the knowledge of the execution environment of a scientific workflow?\nIs it possible to use this knowledge to recreate a new execution environment that is able to re-execute the former workflow?\nWhich are the relevant and necessary properties that must be documented in order to achieve the reproduction?\nRelevance\n\nScientific reproducibility is a cornerstone of the scientific method, either in traditional science or in computational science. Nowadays most of the current approaches on computational experiment conservation/preservation and reproducibility focus on how to deal with the data and scientific procedure decay. Many studies show the need of addressing the third pillar of an experiment, that is, the equipment. An approach to achieve it has not been thoughtfully considered yet. In this work we claim that exposing the information about the infrastructure and developing methods for reproducing it in the future based on this information would achieve both the conservation and reproduction of the execution environment.\nResearch hypotheses\n\nThe execution environment of a computational scientific workflow can be conserved for its future reproduction by describing its relevant properties in a standardized way.\nOlga Ximena Giraldo Pasmín\n\nResearch questions\n\nHow to semantically formalize experimental protocols so that sharing and discovering can be supported?\nRelevance\n\nWhy?\nBecause, a protocol is a depiction of a sequence of operations; experimental protocols are usually written in natural language. Generally, they are presented in a “recipe” style providing step-by-step descriptions for processes. Such sequence of tasks and operations in experimental research are fundamental units of knowledge. Investigators follow and generate protocols in their daily activities; as experimental protocols reflect the know-how of a laboratory they are shared and adapted for various purposes. Most importantly, experimental protocols are essential for patenting; they are also central in reproducibility efforts. Several efforts have focused on accurate description of data for interoperability purposes; however, fewer efforts have emphasized on the actual how data was produced.\nResearch hypotheses\n\nThe standardization of laboratory protocols improves the reproducibility of an experiment.\nDaniel Garijo Verdejo\n\nResearch questions\n\nHow to create abstractions in scientific experiments?\nHow do we represent scientific experiment information?\nHow do we link abstract workflow representations to their implementations and executions?\nHow can we detect typical operations in workflows automatically?\nWhat is the relationship among different workflows?\nRelevance\n\nReusing scientific workflows is often difficult. Existent workflows might be too specific for one's purposes, although in many cases parts of these workflows are valuable for reuse (preprocessing steps, visualization steps, etc.). By knowing how different workflows relate to each other (even if it is at an abstract level), a user gains more understanding on the available work by their colleagues, and a designer can be suggested useful fragments that are related to their work.\nIn summary: important for discovery, understanding, exploration, design of scientific workflows.\nResearch hypotheses\n\nIt is possible to identify/extract automatically patterns/abstractions in workflow repositories/catalogs/libraries that are useful for workflow designers and workflow users.\nFreddy Priyatna\n\nResearch questions\n\nIs it possible to formalize SPARQL to SQL query translation for RDB2RDF systems?\nIs it possible to improve the performance SPARQL to SQL query translation for RDB2RDF systems?\nRelevance\n\nMost of the data currently stored in relational database and making them available as RDF datasets, whether materialized or virtual, plays an important role in the emergence of Web of Data. Having the data available as virtual RDF datasets is preferable in many cases, in which many legacy systems still mandate the use of relational databases and the data changes frequently. In these cases, dumping the data into materialized RDF datasets is not a good approach because it is difficult to maintain the consistency of materialized RDF datasets with the data in relational database systems. SPARQL queries used to access the virtual RDF datasets have to be translated into efficients SQL queries so that they can be evaluated over the underlying database, with reasonable peformance. This shows the need and relevance of formalization and optimizations of query translation.\nResearch hypotheses\n\nQuery translation algorithm originally designed for RDB-backed triple stores can be adapted to work with R2RML mappings.\nFurthermore, on some commonly used query patterns, various optimizations can be done to enhance the performance of the algorithm.\nVíctor Rodríguez-Doncel\n\nResearch questions\n\nLicenses are natural language documents with a legal value and referring to actual laws.\nCan a RDF representation of licenses help its understanding for both humans and machines?\nCan licenses for Linked Data and policy expressions for conditional access to Linked Data be more expressive if represented as ODRL rules whose evaluation depend on context acquired from the web of data?\nRelevance\n\nLicensing, provenance and securing technologies are enablers of Linked Data as an industry-ready resource. The adoption of licensed Linked Data in commercial settings may boost the amount of available resources in the web of data, boosting its usefulness and increasing its outreach.\nResearch hypotheses\n\nH1 - There is a need for rights expressions more complex than CreativeCommons, in particular, embracing non-open terms which can demand for payments.\nH2 - The RDF representation of a license, if properly authored, agreed upon and signed, may eventually have a legal value in case of legal conflict.\nH3 - Much as CreativeCommons licenses have gained widespread adoption, Linked Data publishers will adopt more complex rights expressions if provided with the correct tools.\nNandana Mihindukulasooriya\n\nResearch questions\n\nIs it possible to define a RESTful transaction model that is suitable for Linked Data based application integration scenarios?\nRelevance\n\nLinked Data-based application integration is getting the traction in the enterprise as a novel approach for integrating data intensive applications because of the benefits that Linked Data and the Semantic Web technologies brings.  Along these lines the W3C Linked Data Platform (LDP) initiative provides a RESTful protocol for accessing read/write Linked Data.\nDespite of the benefits it brings, one of the main barriers for this approach being adopted in the industry is the lack for support for some enterprise quality of service requirements such as transaction support. The objective of the thesis is bridge this gap by developing a transaction model for Linked Data based applications that can be used in Enterprise Application Integration scenarios.\nResearch hypotheses\n\nA transaction model can be defined that is RESTful and provide transactional gurantees required by the Linked Data based Enterprise Application Integration scenarios.\nMaría Poveda\n\nResearch questions\n\nWhere and how can vocabularies be found?\nWhich vocabularies or elements should be reused?\nHow much information should be reused?\nHow to reuse elements or vocabularies?\nHow to link elements or vocabularies?\nHow should terms be created according to LD and ontological principles?\nRelevance\n\nThe uptake of Linked Data (LD) has promoted the proliferation of datasets and their associated ontologies for describing different domains. \nIn this regard, rescribing data by means of vocabularies or ontologies is crucial for the Semantic Web and LD realization. Particular LD development characteristics such as agility and web-based architecture necessitate the revision, adaption, and lightening of existing methodologies for ontology development. This thesis proposes a lightweight method for ontology development in an LD context which will be based in data-driven agile de-velopments, existing resources to be reused, and the evaluation of the obtained products considering both classical ontological engineering principles and LD characteristics. This method is intended to guide LD practitioners through the process of creating a vocabulary to describe their data.\nResearch hypotheses\n\nH1 Tailoring current methodologies for ontology development by adapting them to the particular characteristic of Linked Data developments will improve the quality of the resulting ontologies as well as reduce the time needed for the development.\nH2 Reducing the number of intermediate products will reduce the time consumed during the ontnology development.\nH3 Reducing the number of interactions between ontological engineers and users will reduce the time consumed during the ontnology development.\nAssumptions\n\nThe ontology requirements taken as input for the method can be directly transformed into sparql queries.\nRestrictions\n\nI will only consider  the development of ontologies that are going to be published as or used to describe data that is going to be published in RDF as Linked Data. Covering particular requiremnts or characteristics out of the Linked Data use case are out of the scope of this thesis.\nThe method will only consider the steps of developing the ontology within the Linked Data life cycle, letting out of the scope of this thesis the rest of the activities involved in a Linked Data project, for example RDF data generation, linking and publishing activities.\nThe evaluation of the work is restricted to the use of the results in real cases, which provide feedback, and to the execution of controlled experiments that use the results of this thesis\nMariano Rico\n\nResearch questions\n\nQ1. An intuitive user interface would increase the interest of end-users and developers in Linked Open Data?\nQ1.1 What are the most frequent queries?\nQ1.1.1 Can we distinguish queries made by human beings of queries made by programmers?\nQ1.2 What are the most used properties/classes?\nQ2. Do SPARQL queries depend on the native language of the user?\nQ3. Do SPARQL queries increase their complexity over time?\nRelevance\n\nRQ1: If Q1 is true, an intuitive user interface would increase the development of LOD applications.\nRQ2: If Q2 is true, language-specific interfaces should be provided.\nRQ3: If Q3 is true, we would have an evidence of the increasing usage of the LOD cloud.\nResearch hypotheses\n\nRH1Q1: An intuitive user interface would increase the number of query types.\nRH1Q1.1 We can define \"query type\", compare query types and provide a metric.\nRH1Q1.1.1 We can find a common pattern for queries made by human beings.\nRH1Q1.2 We can extract the properties/classes used by a given SPARQL query.\nRH1Q2. We can see a significative different between the SPARQL queries types depending on the native language of the user.\nRH1Q3. We can see that the number of SPARQL query types (and/or some other complexity metric) increases over time.\nAssumptions\n\nAQ1. LOD and NLP is interesting to academia/companies.\nAQ1.1 We have a representative set of data, not only DBpedia.\nAQ1.1.1 We find representative features of timestamps and query content.\nAQ1.2 We have technology to solve this problem.\nAQ2. We have a significative set of multilingual SPARQL queries\nAQ3. We have a significative set of multilingual SPARQL queries\nAlejandro Llaves\n\nResearch questions\n\nWhat technologies are suited to optimize query processing over RDF data streams?\nWhich set of query operators are optimizable in the context of Linked Stream Data?\nAre there features of Linked Geospatial Data that make its processing more parallelizable?\nRelevance\n\nThere is a growing number of applications that depend on the usage of real-time spatio-temporal data for decision making. One example would be real-time geomarketing, where decisions on offering discount coupons to customers may be made on short time slots based on the combination of a set of spatio-temporal data streams coming from different providers, e.g. public transport card validations or weather information. Extracting information from these streams is complex because of the heterogeneity of the data, the rate of data generation, and the volume. To tap these data sources accordingly and get relevant information, scalable processing infrastructures are required, as well as approaches to enable data integration and fusion.\nResearch hypotheses\n\nH1 - Given a RDF streaming engine, a SPARQL query, a set of RDF data streams, and finite computing resources, it is possible to define a processing strategy that optimizes the time and resources used to provide a response in near real-time.\nH2 - Using an adaptive query processing strategy, a RDF streaming engine offers better performance against varying input data rates, requests, and system conditions.\nJavier D. Fernández\n\nResearch hypotheses\n\nH1 - Given a set of RDF data streams, it is possible to define an RDF interchange format that optimizes the space and time for the data exchange and parsing.\nH2 - Given an RDF streaming engine, and a set of SPARQL queries, a RDF interchange format can be tuned to offer better performance in data exchange among processing nodes and query resolution.\nResearch questions\n\nIs the compressed format RDF/HDT a good solution for these dynamic data?\nWhich are the particularities/regularities of RDF data streams?\nCan an RDF interchange format be parallelizable for compression and decompression (parsing)?\nRelevance\n\nThere is actually a vast diversity of devices and networks managing data streams; the tremendous success of mobile devices and the emergent Internet of Things are just a small seed of this. RDF streaming is starting to follow the same pattern of deployment. We focus on the efficient transmission of RDF streams, a necessary step to ensure higher throughput for RDF Stream processors. To do so, different representations and compression techniques are required. Previous works on RDF compression show important size reductions of large RDF datasets, hence enabling an efficient RDF exchange. However, these solutions consider static RDF datasets, and need to read the whole dataset to take advantage of data regularities. We put the focus on specific RDF compression for RDF data streams.\nJorge Gracia\n\nResearch questions\n\nHow to reconcile information expressed in different natural languages in the Web of Data?\nHow to create an ecosystem of Linked Data-based Language Resources to support a new generation of LD-aware NLP services?\nRelevance\n\nThe Web of Data is increasingly multilingual. Ideally, users should be able to access information in their own language on the Web of Data, no matter the language in which the information is actually described. To achieve this objective, new methods, techniques and tools are needed to overcome the language barriers among datasets and ontologies on the Web. \nFurther, an emerging ecosystem of Language Resources available as linked data on the Web will largely benefit such objective, allowing also the emergence of a new generation of linked data-aware NLP applications.\nResearch hypotheses\n\nA critical mass of multilingual data and language resources has to be available on the Web (as linked data) to support a new family of multilingual and cross-lingual services (for querying, relation discovery, matching, etc.)\nAlmudena Ruiz-Iniesta\n\nResearch questions\n\nIs it possible to suggest new research ideas to the users in an autonomous manner?\nIs it possible to replicate human creativity in a certain degree by taking advantage of computing power and the Internet to combine the widely accessible resources to generate new concepts with unexpected features?\nRelevance\n\nCreativity is a way in which we examine a problem with an open mind and fresh eyes to explore new possibilities outside the established approaches through the use of our imagination based on knowledge. New technologies, in particular artificial intelligence, are drastically changing the nature of creative processes. Recent studies show significant contributions by the Internet to scientific research. In fact, the heavy presence of all types of research objects on the Web, which are understood as aggregations of scientific items that are important in the context of scientific investigation, such as publications, source code and data in research website, wiki, blog, is now becoming incredibly useful in research and learning. Although the Web has become an extremely rich resource for a huge number of research objects, currently these objects can only be explored manually in research activities due to the lack of effective tools.\nResearch hypotheses\n\nAntonio Sánchez-Padial\n\nResearch questions\n\nHow can the machine-actionable elements that appear in data management plans for agriculture, food and forestry research be used to access and check accomplishment within automatically negotiated services?\nRelevance\n\nThe importance of Data Management Plans is increasing, as awareness of the importance of preservation of research data rises up. However, because of their very nature DMPs can become a burocratic charge for researchers, addind few value to their processes, and be extremely difficult to check by funders, especially for stages in the data lifecycle other than preservation or publishing. Active (or actionable) Data Management Plans are a proposal to be sure that DMP are machine-readable and machine-actionable so that they it's possible to build method to automatically measure their level of accomplishment.\nThis work will propose a way to leverage Active Data Management Plans to provide data managers in research projects with tools able to communicate automatically with data management services.\nResearch hypotheses\n\n1- Given a a model of actions, events and roles in data management plans, and a model to describe the data services provided by differente platforms; it will be possible to develop a map between both model matching requirements in the Data Management Model with data services.\nResearch hypotheses\n\nMaría Navas-Loro\n\nResearch questions\n\nWhat are the particularities of temporal processing in the legal domain?\nAre state-of-the-art generic temporal taggers enough or they need to be adapted to the legal domain?\nShould a brand new approach be considered?\nRelevance\n\nTemporal annotation in the legal domain has not recieved a lot of atention in literature, in contrast to annotation in other kind of documents such as news or historical texts. Nevertheless, being able to process the temporal dimension of legal texts (texts containing legal nuances, such as open data datasets or news) would help in several tasks, such as summarization, timeline generation or time-contraint related norms extraction, and will also be applicable to further applications (such as temporal reasoning on legal texts).\nMy PhD thesis focuses on dealing specifically with judgments and decision courts, both in English and Spanish, annotating and representing it in the form of a timeline. Besides the processing of temporal information, other information surronding temporal expressions or related to their extraction (such as misleading legal references, events or NER) is also in the scope of this thesis, as far as the temporal annotation is concerned.\nResearch hypotheses\n\nA dedicated approach will improve current generic temporal taggers (precision, recall, f-measure).",
  "speaker": "SYSTEM",
  "uuid": "7371cb9f-6cad-4eca-a661-70b2e47d5cbb"
}