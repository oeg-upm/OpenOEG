{
  "message": "SYSTEM: Barzilay2004.pdf: Página 6\nDomain\nSystem\nRank\nOSO\n\u0007\npred.\nContent\n2.67\n72%\n0.81\nEarthquakes\nLapata\n(N/A)\n24%\n0.48\nBigram\n485.16\n4%\n0.27\nContent\n3.05\n48%\n0.64\nClashes\nLapata\n(N/A)\n27%\n0.41\nBigram\n635.15\n12%\n0.25\nContent\n15.38\n38%\n0.45\nDrugs\nLapata\n(N/A)\n27%\n0.49\nBigram\n712.03\n11%\n0.24\nContent\n0.05\n96%\n0.98\nFinance\nLapata\n(N/A)\n18%\n0.75\nBigram\n7.44\n66%\n0.74\nContent\n10.96\n41%\n0.44\nAccidents\nLapata\n(N/A)\n10%\n0.07\nBigram\n973.75\n2%\n0.19\nTable 2: Ordering results (averages over the test cases).\nDomain\nRank range\n[0-4]\n[5-10]\n\u0000'\n\u0000Earthquakes\n95%\n1%\n4%\nClashes\n75%\n18%\n7%\nDrugs\n47%\n8%\n45%\nFinance\n100%\n0%\n0%\nAccidents\n52%\n7%\n41%\nTable 3: Percentage of cases for which the content model\nassigned to the OSO a rank within a given range.\n5.4\nSummarization Experiments\nThe evaluation of our summarization algorithm was\ndriven by two questions: (1) Are the summaries produced\nof acceptable quality, in terms of selected content? and\n(2) Does the content-model representation provide addi-\ntional advantages over more locally-focused methods?\nTo address the ﬁrst question, we compare summaries\ncreated by our system against the “lead” baseline, which\nextracts the ﬁrst\n\u0000 sentences of the original text — de-\nspite its simplicity, the results from the annual Docu-\nment Understanding Conference (DUC) evaluation sug-\ngest that most single-document summarization systems\ncannot beat this baseline. To address question (2), we\nconsider a summarization system that learns extraction\nrules directly from a parallel corpus of full texts and their\nsummaries (Kupiec et al., 1999). In this system, sum-\nmarization is framed as a sentence-level binary classiﬁ-\ncation problem: each sentence is labeled by the publicly-\navailable BoosTexter system (Schapire and Singer, 2000)\nas being either “in” or “out” of the summary. The fea-\ntures considered for each sentence are its unigrams and\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n0\n20\n40\n60\n80\n100\nOSO prediction rate\nTraining-set size\nearthquake\nclashes\ndrugs\nfinance\naccidents\nFigure 2: Ordering-task performance, in terms of OSO\nprediction rate, as a function of the number of documents\nin the training set.\nits location within the text, namely beginning third, mid-\ndle third and end third.10 Hence, relationships between\nsentences are not explicitly modeled, making this system\na good basis for comparison.\nWe evaluated our summarization system on the Earth-\nquakes domain, since for some of the texts in this domain\nthere is a condensed version written by AP journalists.\nThese summaries are mostly extractive11; consequently,\nthey can be easily aligned with sentences in the original\narticles. From sixty document-summary pairs, half were\nrandomly selected to be used for training and the other\nhalf for testing. (While thirty documents may not seem\nlike a large number, it is comparable to the size of the\ntraining corpora used in the competitive summarization-\nsystem evaluations mentioned above.) The average num-\nber of sentences in the full texts and summaries was 15\nand 6, respectively, for a total of 450 sentences in each of\nthe test and (full documents of the) training sets.\nAt runtime, we provided the systems with a full doc-\nument and the desired output length, namely, the length\nin sentences of the corresponding shortened version. The\nresulting summaries were judged as a whole by the frac-\ntion of their component sentences that appeared in the\nhuman-written summary of the input text.\nThe results in Table 4 conﬁrm our hypothesis about\nthe beneﬁts of content models for text summarization —\nour model outperforms both the sentence-level, locally-\nfocused classiﬁer and the “lead” baseline. Furthermore,\nas the learning curves shown in Figure 3 indicate, our\nmethod achieves good performance on a small subset of\nparallel training data: in fact, the accuracy of our method\non one third of the training data is higher than that of the\n10This feature set yielded the best results among the several\npossibilities we tried.\n11Occasionally, one or two phrases or, more rarely, a clause\nwere dropped.\n",
  "speaker": "SYSTEM",
  "uuid": "846e60ab-7df9-4766-b25b-b6cd9cce94fb"
}