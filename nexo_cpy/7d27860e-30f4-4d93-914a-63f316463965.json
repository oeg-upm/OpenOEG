{
  "message": "SYSTEM: Sauper2007.pdf: Página 7\nTask\nLabeled\nUnlabeled\nAvg. Size\nTrain\nTest\nWords\nSents\nMulti-aspect sentiment\n600\n65\n—\n1,027\n20.5\nMulti-aspect summarization\nAmazon\n35\n24\n12,684\n214\n11.7\nYelp\n48\n48\n33,015\n178\n11.2\nTable 2: This table summarizes the size of each corpus. In each case, the unlabeled texts of both labeled and\nunlabeled documents are used for training the content model, while only the labeled training corpus is used\nto train the task model. Note that the entire data set for the multi-aspect sentiment analysis task is labeled.\nwith content features as described above. For this\napplication, we ﬁx the number of HMM states to be\nequal to the predeﬁned number of aspects.\nWe test our sentiment ranker on a set of DVD re-\nviews from the website IGN.com.5 Each review is\naccompanied by 1-10 scale ratings in four categories\nthat assess the quality of a movie’s content, video,\naudio, and DVD extras. In this data set, segments\ncorresponding to each of the aspects are clearly de-\nlineated in each document. Therefore, we can com-\npare the performance of the algorithm using auto-\nmatically induced content models against the gold\nstandard structural information.\nMulti-Aspect Review Summarization\nThe goal\nof this task is to extract informative phrases that\nidentify information relevant to several predeﬁned\naspects of interest. In other words, we would like our\nsystem to both extract important phrases (e.g., cheap\nfood) and label it with one of the given aspects (e.g.,\nvalue). For concrete examples and lists of aspects\nfor each data set, see Figures 3b and 3c. Variants of\nthis task have been considered in review summariza-\ntion in previous work (Kim and Hovy, 2006; Brana-\nvan et al., 2009).\nThis task has elements of both information extrac-\ntion and phrase-based summarization — the phrases\nwe wish to extract are broader in scope than in stan-\ndard template-driven IE, but at the same time, the\ntype of selected information is restricted to the de-\nﬁned aspects, similar to query-based summarization.\nThe difﬁculty here is that phrase selection is highly\ncontext-dependent. For instance, in TV reviews such\nas in Figure 3b, the highlighted phrase “easy to read”\nmight refer to either the menu or the remote; broader\n5http://dvd.ign.com/index/reviews.html\ncontext is required for correct labeling.\nWe evaluated our approach for this task on two\ndata sets: Amazon TV reviews (Figure 3b) and Yelp\nrestaurant reviews (Figure 3c). To eliminate noisy\nreviews, we only retain documents that have been\nrated “helpful” by the users of the site; we also re-\nmove reviews which are abnormally short or long.\nEach data set was manually annotated with aspect\nlabels using Mechanical Turk, which has been used\nin previous work to annotate NLP data (Snow et al.,\n2008). Since we cannot select high-quality annota-\ntors directly, we included a control document which\nhad been previously annotated by a native speaker\namong the documents assigned to each annotator.\nThe work of any annotator who exhibited low agree-\nment on the control document annotation was ex-\ncluded from the corpus.\nTo test task annotation\nagreement, we use Cohen’s Kappa (Cohen, 1960).\nOn the Amazon data set, two native speakers anno-\ntated a set of four documents. The agreement be-\ntween the judges was 0.54. On the Yelp data set, we\nsimply computed the agreement between all pairs of\nreviewers who received the same control documents;\nthe agreement was 0.49.\n4.2\nBaseline Comparison and Evaluation\nBaselines\nFor all the models, we obtain a baseline\nsystem by eliminating content features and only us-\ning a task model with the set of features described\nabove. We also compare against a simpliﬁed vari-\nant of our method wherein a content model is in-\nduced in isolation rather than learned jointly in the\ncontext of the underlying task. In our experiments,\nwe refer to the two methods as the No Content\nModel (NoCM) and Independent Content Model\n(IndepCM) settings, respectively. The Joint Content\n",
  "speaker": "SYSTEM",
  "uuid": "7d27860e-30f4-4d93-914a-63f316463965"
}